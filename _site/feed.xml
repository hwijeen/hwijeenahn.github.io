<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>Hwijeen Ahn</title>
    <link>http://localhost:4000</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Network Cheatheet</title>
        <description>&lt;h2 id=&quot;path-from-one-computer-to-anoter&quot;&gt;Path from one computer to anoter&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://run-it.tistory.com/17?category=665122&quot;&gt;Post about routing&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://reviewkr.tistory.com/56&quot;&gt;IT 동아 computer netwrok의 기초&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Broadcast, Subnetmask&lt;/li&gt;
  &lt;li&gt;IP: 공인/사설, 고정/유동&lt;/li&gt;
  &lt;li&gt;Gateway / DNS&lt;/li&gt;
  &lt;li&gt;Protocol(TCP/IP, HTTP, FTP, POP/IMAP/SMTP, DHCP)&lt;/li&gt;
  &lt;li&gt;랜카드와 랜케이블&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;useful-commands&quot;&gt;Useful commands&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;netstat &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# show route table&lt;/span&gt;
lsof &lt;span class=&quot;nt&quot;&gt;-Pn&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i4&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# show open ports &lt;/span&gt;
ifconfig 
traceroute
nmap
host &lt;span class=&quot;nv&quot;&gt;$DOMAIN_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# look up domanin name server for ip&lt;/span&gt;
scutil &lt;span class=&quot;nt&quot;&gt;--dns&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# display dns configuration&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Network Utilty is an app to do a number of jobs.&lt;/li&gt;
  &lt;li&gt;Tech stuff](https://www.zytrax.com/tech/protocols/ip-classes.html)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ssh&quot;&gt;SSH&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh &lt;span class=&quot;nt&quot;&gt;-J&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$MIDDLE_USER&lt;/span&gt;@&lt;span class=&quot;nv&quot;&gt;$MIDDLE_HOST&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$REMOTE_USER&lt;/span&gt;@&lt;span class=&quot;nv&quot;&gt;$REMOTE_HOST&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# ssh into remoteuser via middle user&lt;/span&gt;
scp &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'ProxyJump $MIDDLE_USER@$MIDDLE_HOST'&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$REMOTE_USER&lt;/span&gt;@&lt;span class=&quot;nv&quot;&gt;$REMOTE_HOST&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$REMOTE_PATH&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# scp using ssh jump&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;http&quot;&gt;HTTP&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; http.server 60000 &lt;span class=&quot;nt&quot;&gt;--bind&lt;/span&gt; 127.0.0.1 &lt;span class=&quot;c&quot;&gt;# simple python HTTP web server&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;connection&quot;&gt;Connection&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nmap &lt;span class=&quot;nt&quot;&gt;-Pn&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PORT&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$REMOTE_IP&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# port scan. -Pn if host is up for sure&lt;/span&gt;
nc &lt;span class=&quot;nt&quot;&gt;-z&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$REMOTE_IP&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PORT&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# another way of portscan&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;python-socket&quot;&gt;Python socket&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;server_socket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setsockopt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;socket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SOL_SOCKET&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;socket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SO_REUSEADDR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# to reuse address &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;recv, accept are blocking by default!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;p&gt;https://jvns.ca/blog/2017/09/03/network-interfaces/&lt;/p&gt;

&lt;p&gt;https://flaviocopes.com/http-curl/&lt;/p&gt;
</description>
        <pubDate>Tue, 31 Dec 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-12-31/Network-Cheatsheet/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-12-31/Network-Cheatsheet/</guid>
      </item>
    
      <item>
        <title>Brew Cheatheet</title>
        <description>&lt;h2 id=&quot;brew&quot;&gt;Brew&lt;/h2&gt;

&lt;p&gt;각종 프로그램 설치를 도와주는데, 깔끔한 환경 관리를 위해 되도록이면 brew를 쓰자. 설치한 프로그램은 &lt;code class=&quot;highlighter-rouge&quot;&gt;usr/local/Cellar&lt;/code&gt;에 저장되며, &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/bin&lt;/code&gt;에 symbolic link가 생성된다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew list 

brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;tldr
brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;mysql
brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;vim &lt;span class=&quot;c&quot;&gt;# for +clipboard enabled vim&lt;/span&gt;
brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;gcc@8
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;brew-cask&quot;&gt;Brew Cask&lt;/h2&gt;

&lt;p&gt;Mac app을 CLI 환경에서 간단하게 깔아주는 프로그램&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew cask list

brew cask &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;java
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;brew-service&quot;&gt;Brew service&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew services start &lt;span class=&quot;nv&quot;&gt;$PROGRAM&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# start at login&lt;/span&gt;
brew services restart &lt;span class=&quot;nv&quot;&gt;$PROGRAM&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# it's like restarting..&lt;/span&gt;

brew services run &lt;span class=&quot;nv&quot;&gt;$PROGRAM&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# run but don't start it at login&lt;/span&gt;

brew servies list &lt;span class=&quot;c&quot;&gt;# list all services managed by brew servies&lt;/span&gt;

brew services cleanup &lt;span class=&quot;c&quot;&gt;# remove all unused servies&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;brew-tap&quot;&gt;Brew tap&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;brew tap&lt;/code&gt; adds more repositories to the list of formulae that &lt;code class=&quot;highlighter-rouge&quot;&gt;brew&lt;/code&gt; tracks, updates, and installs from. By default, &lt;code class=&quot;highlighter-rouge&quot;&gt;tap&lt;/code&gt; assumes that the repositories come from GitHub, but the command isn’t limited to any one location.&lt;/p&gt;

&lt;h2 id=&quot;dependencies&quot;&gt;Dependencies&lt;/h2&gt;

&lt;p&gt;https://blog.jpalardy.com/posts/untangling-your-homebrew-dependencies/&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Dec 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-12-11/HomeBrew-Cheatsheet/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-12-11/HomeBrew-Cheatsheet/</guid>
      </item>
    
      <item>
        <title>Shell Cheat sheet</title>
        <description>&lt;p&gt;From &lt;a href=&quot;https://devhints.io/bash&quot;&gt;cheatsheet&lt;/a&gt;, &lt;a href=&quot;https://www.cyberciti.biz/faq/bash-for-loop/&quot;&gt;for loop&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;variables&quot;&gt;Variables&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Hi&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$A&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# command substitution&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;python &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;print(1)&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# $()은 우선적으로 evaluate하라는 의미(shell execution)&lt;/span&gt;
ctags &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;python &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;import sys; print(sys.path)&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# python 실행 결과가 커맨드로 입력됨&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;for-loops&quot;&gt;For loops&lt;/h2&gt;

&lt;h2 id=&quot;some-bash-commands&quot;&gt;Some bash commands&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;find &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-name&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'*keyword*'&lt;/span&gt;
find &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-name&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'.*'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-delete&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# recursively delete all the hidden files&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--exclude-dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;data/ &lt;span class=&quot;s1&quot;&gt;'keyword'&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# -i 대소문자 구별 안 함 -n line number&lt;/span&gt;

scp &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$localfolder&lt;/span&gt; hwijeen@163.239.199.230:&lt;span class=&quot;nv&quot;&gt;$remotefolder&lt;/span&gt;/ &lt;span class=&quot;c&quot;&gt;# from to&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;folder &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# second commnad executed only if the first succeeds&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;folder&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# just two independent commands&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;sort&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; input | &lt;span class=&quot;nb&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; 100 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;output &lt;span class=&quot;c&quot;&gt;# randomly select sentences&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; python program.py&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# to check runtime&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; +2 &lt;span class=&quot;nv&quot;&gt;$filename&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# from 2nd line to last(skip header)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'{T+= NF} END { print T/NR}'&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$FILE&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# average length of tokens &lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'{print NF}'&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;sort&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# lengths of longest lines&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;cut&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'.'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f3-&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$filename&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# cut 3rd fields to the end&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'s/^&quot;//'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'s/&quot;$//'&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$filename&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# delete heading and trailing &quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Tue, 05 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-11-05/Shell-Cheatsheet/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-11-05/Shell-Cheatsheet/</guid>
      </item>
    
      <item>
        <title>Matrix multiplication</title>
        <description>&lt;p&gt;From &lt;a href=&quot;https://twlab.tistory.com/10&quot;&gt;Learn Again!&lt;/a&gt; and &lt;a href=&quot;http://nyc.lti.cs.cmu.edu/classes/11-741/f19/index.html&quot;&gt;11-741&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;matrix-vector-multiplication&quot;&gt;Matrix-vector multiplication&lt;/h2&gt;

&lt;p&gt;###Weighted sum of columns&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Users/hwii/Library/Application Support/typora-user-images/image-20191105155704012.png&quot; alt=&quot;image-20191105155704012&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$Ax$ is linear combination of columns of $A$.&lt;/li&gt;
  &lt;li&gt;The weights are given by the elements of $x$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;weighted-sum-of-rows&quot;&gt;Weighted sum of rows&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;file:///Users/hwii/Library/Application%20Support/typora-user-images/image-20191105153615011.png?lastModify=1572986243&quot; alt=&quot;image-20191105153615011&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$x^TA$ is linear combination of rows of $A$.&lt;/li&gt;
  &lt;li&gt;The weights are given by the elements of $x^T$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;matrix-diagonal-matrix-multiplication&quot;&gt;Matrix-diagonal matrix multiplication&lt;/h2&gt;

&lt;h3 id=&quot;weights-on-columns&quot;&gt;Weights on columns&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/Users/hwii/Library/Application Support/typora-user-images/image-20191105155257809.png&quot; alt=&quot;image-20191105155257809&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A column in $A\Lambda$ = a column of $A$ multiplied by the weight.&lt;/li&gt;
  &lt;li&gt;The weights are given by the diagonal elements.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;weights-on-rows&quot;&gt;Weights on rows&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/Users/hwii/Library/Application Support/typora-user-images/image-20191105155310859.png&quot; alt=&quot;image-20191105155310859&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A row in $\Lambda B$ = a row of $B$ multiplied by the weight.&lt;/li&gt;
  &lt;li&gt;The weights are given by the diagonal elements.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;matrix-matrix-multiplication&quot;&gt;Matrix-matrix multiplication&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;AB = C&lt;/script&gt;

&lt;h3 id=&quot;row--column&quot;&gt;Row * column&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C_{i,j} = \sum \limits_{k} A_{i,k}B_{k,i}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;A element in C = inner product of a row in A and a column in B.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;column--row&quot;&gt;Column * row&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C = \sum\limits_{i} a_ib_i^T&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;C is a sum of outer product of columns in A and rows in B.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;###Column-wise&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Users/hwii/Library/Application Support/typora-user-images/image-20191105154505379.png&quot; alt=&quot;image-20191105154505379&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A column in C = a weighted combination of the columns of A.&lt;/li&gt;
  &lt;li&gt;The weights are given by a columns in B.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;###Row-wise&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Users/hwii/Library/Application Support/typora-user-images/image-20191105154442420.png&quot; alt=&quot;image-20191105154442420&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A row in C = a weighted combination of the rows in B&lt;/li&gt;
  &lt;li&gt;The weights are given by a row in A.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;useful-identities&quot;&gt;Useful identities&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;VV^T = \sum \limits_i^n v_iv_i^T \\
V \Lambda V^T = \sum_i^n \lambda_i v_i v_i^T \\&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z^TLZ =&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/Users/hwii/Library/Application Support/typora-user-images/image-20191118221843468.png&quot; alt=&quot;image-20191118221843468&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Notation for a vector and a scalar can be confusing here.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(V^TV)_{i,j} = v_i^Tv_j \\&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u^T \Lambda u = \sum \limits_i \lambda_i u_iu_i\\
u^TAu = \sum_i \sum_j u_i u_j A_{i,j} \\&lt;/script&gt;

</description>
        <pubDate>Tue, 05 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-11-05/Matrix-multiplication/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-11-05/Matrix-multiplication/</guid>
      </item>
    
      <item>
        <title>Theory of NP</title>
        <description>&lt;p&gt;From Foundations of Algorithms by Richard E.Neapolitan.&lt;/p&gt;

&lt;h2 id=&quot;intractability&quot;&gt;Intractability&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Polynomial-time algorithm&lt;/strong&gt; is one whose worst-cast time complexity is bounded by a polynomial function of its input size.&lt;/li&gt;
  &lt;li&gt;A problem is &lt;strong&gt;intractable&lt;/strong&gt; if it is impossible to solve it with a polynomial-time algorithm.&lt;/li&gt;
  &lt;li&gt;Three feneral categories of problems as far as intractability is concerned:
    &lt;ul&gt;
      &lt;li&gt;Problems for which polynomial-time algorithms have been found&lt;/li&gt;
      &lt;li&gt;Problems that have proven to be intractable&lt;/li&gt;
      &lt;li&gt;Problems that have not been proven to be intractable, but for which polynomial-time algorithms have never been found&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All problems that to this date have been proven intractable have also been proven not to be in the set NP.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Most problems that appear to be intractable are in the set NP.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;theory-of-np&quot;&gt;Theory of &lt;em&gt;NP&lt;/em&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;We restrict ourselves to decision problems.
    &lt;ul&gt;
      &lt;li&gt;Each optimization problem has a corresponding decision problem.&lt;/li&gt;
      &lt;li&gt;A solution to an optimization problem produces a solution to the correspnding decision problem.&lt;/li&gt;
      &lt;li&gt;For many decision problems, it has been shown that a polynomial-time algorithm for the decision problem would yield a polynomial-time algorithm for the corresponding optimization problem.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-sets-p-and-np&quot;&gt;The sets &lt;em&gt;P&lt;/em&gt; and &lt;em&gt;NP&lt;/em&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;P&lt;/em&gt;&lt;/strong&gt; is the set of all decision problems that can be solved by polynomial-time algorithms.
    &lt;ul&gt;
      &lt;li&gt;For some problems, we simply do not know if there exists an polynomial-time algorithm.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Nondeterministic algorithm&lt;/strong&gt; simply produces some string S. The string can be though of a guess at a solution to the instance. However, it could just be a string of nonsense.
    &lt;ul&gt;
      &lt;li&gt;It is called nondeterministic because unique step-by-step instructions are not specified for it.&lt;/li&gt;
      &lt;li&gt;It is simply a defitional device for the purpose of obtaining the notion of polynomial time verifiability.  It is not a realistic method for solving a decision problem.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Polynomial time verifiability&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;A polynomial-time nondeterminsitic algorithm&lt;/strong&gt; is a nondeterministic algorithm whose verification stage is a polynomial-time algorithm.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;NP&lt;/em&gt;&lt;/strong&gt; is the set of all decision problems that can be solved by polynoial-time nondeterministic algorithms.
    &lt;ul&gt;
      &lt;li&gt;This does not mean that problems in this set can necessarily be solved in polynomial time.&lt;/li&gt;
      &lt;li&gt;The purpose of introducing the concepts of nondeterministic algorithm and NP is to classify algorithms.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;There are thousand of problems that no one has been able to solve with polynomial-time algorithms but that have been proven to be in NP because polynomial-time nondeterministic algorithms have been developed for them.&lt;/li&gt;
  &lt;li&gt;Trivially, every problem in P is also in NP.&lt;/li&gt;
  &lt;li&gt;The only decision problems that have been proven not to be in NP are the same ones that have been proven to be intractable. e.g) Halting problem.&lt;/li&gt;
  &lt;li&gt;No one has ever proven that there is a problem in NP that is not in P. If P = NP, we would have polynomial-time algorithms for most known decision problems (in NP).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;np-complete-problems&quot;&gt;&lt;em&gt;NP&lt;/em&gt;-complete problems&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CNF-satisfiability problem&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;This is NP problem.&lt;/li&gt;
      &lt;li&gt;We do not know if it is in P.&lt;/li&gt;
      &lt;li&gt;If this is in P, then P = NP.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Transfomation algorithm&lt;/strong&gt; creates an instance y of problem B from every instance x of problem A such that an algorithm for problem B answers “yes” for y if and only if the answer to problem A is “yes” for x.&lt;img src=&quot;/Users/hwii/Library/Application Support/typora-user-images/image-20190904151828561.png&quot; alt=&quot;image-20190904151828561&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;If there exists a polynomial-time transformation algorithm from decision problem &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; to decision problem B, problem A is &lt;strong&gt;polynomial-time many-one reducible&lt;/strong&gt; to problem &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;.
    &lt;ul&gt;
      &lt;li&gt;If the decision probem B is in P and A &lt;script type=&quot;math/tex&quot;&gt;\propto&lt;/script&gt; B, then decision problem A in in P.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A problem B is called &lt;strong&gt;&lt;em&gt;NP&lt;/em&gt;-complete&lt;/strong&gt; if both of the following are true:
    &lt;ul&gt;
      &lt;li&gt;B is in NP.&lt;/li&gt;
      &lt;li&gt;For every other problem in A NP, A &lt;script type=&quot;math/tex&quot;&gt;\propto&lt;/script&gt; B.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If we could show that any NP-complete problem is in P, we could conclude that P = NP.&lt;/li&gt;
  &lt;li&gt;CNF-satisfiabilty problem is proven to be NP-complete.&lt;/li&gt;
  &lt;li&gt;A problem C is NP-complete if both of the following are true:
    &lt;ul&gt;
      &lt;li&gt;C is in NP.&lt;/li&gt;
      &lt;li&gt;For some other NP-complete problem B, B &lt;script type=&quot;math/tex&quot;&gt;\propto&lt;/script&gt; C.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-state-of-np&quot;&gt;The state of NP&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/Users/hwii/Library/Application Support/typora-user-images/image-20190904153207321.png&quot; alt=&quot;image-20190904153207321&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;P is a proper subset of NP. It may be true that they are the same.&lt;/li&gt;
  &lt;li&gt;NP-complete is a subset of NP by definition.&lt;/li&gt;
  &lt;li&gt;A decision problem that is in NP and is not NP-complete is the trivial decision problems that answers “yes” for all instances.
    &lt;ul&gt;
      &lt;li&gt;It is not NP-complete because it is not possible to transform a nontrivial decision problem to it.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If P &lt;script type=&quot;math/tex&quot;&gt;\neq&lt;/script&gt; NP, P &lt;script type=&quot;math/tex&quot;&gt;\cap&lt;/script&gt; NP-complete = &lt;script type=&quot;math/tex&quot;&gt;\emptyset&lt;/script&gt;. This is so because if some problem in P were NP-complete, it is implied that we could sove any problem in NP in polynomial time.&lt;/li&gt;
  &lt;li&gt;If P &lt;script type=&quot;math/tex&quot;&gt;\neq&lt;/script&gt; NP, NP - (P &lt;script type=&quot;math/tex&quot;&gt;\cup&lt;/script&gt; NP-compete ) is not empty.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;np-hard&quot;&gt;&lt;em&gt;NP&lt;/em&gt;-Hard&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;If problem A can be solved in polynomial time using a &lt;strong&gt;hypothetical&lt;/strong&gt; polynomial time algorithm for problem B, the problem A is &lt;strong&gt;polynomial-time Turing reducible&lt;/strong&gt; to problem B.&lt;/li&gt;
  &lt;li&gt;If A &lt;script type=&quot;math/tex&quot;&gt;\propto&lt;/script&gt; B, then A &lt;script type=&quot;math/tex&quot;&gt;\propto_{T}&lt;/script&gt; B.&lt;/li&gt;
  &lt;li&gt;A problem B is called &lt;strong&gt;NP-hard&lt;/strong&gt;, if for some NP-complete problem A, A &lt;script type=&quot;math/tex&quot;&gt;\propto_{T}&lt;/script&gt; B.&lt;/li&gt;
  &lt;li&gt;NP problem A &lt;script type=&quot;math/tex&quot;&gt;\propto_{T}&lt;/script&gt; NP-complete problem B &lt;script type=&quot;math/tex&quot;&gt;\propto_{T}&lt;/script&gt; NP-hard problem C.
    &lt;ul&gt;
      &lt;li&gt;All problems in NP reduce to any NP-hard problem.&lt;/li&gt;
      &lt;li&gt;If a polynomial-time algorithm exists for any NP-hard problem, then P = NP.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Every NP-complete problem is NP-hard.
    &lt;ul&gt;
      &lt;li&gt;The optimization problem corresonding to any NP-complete (decision) problem is NP-hard.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If we were to prove that some problem was not NP-hard, we would be proving that P &lt;script type=&quot;math/tex&quot;&gt;\neq&lt;/script&gt; NP.
    &lt;ul&gt;
      &lt;li&gt;If P = NP, all problems in NP can be solved with a polynomial-time algorithm. Then, all problems would be NP-hard.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If we were to prove that some problem for which we had a polynomial-time algorithm was NP-hard, we would be proving the P = NP.
    &lt;ul&gt;
      &lt;li&gt;NP &lt;script type=&quot;math/tex&quot;&gt;\propto_{T}&lt;/script&gt; NP-hard이니까 NP 문제를 polynomial time에 풀어버리니까&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/Users/hwii/Library/Application Support/typora-user-images/image-20190904155131859.png&quot; alt=&quot;image-20190904155131859&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/P_np_np-complete_np-hard.svg/800px-P_np_np-complete_np-hard.svg.png&quot; alt=&quot;File:P np np-complete np-hard.svg&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 05 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-09-05/Theory-of-NP/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-09-05/Theory-of-NP/</guid>
      </item>
    
      <item>
        <title>Docker cheat sheet</title>
        <description>&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Basics: &lt;a href=&quot;https://www.nvidia.co.kr/content/apac/event/kr/deep-learning-day-2017/dli-1/Docker-User-Guide-17-08_v1_NOV01_Joshpark.pdf&quot;&gt;Slides from Nvidia&lt;/a&gt; / &lt;a href=&quot;https://subicura.com/2017/01/19/docker-guide-for-beginners-1.html&quot;&gt;한국어 블로그&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;사전처럼 사용할 수 있는 &lt;a href=&quot;http://pyrasis.com/docker.html&quot;&gt;블로그&lt;/a&gt; or &lt;a href=&quot;https://docs.docker.com/&quot;&gt;official docker docs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;install &lt;a href=&quot;[https://subicura.com/2017/01/19/docker-guide-for-beginners-2.html#%EB%8F%84%EC%BB%A4-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0](https://subicura.com/2017/01/19/docker-guide-for-beginners-2.html#도커-설치하기)&quot;&gt;docker&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;install &lt;a href=&quot;https://github.com/NVIDIA/nvidia-docker&quot;&gt;nvidia-docker&lt;/a&gt;
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;usermod &lt;span class=&quot;nt&quot;&gt;-aG&lt;/span&gt; docker &lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 현재 접속중인 사용자에게 권한주기&lt;/span&gt;
docker run &lt;span class=&quot;nt&quot;&gt;--runtime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;nvidia &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; nvidia/cuda nvidia-smi &lt;span class=&quot;c&quot;&gt;# check if nvidia-docker is installed&lt;/span&gt;
docker info
docker stats
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;images&quot;&gt;Images&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker search &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;IMAGE_KEYWORD&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# search from dockerhub&lt;/span&gt;
docker pull &lt;span class=&quot;nv&quot;&gt;$IMAGE_NAME&lt;/span&gt;

docker images &lt;span class=&quot;c&quot;&gt;# list images&lt;/span&gt;
docker rmi &lt;span class=&quot;nv&quot;&gt;$IMAGE_NAME&lt;/span&gt;

docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;IMAGE_NAME&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# build image from dockerfile&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;containers&quot;&gt;Containers&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker ps &lt;span class=&quot;c&quot;&gt;# list containers&lt;/span&gt;
docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /home/host:/home/container &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8888:8888 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;CONTAINER&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; HOME:&lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$IMAGE&lt;/span&gt; 
docker start &lt;span class=&quot;nv&quot;&gt;$CONTAINER&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# start stopped container&lt;/span&gt;
docker attach &lt;span class=&quot;nv&quot;&gt;$CONTAINER&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# get into the container&lt;/span&gt;
docker &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$CONTAINER&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# warning - this deletes file!&lt;/span&gt;
docker commit &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'(USERNAME) (&amp;lt;EMAIL&amp;gt;)'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'(MESSAGE)'&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$CONTAINER&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;IMAGENAME&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;:&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;TAG&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# from container to image&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;dockerfileexample&quot;&gt;Dockerfile(example)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;use &lt;code class=&quot;highlighter-rouge&quot;&gt;.dockerignore &lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;consider &lt;a href=&quot;https://subicura.com/2017/02/10/docker-guide-for-beginners-create-image-and-deploy.html&quot;&gt;build cache&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; nvidia/cuda:10.0-base-ubuntu16.04&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;LABEL&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; maintainer &quot;NVIDIA CORPORATION &amp;lt;cudatools@nvidia.com&amp;gt;&quot;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; NCCL_VERSION 2.4.2&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apt-get update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--no-install-recommends&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        cuda-libraries-&lt;span class=&quot;nv&quot;&gt;$CUDA_PKG_VERSION&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        cuda-nvtx-&lt;span class=&quot;nv&quot;&gt;$CUDA_PKG_VERSION&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        &lt;span class=&quot;nv&quot;&gt;libnccl2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$NCCL_VERSION&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-1&lt;/span&gt;+cuda10.0 &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    apt-mark hold libnccl2 &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; /var/lib/apt/lists/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;# from kaixhin/torch/dockerfile&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apt-get update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        git &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        software-properties-common &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        libssl-dev &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        libzmq3-dev &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        python-dev &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        python-pip &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        python-zmq &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        &lt;span class=&quot;nb&quot;&gt;sudo&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;git clone https://github.com/torch/distro.git /root/torch &lt;span class=&quot;nt&quot;&gt;--recursive&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /root/torch &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    bash install-deps &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    ./install.sh

&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; LUA_PATH='/root/.luarocks/share/lua/5.1/?.lua;/root/.luarocks/share/lua/5.1/?/init.lua;/root/torch/install/share/lua/5.1/?.lua;/root/torch/install/share/lua/5.1/?/init.lua;./?.lua;/root/torch/install/share/luajit-2.1.0-beta1/?.lua;/usr/local/share/lua/5.1/?.lua;/usr/local/share/lua/5.1/?/init.lua'&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; LUA_CPATH='/root/.luarocks/lib/lua/5.1/?.so;/root/torch/install/lib/lua/5.1/?.so;./?.so;/usr/local/lib/lua/5.1/?.so;/usr/local/lib/lua/5.1/loadall.so'&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; PATH=/root/torch/install/bin:$PATH&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; LD_LIBRARY_PATH=/root/torch/install/lib:$LD_LIBRARY_PATH&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; DYLD_LIBRARY_PATH=/root/torch/install/lib:$DYLD_LIBRARY_PATH&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; LUA_CPATH='/root/torch/install/lib/?.so;'$LUA_CPATH&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;## added by hwijeen&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        vim &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        gcc-4.8-plugin-de &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;        wget
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /home/Downloads
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /home/Downloads&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; http://www.lua.org/ftp/lua-5.3.5.tar.gz &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    &lt;span class=&quot;nb&quot;&gt;tar &lt;/span&gt;zxf lua-5.3.5.tar.gz &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    &lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;lua-5.3.5 &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    make linux &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;make &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /home/Downloads&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;wget https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.20/src/hdf5-1.8.20.tar.gz &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    &lt;span class=&quot;nb&quot;&gt;tar&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-zxvf&lt;/span&gt; hdf5-1.8.20.tar.gz &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    &lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;hdf5-1.8.20 &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    make &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; PATH=/home/Downloads/hdf5-1.8.20/hdf5:$PATH&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;luarocks &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;hdf5 20-0
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /home/WNGT2019
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /home/WNGT2019&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; . ./&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;temporary-shell-fileexample&quot;&gt;Temporary shell file(example)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;make shell before writing dockerfile&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get update
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;vim
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;gcc-4.8-plugin-de
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;wget

curl &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; http://www.lua.org/ftp/lua-5.3.5.tar.gz &lt;span class=&quot;c&quot;&gt;# install lua&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;tar &lt;/span&gt;zxf lua-5.3.5.tar.gz
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;lua-5.3.5
make linux &lt;span class=&quot;nb&quot;&gt;test
sudo &lt;/span&gt;make &lt;span class=&quot;nb&quot;&gt;install

&lt;/span&gt;wget https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.20/src/hdf5-1.8.20.tar.gz
&lt;span class=&quot;nb&quot;&gt;tar&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-zxvf&lt;/span&gt; hdf5-1.8.20.tar.gz
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;hdf5-1.8.20
make &lt;span class=&quot;nb&quot;&gt;install

&lt;/span&gt;luarocks &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;hdf5 20-0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Mon, 15 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-07-15/Docker-Cheatsheet/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-07-15/Docker-Cheatsheet/</guid>
      </item>
    
      <item>
        <title>Data-to-Text Generation with Content Selection and Planning</title>
        <description>&lt;p&gt;Data-to-Text Generation with Content Selection and Planning., AAAI 2019. &lt;a href=&quot;https://arxiv.org/abs/1809.00582&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;
&lt;p&gt;이 논문은 RotoWire dataset을 이용한 data-to-text task 성능을 높이기 위한 새로운 모델을 제시한다. 저자는 data-to-task를 1) content plan과 2) text generation 두 부분으로 세분화해서 접근한다. 제안 모델도 unordered record로부터 content plan을 만드는 부분과 주어진 content plan에 기반해 자연어 summary를 만들어내는 부분으로 구성된다. 두 개의 submodule로 접근하는 전통적인 방법을 neural network를 사용한 end-to-end 학습에 적용한 것이다.&lt;/p&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;p(y|r) = \sum_z p(y,z|r) = \sum_z p(z|r) p(y|r,z)&lt;/script&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Records $r$에 따른 summary $y$의 확률 모델은 $p(z&lt;/td&gt;
      &lt;td&gt;r)$과 $p(y&lt;/td&gt;
      &lt;td&gt;r,z)$ 두 부분으로 쪼개진다. 각각의 확률 분포는 Encoder-Decoder로 모델링된다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;1-pzr&quot;&gt;1. $p(z|r)$&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Encoder-Decoder&lt;/em&gt;&lt;/strong&gt;{
    &lt;strong&gt;&lt;em&gt;Embedding&lt;/em&gt;&lt;/strong&gt;:  simple lookup table + &lt;em&gt;self-attention(?)&lt;/em&gt;&lt;br /&gt;
    &lt;strong&gt;&lt;em&gt;Encoder&lt;/em&gt;&lt;/strong&gt;: 별다른 처리 없이 embedding을 쌓아두기&lt;br /&gt;
    &lt;strong&gt;&lt;em&gt;Decoder&lt;/em&gt;&lt;/strong&gt;:  RNN&lt;br /&gt;
    &lt;strong&gt;&lt;em&gt;Generator&lt;/em&gt;&lt;/strong&gt;:  Pointer network&lt;br /&gt;
    }&lt;/p&gt;

&lt;h4 id=&quot;record-encoder&quot;&gt;Record Encoder&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{r}_j = ReLU(W_r[r_{j,1}; r_{j,2}; r_{j,3}; r_{j,4}])&lt;/script&gt;

&lt;p&gt;Raw text를 input으로 받아서 이에 대한 vector representation $r_j$을 만들어준다. record에 포함된 type, entity, value, home/away 네 가지 항목을 simple lookup-table을 통해 임베딩 한 뒤 MLP을 거쳐 하나의 벡터로 합친다.&lt;/p&gt;

&lt;h4 id=&quot;content-selection-gate&quot;&gt;Content Selection Gate&lt;/h4&gt;

&lt;p&gt;하나의 record representation $r_j$를 만들 때 주변 record $r_{i \neq{j} }$를 고려하면 더 풍부한 정보를 담을 수 있다는 아이디어다. 예를 들어, ‘한 선수(entity)가 고(value)득점(type)을 했다’는 record의 표현을 만들 때 이 선수의 rebound정보, 3점슛 개수 등을 반영하면 summary를 만들 때 더 도움이 된다는 것이다. Self-attention mechanism 활용해서 주변 정보를 반영한다.
&lt;script type=&quot;math/tex&quot;&gt;\alpha_{j,k} \propto exp(r_j^\intercal W_a r_k) \\
c_j = \sum_{k \neq j} \alpha_{j,k} r_k \\
r_j^{att} = W_g[r_j; c_j]&lt;/script&gt;
$r_j^{att}$에는 j번 째 record에 대한 정보($r_j$)와 주변 정보($c_j$)가 모두 담겨있다. 우리가 원하는 건 전체 record에 대한 정보가 아니라 j번 째 record에 대한 representation고, 이때 context정보를 일정부분 고려하자는 것이다. 이를 위해 context mechanism을 활용한다.
&lt;script type=&quot;math/tex&quot;&gt;g_j = sigmoid(r_j^{att}) \\
r_j^{cs} = g_j \odot r_j&lt;/script&gt;
$r_j^{cs}$가 j번 째 record에 대한 최종 representation, 즉 $p(z|r)$ 모델의 인코더의 output이다.&lt;/p&gt;

&lt;h4 id=&quot;content-planning&quot;&gt;Content Planning&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
p(z|r) = \prod_{k=1}^{|z|} p(z_k | z_{&lt;k}, r) %]]&gt;&lt;/script&gt;

&lt;p&gt;$p(z|r)$모델의 디코더로서, 인코딩된 record representation을 입력으로 받아서 content sequence에 대한 확률 분포를 autoregressive한 방식으로 내뱉는다. 이때 content($z_k$) sequence는 input으로 들어온 여러 개의 record에 순서를 부여한 것이다. 매 time step에서 source side에 입력으로 들어온 여러 record중 하나의 record를 고르는데 pointer network가 사용된다. 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
p(z_k= r_j|z_{&lt;k}, r) \propto exp(h_k^{\intercal} W_c r_j^{cs}) %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;first hidden으로는$r_j^{cs}$의 평균을 사용한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이와 같은 모델을 훈련하려면 정답 planning sequence가 필요한데, 이는 별도의 information extraction module을 통해서 얻어낸다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;### 2. $p(y&lt;/td&gt;
      &lt;td&gt;r,z)$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Encoder-Decoder&lt;/strong&gt;{
      &lt;strong&gt;&lt;em&gt;Embedding&lt;/em&gt;&lt;/strong&gt;:  simple look-up table&lt;br /&gt;
      &lt;strong&gt;&lt;em&gt;Encoder&lt;/em&gt;&lt;/strong&gt;: Bidirectional RNN
      &lt;strong&gt;&lt;em&gt;Decoder&lt;/em&gt;&lt;/strong&gt;:  RNN&lt;br /&gt;
      &lt;strong&gt;&lt;em&gt;Generator&lt;/em&gt;&lt;/strong&gt;:  Copy mechanism(joint copy or conditional copy)
      }&lt;/p&gt;

&lt;p&gt;#### Text Generation&lt;/p&gt;

&lt;p&gt;$p(z|r)$ 모듈을 통해 content plan sequence가 만들어졌으면, 이에 condition해서 자연어 text를 만들어낸다. Encoder-decoder모델의 source side(bidrectional LSTM)에 input으로 들어가는 것은 순서가 정해진 record sequence이며, decoder는 autogressive한 방식으로 텍스트를 만들어낸다. Attention mechanism과 copy mechanism을 통해 성능을 향상시켰다. 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\beta_{t,k} \propto exp(d_t^{\intercal}W_be_k) \\
  q_t = \sum_{k} \beta_{t,k} e_k \\
  d_t^{att} = tanh(W_d[d_t;q_t]) \\
  p_{gen}(y_t|y_{&lt;t}, z, r) = \sum_{u_t \in\left\{0, 1\right\}}p(y_t, u_t | y_{&lt;t}, z, r) %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;## Experiment
  &lt;img src=&quot;/assets/img/puduppully_result.png&quot; alt=&quot;result&quot; /&gt;&lt;/p&gt;

&lt;p&gt;## Issue&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Content Selection gate에서 사용하는 게 Self-attention 맞습니까?&lt;/li&gt;
  &lt;li&gt;Pointer network를 통해 source side에 대한 distribution을 만들텐데, 이후에 source token중에서 하나로 특정하는 건 argmax? sampling?&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 10 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-07-10/Data-to-Text-Generation-with-Content-Selection-and-Planning/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-07-10/Data-to-Text-Generation-with-Content-Selection-and-Planning/</guid>
      </item>
    
      <item>
        <title>Challenges in Data-to-Document Generation</title>
        <description>&lt;p&gt;Challenges in Data-to-Document Generation, Wiseman et al., EMNLP 2017. &lt;a href=&quot;https://arxiv.org/abs/1707.08052&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;
&lt;p&gt;논문의 핵심은 1) 새로운 data-to-text generation task를 제안, 2) baseline 모델 제안, 3) RotoWire dataset 배포, 4)  extractive evaluation 방법을 제안한 것이다. Table과 같은 structured data에서 natural language를 만들어 내려면 1) what to say, 2) how to say 두 가지 문제를 해결해야하는데, 이를 위해 neural network 기반의 end-to-end 모델을 사용한다. 제안한 여러 가지 Baseline 중 가장 성능이 높았던 것은 Encoder-Decoder + copy mechanism 모델에 additional reconstruction loss를 사용한 방법이다.&lt;/p&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Encoder-Decoder&lt;/em&gt;&lt;/strong&gt;{
    &lt;strong&gt;&lt;em&gt;Embedding&lt;/em&gt;&lt;/strong&gt;:  simple look-up table&lt;br /&gt;
    &lt;strong&gt;&lt;em&gt;Encoder&lt;/em&gt;&lt;/strong&gt;: 별다른 처리 없이 embedding을 쌓아두기&lt;br /&gt;
    &lt;strong&gt;&lt;em&gt;Decoder&lt;/em&gt;&lt;/strong&gt;:  RNN&lt;br /&gt;
    &lt;strong&gt;&lt;em&gt;Generator&lt;/em&gt;&lt;/strong&gt;:  Copy mechanism(joint copy or conditional copy)
    }&lt;/p&gt;

&lt;h4 id=&quot;copy-mechanism&quot;&gt;Copy mechanism&lt;/h4&gt;

&lt;p&gt;사용된 모델은 기존에 제안된 모듈들 조합해서 만든 것이며 (성능 향상에) 핵심적인 부분은 copy mechanism이다. Copy mechanism은 decoder가 단어를 생성할 때 vocab에 등록된 단어 뿐만아니라 input으로 들어오는 source token도 그대로 가져다 쓸 수 있게 하자는 방법이다. 직관적으로 생각해봐도 Input table에 담긴 내용을 natural language로 만들어내야하는 task에 잘 맞는 기법이다. Table에있는 record 하나가 (Point, Russel Westbrook, 50)일 때 우리가 copy해오는 대상은 50, 즉 record의 ‘value’를 가져오는 것이다.
&lt;script type=&quot;math/tex&quot;&gt;p(\hat{y}_t | \hat{y}_{1:t-1}, s) = \sum_{z\in\left\{0,1\right\}}p(\hat{y}_t, z_t=z|\hat{y}_{1:t-1}, s)&lt;/script&gt;
Copy mechanism을 사용해서 단어 $\hat{y}&lt;em&gt;t$의 생성확률을 모델링한다는 것은 이 단어가 copy를 통해 만들어졌을 확률과 generation을 통해서 만들어졌을 확률을 구해서 더한다는 의미이다.  확률값 $p(\hat{y}_t, z_t=z|\hat{y}&lt;/em&gt;{1:t-1}, s)$을 구하는 방법으로는 &lt;strong&gt;joint copy model&lt;/strong&gt;과 &lt;strong&gt;conditional copy model&lt;/strong&gt;이 있다. 둘 다 decoder의 hidden state을 input으로 확률값을 output으로 하는 함수인데, 계산 과정에 차이가 있다. joint model은 $copy(\hat{y}&lt;em&gt;t)$와 $gen(\hat{y}_t)$를 구한 뒤 normalization term을 이용해 이를 확률값으로 바꾸는 반면, conditional model은 $p&lt;/em&gt;{copy}({\hat{y}&lt;em&gt;t})$와 $p&lt;/em&gt;{gen}({\hat{y}_t})$이라는 확률값에 $p(z_t=1), p(z_t=0)$이라는 일종의 weight를 이용해서 sum을 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/copy.jpg&quot; alt=&quot;copy&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;u&gt;same-sentence 제한&lt;/u&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;reconstruction-loss&quot;&gt;Reconstruction loss&lt;/h4&gt;

&lt;p&gt;decoder에 hidden state을 이용하여 input의 내용을 맞추는 task를 학습과정에 추가하는 방법이다. 매 time step에서 만들어지는 hidden state가 input에 대한 전역적인 정보를 다 담고 있게끔 도와준다고 이해할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;u&gt;segment into $\frac{T}{B}$..?&lt;/u&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;tvd-loss&quot;&gt;TVD Loss&lt;/h4&gt;

&lt;p&gt;제대로 안 나와 있음&lt;/p&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/wiseman_result.PNG&quot; alt=&quot;result&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;가장 높은 성능을 낸 건 conditional copy 모델이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;issue&quot;&gt;Issue&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;학습시 truncated BPTT를 사용한다. Summary 평균 길이가 300이 넘는데, 100을 기준으로 truncate를 한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;OpenNMT-py 코드를 많이 봐야할듯……..ㅠ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 10 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-07-10/Challenges-in-Data-to-Document-Generation/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-07-10/Challenges-in-Data-to-Document-Generation/</guid>
      </item>
    
      <item>
        <title>Dataloading with torchtext</title>
        <description>&lt;p&gt;Torchtext는 ‘파일 읽어오기 - tokenization - dataset split - build vocabulary -  numericalization - pretrained word embedding - batch iterator’ 일련의 과정을 편리하게 할 수 있도록 도와준다. Supervised learning할 때 torchtext를 사용하는 방법과 그 설명을 정리했다 &lt;a href=&quot;http://anie.me/On-Torchtext/&quot;&gt;외국 블로그&lt;/a&gt;글, torchtext 소스코드를 참고했다.&lt;/p&gt;

&lt;h2 id=&quot;torchtextdatafield&quot;&gt;torchtext.data.Field&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;는 data의 한 영역을 represent하는 데 쓰인다. Supervised learning을 위한 paired data의 경우 src문장과 trg문장이 하나의 파일에 저장되어 있는 경우가 많다. 이때,  tab등의 구분자로 나뉘는 각 열에 해당하는 게 &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;이다. &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;를 만들 때 인자로 넘기는 것들은 그 필드에 적용할 전/후처리 등에 대한 정보이다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;MAXLEN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SRC&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ReversibleField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'spacy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_first&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;include_lengths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;preprocessing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MAXLEN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;TGT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ReversibleField&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'spacy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_first&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;include_lengths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;preprocessing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MAXLEN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Include_lengths&lt;/code&gt;와 &lt;code class=&quot;highlighter-rouge&quot;&gt;batch_first&lt;/code&gt;는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Iterator&lt;/code&gt;가 batch tensor을 내뱉을 때 영향을 미치지만, 여기서부터 설정해줘야한다. Pytorch의 &lt;code class=&quot;highlighter-rouge&quot;&gt;pack_padded_sequence&lt;/code&gt;는 인자로 paddding된 batch와 batch내 example들의 길이 정보를 받기 때문에 &lt;code class=&quot;highlighter-rouge&quot;&gt;include_lengths=True&lt;/code&gt;가 필요하다. &lt;code class=&quot;highlighter-rouge&quot;&gt;batch_first&lt;/code&gt;는 선택사항이다. 텍스트 데이터를 다룰 때 최대 길이 설정도 사실상 필수인데, 여기서 lambda function을 통해 최대 길이를 넘는 부분은 잘라버린다. 이후 lambda function의 input으로 들어가는 x는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Example&lt;/code&gt;(=list of strings)이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spacy tokenizer 사용시 다른 기능은 load하지 않는 방법은 &lt;a href=&quot;https://spacy.io/usage/processing-pipelines#disabling&quot;&gt;여기&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;torchtextdatadataset&quot;&gt;torchtext.data.Dataset&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Dataset&lt;/code&gt;는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Example&lt;/code&gt; 단위로 정리된 데이터들을 갖는 클래스이다. 많은 경우에 로컬에 특정 형태로 저장된 데이터를 사용할텐데, 이 경우 사용해야하는 class는 &lt;code class=&quot;highlighter-rouge&quot;&gt;TabularDataset&lt;/code&gt;이다. Instance를 만든 뒤엔 인덱싱을 통한 &lt;code class=&quot;highlighter-rouge&quot;&gt;Example&lt;/code&gt; 접근(__getitem__), &lt;code class=&quot;highlighter-rouge&quot;&gt;len&lt;/code&gt; 메서드 사용(__len__), &lt;code class=&quot;highlighter-rouge&quot;&gt;Example&lt;/code&gt;에 대한 iteration(__iter__), &lt;code class=&quot;highlighter-rouge&quot;&gt;Example&lt;/code&gt;들의 특정 Field에 대한 iteration(__getattr__)을 지원한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TabularDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'./data/en-fr.toy.txt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tsv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                            &lt;span class=&quot;n&quot;&gt;fields&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'src'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SRC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tgt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TGT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corpus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split_ratio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TabularDataset&lt;/code&gt;이 &lt;code class=&quot;highlighter-rouge&quot;&gt;Example&lt;/code&gt;을 만들려면 데이터가 어떻게 생겼는지에 대한 정보, 즉 &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt; 정보가 필요하기에 instance를 만들 때 &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;를 dictionary혹은 list of tuples로 넘겨줘야한다. &lt;code class=&quot;highlighter-rouge&quot;&gt;TabularDataset&lt;/code&gt; instance를 train, valid, test로 나누는 건 &lt;code class=&quot;highlighter-rouge&quot;&gt;.split()&lt;/code&gt;라는 instance method를 이용한다. 만약 로컬에 있는 파일이 이미 split된 상태라면, &lt;code class=&quot;highlighter-rouge&quot;&gt;.splits()&lt;/code&gt;라는 class method를 통해 각각 읽어올 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/text/issues/375&quot;&gt;두 Dataset을 병합하는 방법&lt;/a&gt;. 만약 데이터가 label 별로 다른 파일에 저장되어 있을 때 유용하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;torchtextdatafieldbuild_vocab&quot;&gt;torchtext.data.Field.build_vocab()&lt;/h2&gt;

&lt;p&gt;Torchtext에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Vocab&lt;/code&gt;은 &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;의 attribute이다. Torchtext는 하나의 &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;가 하나의&lt;code class=&quot;highlighter-rouge&quot;&gt;Vocab&lt;/code&gt;을 갖도록 만들어졌기 때문에(data가 갖는 게 아님!), 두 &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;가 &lt;code class=&quot;highlighter-rouge&quot;&gt;Vocab&lt;/code&gt;을 공유해야하는 경우엔 약간의 hack이 필요하다. 기계 번역의 경우엔 일반적으로 &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;마다 &lt;code class=&quot;highlighter-rouge&quot;&gt;Vocab&lt;/code&gt;이 필요하겠지만, src와 trg의 언어가 같은 대화 데이터 같은 경우엔 &lt;code class=&quot;highlighter-rouge&quot;&gt;Vocab&lt;/code&gt;을 공유할 필요가 있다. 그럴 경우, &lt;a href=&quot;https://github.com/pytorch/text/issues/232&quot;&gt;임의의 &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;가 &lt;code class=&quot;highlighter-rouge&quot;&gt;Vocab&lt;/code&gt;을 갖도록 하고,  만들 때 src와 trg의 단어를 모두 사용하도록 한다.&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;SRC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Dataset&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;TGT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tgt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tgt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 'Example의 특정 Field 내용을 반환하는 iterator'&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# SRC.build_vocab(train.src, train.tgt, val.src, val.tgt) # possible&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;무슨 데이터를 사용해서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Vocab&lt;/code&gt;을 만들지를 알려주기 위해 &lt;code class=&quot;highlighter-rouge&quot;&gt;.build_vocab()&lt;/code&gt;의 인자로 &lt;code class=&quot;highlighter-rouge&quot;&gt;Dataset&lt;/code&gt; 혹은 ‘&lt;code class=&quot;highlighter-rouge&quot;&gt;Example&lt;/code&gt;의 특정 &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;에 대한 iterator’를 넣어준다. 만약 pretrained vector를 다운로드 해서 쓸 거라면 여기서 vectors 인자를 넘겨야한다. 안 쓰는 경우엔 train, valid 데이터만 넣어줘서 test data 대한 정보 없이 vocab을 구성하는 것이 맞다. 하지만 pretrained vector을 쓸 거라면 train, valid, test 데이터 모두 넣어줘야한다(‘vocabulary expansion’). Train, valid에 등장하지 않았더라도 pretrained vector에 있는 단어라면 vocab에 포함시켜야한다는 것이다. 그러한 단어들은 어차피 train 때 추가적으로 학습되지 않고 기존 pretrained 값을 유지한다.&lt;/p&gt;

&lt;h2 id=&quot;torchtextdataiterator&quot;&gt;torchtext.data.Iterator&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Iterator&lt;/code&gt;는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Example&lt;/code&gt;을 모아 만든 &lt;code class=&quot;highlighter-rouge&quot;&gt;Batch&lt;/code&gt;를 반복적으로 내뱉는 역할을 담당한다.  텍스트 데이터를 다룰 때 padding을 최소화하기 위해서 bucketing을 하는데, &lt;code class=&quot;highlighter-rouge&quot;&gt;BucketIterator&lt;/code&gt;가 이를 담당한다. &lt;code class=&quot;highlighter-rouge&quot;&gt;Dataset&lt;/code&gt;에 들어있는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Example&lt;/code&gt;은 list of string이다. &lt;code class=&quot;highlighter-rouge&quot;&gt;Iterator&lt;/code&gt;는 string를 index로 바꿔서(numericalize), &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.LongTensor&lt;/code&gt;로 내뱉는다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;train_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BucketIterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                                     &lt;span class=&quot;n&quot;&gt;sort_key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                                     &lt;span class=&quot;n&quot;&gt;sort_within_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                     &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                                     &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                                     &lt;span class=&quot;n&quot;&gt;repeat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# sort_key는 Dataset안의 Example에 대해서&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# repeat=False 시 1 epoch 돌고 iter 끝남&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# sort_within_batch=True for pack_padded_sequence&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;.splits()&lt;/code&gt;의 인자로 shuffle 값을 주지 않으면 train iterator는 &lt;code class=&quot;highlighter-rouge&quot;&gt;shuffle=True&lt;/code&gt;가 된다. 이는 train할 때 batch들이 random하게 뽑히는 걸 보장한다. &lt;code class=&quot;highlighter-rouge&quot;&gt;batch_size_fn&lt;/code&gt;인자를 주면 &lt;code class=&quot;highlighter-rouge&quot;&gt;Batch&lt;/code&gt;구성 기준을 &lt;code class=&quot;highlighter-rouge&quot;&gt;Example&lt;/code&gt;개수에서 다른 것(e.g. token 개수)으로 바꿀 수 있다. 만약 GPU가 한 번에 몇 token까지 처리할 수 있는지를 알고있다면 cumstom &lt;code class=&quot;highlighter-rouge&quot;&gt;batch_size_fn&lt;/code&gt;을 사용하는 게 GPU를 최대한으로 이용할 수 있는 방법이다.&lt;/p&gt;

&lt;p&gt;모델 훈련 부분의 코드는 이렇게 구성할 수 있다. Iterator에서 repeat=False를 해줬기에 다음과 같이 epoch에 대한 for문을 사용할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;EPOCH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EPOCH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# repeat=False 필요&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;src_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tgt_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tgt&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# train for one epoch&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# record valid performance&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;template&quot;&gt;Template&lt;/h2&gt;

&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;is_target: Batch내에서 iterate할 때 target field에 해당하는 값이 나중에 나오도록&lt;/li&gt;
  &lt;li&gt;ReversibleField 사용법?&lt;/li&gt;
  &lt;li&gt;이미 있는 pretrained vector파일 불러와서 적용하는 방법은?&lt;/li&gt;
  &lt;li&gt;vocab 저장&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 23 Feb 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-02-23/Dataloading-with-torchtext/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-02-23/Dataloading-with-torchtext/</guid>
      </item>
    
      <item>
        <title>A Deep Generative Framework for Paraphrase Genertion</title>
        <description>&lt;p&gt;LearA Deep Generative Framework for Paraphrase Generation,  Gupta et al., ACL 2017. &lt;a href=&quot;https://arxiv.org/abs/1709.05074&quot;&gt;paper&lt;/a&gt; &lt;a href=&quot;https://github.com/hwijeen/VAE-LSTM&quot;&gt;my implementation&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT&lt;/h2&gt;

&lt;p&gt;Conditional generation의 일종인 paraphrase generation을 하기 위해서 CVAE를 사용했다. 기존의 sequence to sequence 모델에 비해 ‘principled generative framework’를 가진다는 게 장점이다. 또한, one-to-many generation을 하려면 CVAE 모델이 더 적합하다.&lt;/p&gt;

&lt;h2 id=&quot;how&quot;&gt;HOW&lt;/h2&gt;

&lt;p&gt;Training objective는 CVAE에서 다루는 것과 동일하다. Variational 분포에서 뽑은 $z​$ 를 사용한 reconstruction loss를 최소화하면서, 그 분포가 최대한 prior에 가깝게 하는 것이다. 위 논문과 차이점은 &lt;strong&gt;prior network를 unconditional distribution으로 사용한다는 것&lt;/strong&gt;! CVAE를 처음 제안한 Sohn et al.,에 따르면 이는 선택 사항이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The prior of the latent variable $z$ is modulated by the input $x$ in our formulation; however, the constraint can be easily relaxed to make the latent variables statistically independent of input variables, i.e., $p_\theta(z|x) =p_\theta(x)$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(\theta, \phi; x^p, x^o) = \mathbb{E}_{q_{\phi}(z|x^o, x^p)}[\log{p_\theta(x^p | x^o, z)}] - KL(q_{\phi}(z|x^o, x^p)||p(z))&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/gupta2018.PNG&quot; alt=&quot;VAE-LSTM&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recognition network $q_\phi(z|x^o, x^p)​$ 표현에는 두 개의 LSTM을 사용한다. 첫 번째 LSTM은 $x^o​$을 인코딩하고, 마지막 hidden state를 두 번째 LSTM으로 넘긴다. 두 번째 LSTM은 $x^p​$을 인풋으로 받는다. 마지막 hidden state는 MLP를 통과해 $q_\phi(z|x^o, x^p)​$의 파라미터를 내뱉는다. Decoder에서도 동일한 방식으로 두 개의 LSTM이 사용된다. Reparameterization trick을 통해 recongition network에서 sampling된 $z​$는 decoder의 두 번째 LSTM($x^p​$를 reconstruction 하는 LSTM)에서 매 time step 입력으로 들어간다. (Encoder와 decoder 모두에 $x^o​$인코더가 존재하는 셈인데, 이를 다른 LSTM으로 구성했을 때 성능이 더 좋았다고 한다.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generation LSTM의 input으로 $z$가 어떻게 들어가지? $w_{e1}^p$도 들어가야할텐데? 더 할까 concat할까&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 20 Feb 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-02-20/A-Deep-Generative-Framework-for-Paraphrase-Generation/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-02-20/A-Deep-Generative-Framework-for-Paraphrase-Generation/</guid>
      </item>
    
      <item>
        <title>Learning Discourse-level Diversity for Neural Dialog Models Using Conditional Variational Autoencoders</title>
        <description>&lt;p&gt;Learning Discourse-level Diversity for Neural Dialog Models Using Conditional Variational Autoencoders, Zhao et al., ACL 2017. &lt;a href=&quot;https://arxiv.org/abs/1703.10960&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what&quot;&gt;WHAT&lt;/h2&gt;

&lt;p&gt;기존의 encoder-decoder기반 대화 모델은 일반적이고 뻔한 대답을 내뱉는다는 문제가 있다. 이 문제를 해결하기 위해서 CVAE 기반의 모델을 제안한다. 이 모델의 아이디어는 대화를 one-to-many 문제로 보자는 것이다. Dialog level의 정보를 잡아내는 latent variable을 도입함으로써 다양한 발화를 생성할 수 있는 모델을 개발했다.&lt;/p&gt;

&lt;h2 id=&quot;how-modeling&quot;&gt;HOW: modeling&lt;/h2&gt;

&lt;p&gt;대화를 다음 세 가지 확률 변수로 모델링한다. Dialog context $c$, response utterance $x$, latent variable $z$. 이러한 확률 변수를 가지고 정의하는 분포는 아래와 같다.
&lt;script type=&quot;math/tex&quot;&gt;p(x, z|c) = p(z|c)p(x|z,c)&lt;/script&gt;
뉴럴넷의 용어로 표현하면 $p(z|c)$는 (conditional) &lt;strong&gt;prior network&lt;/strong&gt;, $p(x|z,c)$는 &lt;strong&gt;response decoder&lt;/strong&gt;이다. 뉴럴넷 파라미터는 $p(x,z|c)$를 최대화하도록 학습된다. 논문 표현에 따르면 이 방법은 maximizing the conditional likelihood of x given c이다. 통계 문헌에서 얘기하는 파라미터 추정 방법중 하나인 marginal likelihood of -parameter를 최대화하는 방법을 일컫는 것 같다.($p(x|c) = \int p(x|z,c)p(z|c)dz$)&lt;/p&gt;

&lt;p&gt;Test time때는 prior network에서 $z$를 sampling한 뒤, 이를 response decoder에 집어넣어 대화를 생성해낸다. 이때, sampling한 $z$값에 따라 생성되는 대화가 달라진다. &lt;strong&gt;One-to-many&lt;/strong&gt;!&lt;/p&gt;

&lt;h2 id=&quot;how-training&quot;&gt;How: training&lt;/h2&gt;

&lt;p&gt;$p(x|c)$를 직접 optimize하는 건 intractable하기 때문에 이의 lower bound = ELBO = variational enery를 objective function으로 삼아 학습을 한다. Approximation하는 거다. 통계 용어로 variational distribution = 뉴럴넷 용어로 &lt;strong&gt;recognition network&lt;/strong&gt; $q(z|x,c )$ 를 도입하면 variational lower bound 식을 도출할 수 있다. 여기서 $q(z|x,c)$와 $p(z|c)$가 정규분포를 따른다고 가정하면 아래 식을 깔끔하게 계산까지 할 수 있다.
&lt;script type=&quot;math/tex&quot;&gt;P(x|c) \geq \mathbb{E}_{q(z|x,c)}{p(x|z,c)} - KL(q(z|x,c) || p(z|c))&lt;/script&gt;
Objective function을 최대화 하는 과정을 말로 해석하면 첫번째 term은 recognition network에서 샘플링한 $z$로 계산한 likelihood를 높이기, 두 번째 term은 recognition network가 prior network와 비슷하도록 하기 정도겠다. 이는 한 단계 위에서(?) 보면 recognition network로 posterior distribution를 근사하는 과정으로 해석할 수 있다&lt;a href=&quot;https://www.edwith.org/bayesiandeeplearning/lecture/25284/&quot;&gt;(해석 설명)&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;VAE가 CVAE로 진화하면서 생긴 차이점은 prior가 conditional distribution이 된다는 점이다. VAE에서 prior distribution은 고정이었지만 CVAE에선 condition으로 무엇이 주어지냐에 따라 바뀌게 되고, 파라미터 추정의 대상이 된다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;조건부 확률분포를 뉴럴넷으로 표현할 때 사용하는 방법들: Utterance encoder는 BiGRU, context encoder는 GRU, response decoder은 GRU이다. Encoder가 내뱉는 $\sigma, \mu$로부터 sampling(through reparameterization trick)된 벡터를 response decoder의 initial hidden state로 삼음으로써 $(x|z, c)$라는 condition을 모델링한다.&lt;/p&gt;

&lt;p&gt;이 방법은 vanishing latent variable problem문제가 발생한다. Decoder를 RNN계열로 구성해버리면 backprop시 encoder까지 loss가 잘 전파되지 않고, 결국 encoder가 $z$에 유용한 정보를 encode하지 못하는 상황을 일컫는 것 같다.  VAE를 text에 처음 적용한 &lt;a href=&quot;https://arxiv.org/pdf/1511.06349.pdf&quot;&gt;Bowman et al.,2015&lt;/a&gt;에서 이 문제에 대한 해결책을 제시한 바 있지만 여기서는 또 다른 해결책을 제시한다. Bag-of-word loss를 도입하는 것이다. Encoding된 $z$가 순서를 보존한 원래 문장을 복원할 뿐만 아니라 bag-of-word까지 맞춰야하는 task까지 풀게 하는 것이다.
&lt;script type=&quot;math/tex&quot;&gt;p(x,z|c) = p(x_{original}|z,c)p(x_{bow}|z,c)p(z|c)&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{when } x_o \text{ and } x_{original} \text{ are conditionally indep given z,c}&lt;/script&gt;

</description>
        <pubDate>Tue, 19 Feb 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-02-19/Learning-Discourse-level-Diversity-for-Neural-Dialog-Models-Using-Conditional-Variational-Autoencoders/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-02-19/Learning-Discourse-level-Diversity-for-Neural-Dialog-Models-Using-Conditional-Variational-Autoencoders/</guid>
      </item>
    
      <item>
        <title>VAEs in NLP</title>
        <description>&lt;h2 id=&quot;vae-cvae-why-not-seq2seq&quot;&gt;VAE? CVAE? why not seq2seq?&lt;/h2&gt;

&lt;p&gt;VAE기반 모델의 장점은 one-to-many mapping이 가능하다는 것이다. 기존의 sequence to sequence 모델은 주어진 입력에 대한 deterministic representation을 만들고 이를 기반으로 디코더가 문장을 생성한다. VAE는 주어진 입력에 대한 representation을 확률분포로 정의하기 때문에, 샘플링을 통해 다양한 문장을 생성할 수 있다. CVAE는 VAE에서 발전된 모델이다. 두 모델 모두 generation 모델이지만, VAE는 무엇을 만들어낼 것이냐를 제어할 수 없다. CVAE는 대화 기록이 주어지고 다음 발화를 생성해라, 문장이 주어지고 paraphrase를 만들어내라와 같이 목적이 있는 문장 생성을 할 수 있다. &lt;a href=&quot;https://jaan.io/what-is-variational-autoencoder-vae-tutorial/&quot;&gt;VAE 개념 참고 블로그 글&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1812.06834&quot;&gt;A Tutorial on Deep Latent Variable Models of Natural Language&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1606.05908&quot;&gt;Tutorial on Variational Autoencoders&lt;/a&gt;, &lt;a href=&quot;http://ruishu.io/2018/03/14/vae/&quot;&gt;매우 잘 정리된 블로그&lt;/a&gt;에서 개념을 참고했다.&lt;/p&gt;

&lt;p&gt;이 포스트에서 정리할 논문들은 아래와 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf&quot;&gt;Sohn et al., 2015&lt;/a&gt;: CVAE를 처음 제안. 이론적 배경이 탄탄하다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.06349&quot;&gt;Bowman et al., 2015&lt;/a&gt;: VAE를 NLP에 처음 적용한 논문. Training difficulty에 대한 대한 대처법을 잘 고안했다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.10960&quot;&gt;Zhao et al., 2017&lt;/a&gt;: CVAE를 dialog에 적용했다.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1709.05074&quot;&gt;Gupta et al., 2018&lt;/a&gt;: CVAE를 사용해 paraphrase generation을 했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vae-motivations-and-ideas&quot;&gt;VAE: motivations and ideas&lt;/h2&gt;

&lt;p&gt;Generative 모델은 $p(x)$를 찾는 것을 목표로 한다. $p(x) $를 알고있다면 sampling을 통해 data를 만들어낼 수 있기 때문에 generation model이라고 할 수 있다. 일반적으로 generation model은 1 strong assumption about the structure in data, 2 severe approximation, 3 rely on computationally expesive inference procedure가 단점으로 꼽힌다. VAE의 장점은 이러한 단점을 극복하는 generation 모델이라는 것이다. VAE는 data structure에 대한 가정이 적고 backpropagation을 통한 빠른 훈련이 가능하다. VAE도 물론 approximation을 사용하지만 그 error가 적다.&lt;/p&gt;

&lt;p&gt;VAE는 latent variable model이다. Latent variable $z​$가 도입된 VAE를 plate notation으로 표현하면 다음과 같다. &lt;a href=&quot;&quot;&gt;!그림 - x 하나마다 z하나씩 가지고 있음&lt;/a&gt;. 이러한 latent variable model은 실제 data를 만들어내는 process를 더 잘 표현할 수 있다는 장점이 있다. 통계적으로 볼땐, 하나의 variable을 표현하는 distribution보다 더 complext한 distribution을 표현할 수 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Maching learning의 궁긍적인 목표는 train data와 test data뿐만 아니라 data 전체를 만들어내는 true data distribution $p^*(x)​$를 찾는 것이다. 그런데 우리가 가진 건 여기서 나온 몇 개의 sample들(train data)뿐이다. 이런 상황에서 우리가 할 수 있는 건 이 sample들이 구성하는 empirical distribution $\hat{p}(x)​$를 찾는 것이다. 찾는다는 것은 여러 개의 후보 distribution set을 구성하고  empirical distribution $\hat{p}(x)​$과 가장 가까운 $p(x)​$를 고르는 과정으로 이해할 수 있다. 가깝다 멀다 기준을 KL divergence로 잡는 경우가 MLE estimation을 하는 것이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) = \int p(x|z;\theta)p(z)dz&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Latent variable을 도입해서 likelihood를 위와 같이 전개하면 두 가지 문제를 해결해야한다. 1 $p(z)$를 어떻게 설정해야하나? 2 integral을 어떻게 처리하나? 첫번 째 문제에 대한 VAE의 답은 unit gaussian으로 두자는 것이다. 그 이유는 any d-dimensional distribution can be approximated with a set of sampels from d-dimensional normal distribution. Universal functiona approximator인 neural network를 이용하면 unit gaussian에서 뽑은 sample들을 실제 우리가 필요한 latent variable들로 변환시킬 수 있다(VAE의 decoder $p(x&lt;/td&gt;
      &lt;td&gt;z)​$에서 초기 layer가 이러한 역할을 담당한다고 이해할 수 있다).&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Continuous variable $z$에 대한 integral은 computationally intractable하다. 그렇기 때문에 VAE는 integral을 포함하는 $p(x)$대신 계산 가능한 $p(x)$의 $ELBO$를 optimize하는 approximation방식을 선택한다. $p(x)$를 전개할 때 임의의 분포 $q(z&lt;/td&gt;
      &lt;td&gt;x)$를 도입하면 $ELBO$가 포함된 식을 전개할 수 있다. 이러한 $ELBO$를 optimize하는 과정은 variational inference를 수반한다. $ELBO$ objective를 들여다보면 autoencoder 구조가 녹아있음을 볼 수 있다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;space of $z$ from $q(z&lt;/td&gt;
        &lt;td&gt;x)$ should be smaller than that of $z$ from $p(z)$, so that expectation in $\mathbb{E}_{z \sim q(z&lt;/td&gt;
        &lt;td&gt;x)}[p(x&lt;/td&gt;
        &lt;td&gt;z)]$ is easily approximated with a few samples of $z$.&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;ELBO 식 전개&lt;/li&gt;
  &lt;li&gt;Reparameterization(conditional form of parametric distribution 만들기, gradient expectation안으로 넣기)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vae-vs-cvae&quot;&gt;VAE vs. CVAE&lt;/h2&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;CVAE와 VAE는 1) variational distribution, 2) prior distribution, 3)decoder network를 어떻게 구성하냐에서 차이를 보인다. CVAE에서 variational distribution은 $q_\phi(z&lt;/td&gt;
      &lt;td&gt;x, y)​$로 표현된다. 이때 $x, y​$는 데이터의 한 쌍을 의미한다. 여기서 중요한 점은 &lt;strong&gt;$z​$에 대한 variational distribution이 $x​$뿐만 아니라 $y​$에까지 conditioning된다는 것&lt;/strong&gt;이다. CVAE는 conditional marginal likelihood $p(y&lt;/td&gt;
      &lt;td&gt;x)​$를 최대화하는 방향으로 훈련이되는데, 이는 variatonal distribution $q_\phi(z&lt;/td&gt;
      &lt;td&gt;x, y)​$가 posterior distribution $p(z&lt;/td&gt;
      &lt;td&gt;x, y)​$에 근사되는 결과를 가져온다. 데이터가 $x, y​$  쌍으로 주어졌으니 때 posterior의 condition으로 $x, y​$가 들어가게되고, variational distribution은 posterior을 근사하기 위한 용도이니까 똑같이 $x, y​$에 대해 condition된다고 이해하면 맞는 걸까?&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;CVAE의 decoder network는 $p_\theta(y&lt;/td&gt;
      &lt;td&gt;z, x)$로 표현되고, VAE와의 차이는 &lt;strong&gt;decoder network가 original sentence $x$에 대해서도 conditioning된다는 것&lt;/strong&gt;이다. 이 차이 덕분에 test time때 input specific한 generation이 가능해지는 것이다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;todo&quot;&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;VAE 식 전개&lt;/li&gt;
  &lt;li&gt;What is bayesian in VAE?&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;cs231n이 설명하는 VAE에서 p(z&lt;/td&gt;
          &lt;td&gt;x)를 고려하는 이유?&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;test time때 p(z)에서 sampling하는 것의 근거?&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 18 Feb 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-02-18/VAEs-in-NLP/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-02-18/VAEs-in-NLP/</guid>
      </item>
    
      <item>
        <title>Matching Network for One Shot Learning</title>
        <description>&lt;p&gt;Matching Networks for One shot Learning , Vinayls et al., NIPS 2016. &lt;a href=&quot;https://arxiv.org/abs/1606.04080&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;preliminary&quot;&gt;Preliminary&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Few-shot learning: 적은 양의 데이터로부터 학습하기. 사람은 잘 하지만 딥러닝은 못하는 것의 대표적인 예시로 쓰인다.&lt;/li&gt;
  &lt;li&gt;N-way, k-shot learning task: few-shot learning의 성능을 측정하는 데 사용되는 대표적인 task. 학습시 주어지지 않은 class를 가지는 test example을 얼마나 잘 분류하는지 평가하는 task이다. 이때, classification은 N-way 분류 문제이고, 각 class마다 k개의 example들이 주어진다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what&quot;&gt;What&lt;/h2&gt;

&lt;p&gt;One-shot learning을 하는 모델과 training procedure을 제안한다. Deep learning은 많은 분야에서 뛰어난 성능을 보이고 있지만 엄청난 양의 데이터를 필요로 하는 단점이 있다. 이 논문은 그 원인이 딥러닝의 parametric한 측면, 즉 parameter들이 천천히 train example을 배워가는 데 있다고 본다. 제안모델은 attention을 사용하는 memory network로 classification 과정에서 파라미터가 필요없다는 특징을 가진다. Training procedure은 train시와 test시의 환경이 동일하도록 구성된다.&lt;/p&gt;

&lt;h2 id=&quot;how&quot;&gt;How&lt;/h2&gt;

&lt;p&gt;제안 모델은 one-shot learning을 set-to-set framework의 변형으로 설명한다. K개의 example 및 그 label로 이루어진  support set $S={(x_i, y_i)}$를 classifier $C_S(\hat{x})$로 mapping시키는게 목표이다. 이 classifier는 새로운 example $\hat{x}$가 주어졌을 때 어떤 class에 속하는지에 대한 확률분포 $\hat{y}$를 뱉어낸다.
&lt;script type=&quot;math/tex&quot;&gt;\hat{y} = \sum_{i=1}^{k}a(\hat{x}, x_i)y_i&lt;/script&gt;
 즉, test example의 label은 주어진 support set의 label의 조합으로 구성된다. $y_i$는 memory로 본다면 이 과정은 attention augmented memory network로 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;Attention은 임베딩된 두 이미지에 대해서 계산된다. Support set의 example에 대한 Embedding function을 $g(x_i)​$로 구성할 경우 support set내 각 이미지들은 서로에 대해 independent하게 벡터로 변환된다. 이로 인한 문제점은 support set사이에 유사하지만 다른 class를 갖는 이미지가 있더라도(e.g. 삽살개, 진돗개) $g​$는 두 이미지에 대해 유사한 벡터값을 부여한다는 것이다. 만약 embedding function을 $g(x_i, S)​$로 구성한다면 이를 해결할 수 있다. 주어진 support set을 고려해서 test example에 대한 임베딩을 만들 필요도 있는데, 이는 $f(\hat{x}, S) =attnLSTM(f’(\hat{x}, g(S), K) ​$와 같은 함수를 통해 실현 가능하다.&lt;/p&gt;

&lt;p&gt;Train과 test때 동일한 환경을 만들기 위한 목적함수는 아래와 같이 구성된다.
&lt;script type=&quot;math/tex&quot;&gt;\theta = argmax_{\theta}\mathbb{E}_{L \sim T}\Big[\mathbb{E}_{S\sim L, B \sim L}\Big[\sum_{(x,y)\in B}\log{P_\theta(y|x, S)}\Big]\Big]&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Memory network를 one-shot learning에 이용했다. Memory는 support set의 label들이며, 이 memory를 찾아오기 위해 attention을 사용한다.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 15 Feb 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-02-15/Matching-Network-for-One-Shot-Learning/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-02-15/Matching-Network-for-One-Shot-Learning/</guid>
      </item>
    
      <item>
        <title>Pytorch autograd</title>
        <description>&lt;p&gt;Pytorch 기본개념으로써 tensor, computational graph, autograd 등을 잘 설명해놓은 &lt;a href=&quot;https://www.kdnuggets.com/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html&quot;&gt;링크&lt;/a&gt;. 대충 듣던 단어들에 대해서 쉽고 자세하게 얘기해주는 글이다.&lt;/p&gt;

&lt;h2 id=&quot;tensors&quot;&gt;Tensors&lt;/h2&gt;

&lt;p&gt;딥러닝이 인기 몰이를 하기 전에는 배열 자료구조를 표현하기 위해 Numpy를 많이 썼다. Numpy는 주요 기능을 C로 구현해놨기에 Python의 list보다 빠르게 가볍다는 장점이 있지만, GPU에 올릴 수 없다는 치명적인 단점이 있다. GPU는 연산을 병렬적으로 처리하는 데 특화되어있는데, 엄청나게 많은 FLOP을 골자로 하는 딥러닝을 위해선 GPU에 올릴 수 있는 배열, Tensor가 필요하다.&lt;/p&gt;

&lt;p&gt;Pytorch tensor class의 대표적인 attribute는 data, grad, grad_fn이다. &lt;strong&gt;data&lt;/strong&gt;에는 배열의 원소에 해당하는 값이 저장되어있다. &lt;strong&gt;grad&lt;/strong&gt;는 떤 함수를 이 tensor에 대해서 미분한 뒤 evaluate 된 값을 가지고 있다. Backprop을 하기 전까진 None값이 들어있다. &lt;strong&gt;grad_fn&lt;/strong&gt;은 gradient computation 과정에서 호출되는 함수로써, “이 텐서가 무슨 operation을 통해 만들었는지”를 나타낸다.&lt;/p&gt;

&lt;p&gt;추가로 알아두어야할 attribute: requires_grad, is_leaf&lt;/p&gt;

&lt;h2 id=&quot;computation-graphs&quot;&gt;Computation Graphs&lt;/h2&gt;

&lt;p&gt;Computation graph는 이름에서 알 수 있듯이 계산 과정을 표현하는 하나의 자료구조이다. 다른 방식으로도 계산 과정을 표현할 수 있겠지만 효율적으로 chain rule을 적용하는데 특화된 건 computation graph이다.&lt;/p&gt;

&lt;p&gt;Pytorch의 computation graph를 그릴 때 각 노드에 실제로 들어가는 것은 tensor의 grad_fn이다. 모든 tensor는 grad_fn을 가지고 있기 때문에 각 노드에 간접적으로 해당하는 tensor가 있다고 생각할 수 있다. 맨날 하는 &lt;code class=&quot;highlighter-rouge&quot;&gt;loss.backward()&lt;/code&gt;는 크게 보면 다음과 같이 세 부분으로 구성된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;해당 node에서  grad_fn을 이용해 local gradient 계산&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;local gradient를 해당 노드(에 해당하는 tensor)의 grad와 곱하기(chain rule)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;계산된 gradient를 input node(에 해당하는 tensor)의 grad에 저장&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;마지막 node(loss) 부터 맨 앞 node(weight)까지 이 과정을 연쇄적으로 적용하면 gradient of loss w.r.t weight을 얻을 수 있다. 물론 여기서 말하는 gradient는 input을 이용해 evaluated된 값이다. 꼬리에 꼬리를 무는 computational graph가 있기에 ‘연쇄적’인 chain rule 적용이 가능한 것이다.&lt;/p&gt;

&lt;p&gt;Computation graph는 backprop을 하기 위해 필요한 자료구조라고 했다. 그런데 inference시에는 backprop이 필요하지 않다. &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.no_grad()&lt;/code&gt;라는 context manager 아래에 inference code를 쓴다는 것은 computation graph자체를 안 만들겠다는 얘기다. 그러면 당연히 time, space가 절약되니까. 이 Context manage 아래서 결과로 나온 tensor는 항상 requires_grad=False이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Pytorch는 torch.autograd.Function이라는 class로 함수를 관리한다. 이 class는 method로 forward와 backward를 갖는다. 굳이 클래스를 만들어 두 함수를 묶어두는 이유는 backprop과정에서 두 함수가 공유해야할 변수가 있기 때문이다. Pytorch computation graph의 한 node에서 local gradient를 evaluate하기 위해선 backward함수가 forward 함수의 input을 알아야하는 경우가 그렇다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Pytorch는 dynamic graph 방식, Tensorflow는 static graph 방식이라고들 한다. Dynamic하다는 것은 forward할 때마다 computation 그래프를 새롭게 그린다는 것이다. Model이 있고 거기에 input이 주어질 때마다 forward pass를 할 텐데, 그때마다 다른 computation graph를 그린다. 이런 방식의 장점은 dynamic하게 조작(원한다면 매 forward마다 모델이 다르게 작동하도록 코딩 하기)이 가능하다는 것이다. 디버그도 훨씬 쉽다. Forward때마다 만들어지는 computational graph는 .backward()가 완료될 때 메모리에서 지워진다. Leaf node에 해당하는 weight의 .grad를 계산 및 저장하고 나면 intermediate node의 grad 및 computational graph자체가 지워진다. 그러니까 forward-backward의 반복은 computational graph관점에서는 만들었다 지우기의 반복이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Torch.autograd.grad함수를 사용하면 &lt;code class=&quot;highlighter-rouge&quot;&gt;loss.backward()&lt;/code&gt;가 한정하는 방식 말고도 좀 더 자유로운 미분을 할 수 있다. 예를 들면 함수를 미분하는데 특정 하나의 변수로만 미분하기 같은 계산!&lt;/p&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Wed, 30 Jan 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-01-30/Pytorch-autograd/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-01-30/Pytorch-autograd/</guid>
      </item>
    
      <item>
        <title>Data parallelism, multi-GPU</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html&quot;&gt;파이토치 튜토리얼1&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html&quot;&gt;튜토리얼2&lt;/a&gt;, &lt;a href=&quot;http://nlp.seas.harvard.edu/2018/04/03/attention.html#batches-and-masking&quot;&gt;transformer 코드&lt;/a&gt;, &lt;a href=&quot;https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255?source=user_profile---------2------------------&quot;&gt;짱짱 블로그&lt;/a&gt;: 짱짱 블로그 좀 더 보고 공부하기&lt;/p&gt;

&lt;h2 id=&quot;torchnndataparallel&quot;&gt;torch.nn.DataParallel&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.DataParallel&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=dataparallel#torch.nn.DataParallel&quot;&gt;implements data parallelism at the module level.&lt;/a&gt; 여기서 말하는 모듈은 &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt; 을 의미한다. Pytorch로 구현하는 딥러닝 모델은 보통 &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt;에서 상속을 받기 때문에, DataParallel을 통해 간단하게 여러 대의 GPU를 사용할 수 있다. 아래 코드 예시에 나와있듯이, 모듈 레벨에서 data parallelism을 하기 위해서는 한 줄 의 코드만 더하면 된다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;DEVICE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'cpu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# arbitrary layer&lt;/span&gt;
     
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# instantiate a model as usual&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataParallel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# as simple as this!&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DEVICE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;shell에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;nvidia-smi&lt;/code&gt;를 실행시켜서 GPU사용량을 확인해보면 병렬처리가 잘 작동하는지 확인할 수 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;nn.utils.rnn.pack_padded_sequence와 nn.DataParallel을 사용할 경우 오류가 날 수 있다. 배치를 쪼개서 각 GPU로 보내는데, 이때 각 배치에 올라가는 텐서들은 max batch len을 모르고 있기에. &lt;a href=&quot;https://pytorch.org/docs/stable/notes/faq.html#pack-rnn-unpack-with-data-parallelism&quot;&gt;Pytorch FAQ&lt;/a&gt;를 따라하면 오류를 피해갈 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;nnparallel&quot;&gt;nn.parallel&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.DataParallel&lt;/code&gt;은 내부적으로 nn.parallel 모듈에 정의되어 있는&lt;code class=&quot;highlighter-rouge&quot;&gt;replicate&lt;/code&gt;,  &lt;code class=&quot;highlighter-rouge&quot;&gt;scatter&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;parallel_apply&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;parallel_gather&lt;/code&gt;함수를 사용한다. nn.DataParallel을 사용해서 간단히 여러 대의 GPU를 사용할 수 있지만, 여러 부분에서(예를 들어 loss computation 부분) 병렬 처리를 하고 싶으면 이 함수들도 익혀둘 필요가 있다. 각각의 역할을 말로 하면 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;replicate(nn.Module:module, list:devices_ids)&lt;/code&gt; : 인자로 주어진 모듈을 &lt;strong&gt;복사&lt;/strong&gt;해서 device_ids로 받은 GPU에 할당한다. 즉, 하나의 모델을 여러 GPU에 올린다. 이 함수는 복사된 모듈이 담긴 리스트를 반환한다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;scatter(torch.Tensor:input, list:device_ids)&lt;/code&gt;: 인자로 주어진 텐서를 &lt;strong&gt;쪼개서&lt;/strong&gt; 여러 GPU로 보낸다. 텐서를 쪼갠다는 것의 의미는 한 batch내에 있는 여러 개의 example을 device_ids로 주어진 GPU 개수로 나눈다는 것이다. 이 함수는 쪼갠 example들의 tuple을 반환한다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;parallel_apply(replicated_modules, scatterd_input)&lt;/code&gt;: 실제로 병렬 연산을 해주는 부분이다. 쪼개진 데이터(subset of batch)는 각각 모델에 들어가서 결과값을 낸다. 반환되는 것은 결과값이 담긴 list다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gather(parallel_apply_output, target_device)&lt;/code&gt;: 병렬 처리된 결과값들을 하나의 텐서로 합쳐주는 부분이다. target_device인자로 넘겨준 GPU에 최종 결과가 안착(?)하게 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;p&gt;Havard NLP의 transformer 구현중 일부분이다. &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.DataParallel&lt;/code&gt;은 물론 사용했고, loss 계산 부분까지 병렬처리를 하고자 아래와 같이 코드를 작성했다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;self.criterion&lt;/code&gt;으로 들어가는 것은 loss를 계산하는 클래스이다. &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.NLLLoss&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.KLDivLoss&lt;/code&gt;등은 &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Module&lt;/code&gt;을 상속받아 작성된 객체여서 &lt;code class=&quot;highlighter-rouge&quot;&gt;replicate&lt;/code&gt;의 인자(module)로 들어갈 수 있다!&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MultiGPULossCompute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;A multi-gpu loss compute and train function.&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chunk_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# Send out to different gpus.&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                               &lt;span class=&quot;n&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;devices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;devices&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chunk_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chunk_size&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__call__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replicate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                                &lt;span class=&quot;n&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out_scatter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                          &lt;span class=&quot;n&quot;&gt;target_gpus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                      &lt;span class=&quot;n&quot;&gt;target_gpus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# Divide generating into chunks.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;chunk_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chunk_size&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chunk_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# Predict distributions&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;out_column&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chunk_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                    &lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; 
                           &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;gen&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallel_apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_column&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# Compute loss. &lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; 
                  &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chunk_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; 
                 &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallel_apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# Sum and normalize loss&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                   &lt;span class=&quot;n&quot;&gt;target_device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# Backprop loss to output of transformer&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;out_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_column&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# Backprop all loss through transformer.            &lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;out_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;og&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;og&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;o1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;o2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                    &lt;span class=&quot;n&quot;&gt;target_device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;o1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Mon, 05 Nov 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018-11-05/Data-parallelism,-multi-GPU/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-11-05/Data-parallelism,-multi-GPU/</guid>
      </item>
    
      <item>
        <title>Mutable과 immutable</title>
        <description>&lt;h2 id=&quot;background-변수와-객체&quot;&gt;Background: 변수와 객체&lt;/h2&gt;

&lt;p&gt;“파이썬에서는 모든 것이 객체이다.” 객체라는 건 메모리 위에 저장된 개별 데이터를 의미한다. 정수 4, 문자열 hi 등 모든 데이터를 객체라고 생각하면 된다. (같이 자주 등장하는 단어인 &lt;a href=&quot;https://python.bakyeono.net/chapter-8-2.html&quot;&gt;클래스는 데이터의 ‘유형’을 나타낸다&lt;/a&gt;). 컴퓨터 메모리에 할당되어서 저장되는 건 객체 그 자체이다.&lt;/p&gt;

&lt;p&gt;변수는 객체의 위치(정확한 메모리 주소는 아니지만 객체의 고유 번호)를 가리킨다. 객체의 위치를 가리키게 하는 과정을 &lt;em&gt;‘바인딩’&lt;/em&gt;이라고 부른다. (마치 C언어의 포인터마냥..)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# python&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 22222라는 객체를 생성하고, 변수 a에 바인딩한다&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;22222&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;“파이썬에서는 모든 것이 객체이다”라는 말을 강조하는 이유는, 다른 언어에는 모든 것이 객체이지 않아서이다. 예를 들어, 아래와 같은 C언어에서는 22222가 객체가 아니다. 메모리에 할당되는 것은 int 타입의 변수 a이고, a 안에 22222이라는 값이 &lt;em&gt;대입&lt;/em&gt;된다. (변수 a의 주소를 가리키는 것은 포인터다.)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;// C
int a = 22222
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;mutable-vs-immutable&quot;&gt;Mutable vs. Immutable&lt;/h2&gt;

&lt;p&gt;Mutable과 immutable은 &lt;strong&gt;객체&lt;/strong&gt;의 종류를 의미한다. Mutable은 값을 수정할 수 있는, immutable은 그렇지 못한 객체를 의미한다. 기술적으로 말하자면 mutable 객체는 &lt;code class=&quot;highlighter-rouge&quot;&gt;__setattr__&lt;/code&gt;이라는  special method가 정의되어 있고, immutable은 그렇지 않다.&lt;/p&gt;

&lt;p&gt;자주 사용하는 파이썬 기본 자료형중 &lt;code class=&quot;highlighter-rouge&quot;&gt;string&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;tuple&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;int&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;float&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;bool&lt;/code&gt; 등이 immutable에 속한다. Mutable은 &lt;code class=&quot;highlighter-rouge&quot;&gt;list&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;dict&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;set&lt;/code&gt;을 포함하며, &lt;strong&gt;사용자가 정의한 class도 기본적으로는 mutable&lt;/strong&gt;로 생성된다.&lt;/p&gt;

&lt;h2 id=&quot;call-by-object-reference&quot;&gt;“Call by object reference”&lt;/h2&gt;

&lt;p&gt;객체가 mutable한지 immutable한지가 중요해지는 것은 함수의 인자로 사용될 때다. &lt;a href=&quot;https://wayhome25.github.io/cs/2017/04/11/cs-13/&quot;&gt;‘call by value’냐 ‘call by reference’냐 관점&lt;/a&gt;으로 본다면, 함수의 인자로 들어온 객체가 mutable냐 immutable냐에 따라 다르다.  아래 예시에서와 같이 &lt;strong&gt;인자가 mutable일 경우 call by reference처럼 동작&lt;/strong&gt;하므로 주의해야한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;append_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;some_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;some_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'end'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;some_list&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;my_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'hi'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'there'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# ['hi', 'there', 'end']&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# ['hi', 'there', 'end', 'end']&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://item4.github.io/2015-07-18/Some-Ambiguousness-in-Python-Tutorial-Call-by-What/&quot;&gt;더 좋은 정리는 python의 함수는 ‘call by object reference’를 따른다는 것이다.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/31435603/python-modify-global-list-inside-a-function&quot;&gt;global 관련해서 헷갈릴 수 있는 부분&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;immutable-as-a-default-function-parameter&quot;&gt;Immutable as a default function parameter&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_func1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_func2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.python-guide.org/writing/gotchas/&quot;&gt;mutable 객체를 함수의 default인자로 사용하는 것을 지양해야한다.&lt;/a&gt; mutable default argument는 함수가 정의되는 시점에서 단 한 번만 evaluate된다. 그러니가 &lt;code class=&quot;highlighter-rouge&quot;&gt;my_func2&lt;/code&gt;에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;args=[]&lt;/code&gt;는 여러 번 호출될 때엔 적용되지 않는다는 얘기다. args를 사용해서 caching을 할 것이 아니라면 &lt;code class=&quot;highlighter-rouge&quot;&gt;my_func1&lt;/code&gt;과 같이 immutable을 default 인자로 삼아야한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TODO: 최상위 클래스 object의 정체? 이걸 상속받기 때문에 `&lt;strong&gt;setattr&lt;/strong&gt;‘이 생기는 게 맞는지&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sat, 27 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018-10-27/Mutable%E1%84%80%E1%85%AA-immutable/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-10-27/Mutable%E1%84%80%E1%85%AA-immutable/</guid>
      </item>
    
      <item>
        <title>Iterable과 iterator</title>
        <description>&lt;h2 id=&quot;iterable&quot;&gt;Iterable&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;for item in iterable&lt;/code&gt;구문을 사용할 수 있는 객체들. 이러한 객체들은 스페셜 메서드 &lt;code class=&quot;highlighter-rouge&quot;&gt;__iter__&lt;/code&gt;이 정의되어 있으며, 이는 이터레이터를 반환한다. 파이썬 내장함수 &lt;code class=&quot;highlighter-rouge&quot;&gt;iter(iterable)&lt;/code&gt;을 통해서 이터러블의 이터레이터(?!)를 얻을 수 있다. 자주 사용하는 기본 자료형 중 &lt;code class=&quot;highlighter-rouge&quot;&gt;list&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;string&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;dictionary&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;set&lt;/code&gt;등이 이터러블이다.&lt;/p&gt;

&lt;p&gt;참고로, &lt;a href=&quot;https://soooprmx.com/archives/8007&quot;&gt;내장 함수 sum()은 모든 iterable에 대해서 동작한다.&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;iterator&quot;&gt;Iterator&lt;/h2&gt;

&lt;p&gt;Iterable객체가 &lt;code class=&quot;highlighter-rouge&quot;&gt;__iter__&lt;/code&gt;함수로 반환하는 것이 이터레이터라고 했다. 이 또한 하나의 객체이며, 스페셜 메서드 &lt;code class=&quot;highlighter-rouge&quot;&gt;__next__&lt;/code&gt;를 가지고 있다. 파이썬 내장 함수 next()를 통해서도 이 함수를 실행할 수 있다. 이 함수가 하는 일은 순차적으로 탐색하며 데이터를 하나씩 반환하는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;for item in iterable&lt;/code&gt; 구문을 사용하면 내부적으로는 iterable의 이터레이터 생성 및 next()함수 호출이 진행된다. (&lt;a href=&quot;https://soooprmx.com/archives/8007&quot;&gt;이는 C언어에서 for문이 작동하는 방식과 매우 다르다.&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;이터러블과 이터레이터는 다른 객체이지만, 경우에 따라서는 동일한 객체가 이터러블이면서 이터레이터로써 작동하게 할 수 있다. &lt;a href=&quot;https://anandology.com/python-practice-book/iterators.html&quot;&gt;하나의 클래스가 &lt;code class=&quot;highlighter-rouge&quot;&gt;__iter__&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;__next__&lt;/code&gt; 메서드를 둘 다 가지는 경우이다.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;참고로,&lt;a href=&quot;https://dojang.io/mod/page/view.php?id=1113&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;__next__&lt;/code&gt;대신 &lt;code class=&quot;highlighter-rouge&quot;&gt;__getitem__&lt;/code&gt;을 통해서 이터레이터를 만들 수도 있고&lt;/a&gt;, &lt;a href=&quot;https://dojang.io/mod/page/view.php?id=1112&quot;&gt;이터러블은 unpacking이 가능하다.&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;generator&quot;&gt;Generator&lt;/h2&gt;

&lt;p&gt;제너레이터는 이터레이터를 만드는 함수이다. 함수의 반환을 나타내는 return대신 yield가 쓰인다는 게 특징이다. yield문을 사용할 경우 파이썬이 함수 내용을 사용해서 제너레이터 객체를 만들어낸다. 직접 이터레이터 프로토콜을 정의하지 않았지만, next()함수를 통해 yield하는 값을 받아올 수 있다. 모든 데이터가 동시에 필요한 상황이 아니라면, 제너레이터를 사용해서 매 시점에 필요한 데이터만을 메모리로 로드하는 효율적인 프로그래밍이 가능하다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_generator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    	&lt;span class=&quot;k&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# yield 이후에 statement 추가 가능&lt;/span&gt;
    
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my_generator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# a is an generator = iterator&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Sat, 27 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018-10-27/Iterable%E1%84%80%E1%85%AA-iterator/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-10-27/Iterable%E1%84%80%E1%85%AA-iterator/</guid>
      </item>
    
      <item>
        <title>Vim cheat sheet</title>
        <description>&lt;h2 id=&quot;misc&quot;&gt;Misc&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;:ls &lt;span class=&quot;c&quot;&gt;# 현재 위치 확인 가능&lt;/span&gt;
:cd &lt;span class=&quot;c&quot;&gt;# 이걸로 cwd 바꾸기&lt;/span&gt;

:%s/&lt;span class=&quot;nv&quot;&gt;$before&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;$after&lt;/span&gt;/g &lt;span class=&quot;c&quot;&gt;# 파일 전체에서 단어 바꾸기&lt;/span&gt;
:s/&lt;span class=&quot;nv&quot;&gt;$before&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;$after&lt;/span&gt;/g &lt;span class=&quot;c&quot;&gt;# 현재 라인에서만&lt;/span&gt;

:set list &lt;span class=&quot;c&quot;&gt;# show white spaces&lt;/span&gt;
:set list! &lt;span class=&quot;c&quot;&gt;# disable&lt;/span&gt;
:set &lt;span class=&quot;nv&quot;&gt;listchars&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;tab:&amp;gt;&amp;gt;&amp;gt; &lt;span class=&quot;c&quot;&gt;# display tabs as &amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;

:sort u &lt;span class=&quot;c&quot;&gt;# sort and remove duplicates&lt;/span&gt;

:g/&lt;span class=&quot;nv&quot;&gt;$pattern&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# show lines with pattern&lt;/span&gt;
:g/&lt;span class=&quot;nv&quot;&gt;$pattern&lt;/span&gt;/d &lt;span class=&quot;c&quot;&gt;# delete lines with pattern&lt;/span&gt;
:g!/&lt;span class=&quot;nv&quot;&gt;$pattern&lt;/span&gt;/d &lt;span class=&quot;c&quot;&gt;# delete lines without pattern&lt;/span&gt;

:windo &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;scrollbind &lt;span class=&quot;c&quot;&gt;# https://stackoverflow.com/a/2986980&lt;/span&gt;
:windo &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;cursorbind
:syntax &lt;span class=&quot;nb&quot;&gt;sync &lt;/span&gt;fromstart &lt;span class=&quot;s2&quot;&gt;&quot; when syntax highlighting is broken
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;시스템 clipboard에 복사하려면 vim 8.1깔고 vim경로 그걸로 잡아주기&lt;/p&gt;

&lt;h2 id=&quot;vimrc&quot;&gt;.vimrc&lt;/h2&gt;

&lt;p&gt;이 파일로 vim 환경 설정을 한다. &lt;a href=&quot;https://vim-bootstrap.com/&quot;&gt;vim-bootstrap&lt;/a&gt;에서 .vimrc파일을 다운받으면 왠만한 건 다 깔려있다. 하나하나 파악하며 깔다간 정신 건강이 나빠진다.. Plugin manager은 &lt;a href=&quot;https://github.com/junegunn/vim-plug&quot;&gt;vim-plug&lt;/a&gt;를 쓴다. 아래는 내가 추가한 단축키.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;mouse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;a

&lt;span class=&quot;s2&quot;&gt;&quot;vim-conda
cnoreabbrev conda CondaChangeEnv
let g:conda_startup_msg_suppress = 1

&quot;&quot; Close buffer
noremap &amp;lt;leader&amp;gt;c :bd&amp;lt;CR&amp;gt;

&quot;&lt;/span&gt;buffer navigate
nmap &amp;lt;leader&amp;gt;l :bnext&amp;lt;CR&amp;gt;
nmap &amp;lt;leader&amp;gt;q :bp&amp;lt;CR&amp;gt;
nmap &amp;lt;leader&amp;gt;T :enew&amp;lt;CR&amp;gt;

&lt;span class=&quot;s2&quot;&gt;&quot;run python
nmap &amp;lt;F5&amp;gt; :!python %&amp;lt;CR&amp;gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Jedi-vim은 안 쓰는 게 나은듯?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tab-buffer-window&quot;&gt;Tab, buffer, window&lt;/h2&gt;

&lt;p&gt;tab은 작업 단위로 분리해서 쓰고, buffer은 파일을 불러와서 들고있는 공간이고, window는 하나의 버퍼를 보여주는 창이다!&lt;/p&gt;

&lt;h3 id=&quot;buffers&quot;&gt;buffers&lt;/h3&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;:e &lt;span class=&quot;c&quot;&gt;#edit a file&lt;/span&gt;
:ls &lt;span class=&quot;c&quot;&gt;# buffer list&lt;/span&gt;
:bn &lt;span class=&quot;c&quot;&gt;# next buffer on window&lt;/span&gt;
:bp &lt;span class=&quot;c&quot;&gt;# previous buffer on window&lt;/span&gt;
:badd &lt;span class=&quot;c&quot;&gt;# add a file to buffer without opening&lt;/span&gt;
:bd &amp;lt;BUFFER NAME&amp;gt; or &amp;lt;BUFFER NUMBER&amp;gt; &lt;span class=&quot;c&quot;&gt;# delete from buffer&lt;/span&gt;
:vs | &amp;lt;BUFFER NAME&amp;gt; or &amp;lt;BUFFER NUMBER&amp;gt; &lt;span class=&quot;c&quot;&gt;# to open a buffer in different window&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;tabs&quot;&gt;tabs&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;:tabe &lt;span class=&quot;c&quot;&gt;#edit a file in new tab&lt;/span&gt;
:tabclose &lt;span class=&quot;c&quot;&gt;# close current tab&lt;/span&gt;
:tabn &lt;span class=&quot;c&quot;&gt;# same as gt&lt;/span&gt;
:tabp &lt;span class=&quot;c&quot;&gt;# same as gT&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;vim.wikia.com/wiki/Using_tab_pages&quot;&gt;단축키 관련 참고&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;macro&quot;&gt;Macro&lt;/h2&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;qa &lt;span class=&quot;c&quot;&gt;# start recording in a&lt;/span&gt;
q &lt;span class=&quot;c&quot;&gt;# finish recording&lt;/span&gt;

@a &lt;span class=&quot;c&quot;&gt;# run macro a&lt;/span&gt;
@@ &lt;span class=&quot;c&quot;&gt;# run last macro run&lt;/span&gt;
25@a &lt;span class=&quot;c&quot;&gt;# run macro a 25 times&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;jedi-vim&quot;&gt;Jedi-vim&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;K&amp;gt; &lt;span class=&quot;c&quot;&gt;# show docstring&lt;/span&gt;
&amp;lt;leader&amp;gt;d &lt;span class=&quot;c&quot;&gt;# go to definition&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;misc-1&quot;&gt;Misc&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/a/3713865&quot;&gt;git difftool을 vimdiff로 설정하기&lt;/a&gt;,  &lt;a href=&quot;[https://goodtogreate.tistory.com/entry/git-difftool-%EC%82%AC%EC%9A%A9%EB%B2%95](https://goodtogreate.tistory.com/entry/git-difftool-사용법)&quot;&gt;vimdiff에 대한 간단한 설명&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git config &lt;span class=&quot;nt&quot;&gt;--global&lt;/span&gt; diff.tool vimdiff
git config &lt;span class=&quot;nt&quot;&gt;--global&lt;/span&gt; difftool.prompt &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# git config --global alias.d difftool&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;use &lt;code class=&quot;highlighter-rouge&quot;&gt;zR&lt;/code&gt; to unfold all. &lt;code class=&quot;highlighter-rouge&quot;&gt;zM&lt;/code&gt; to fold all.&lt;/p&gt;

&lt;h2 id=&quot;install-latest-vim&quot;&gt;Install latest vim&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository ppa:jonathonf/vim
sudo apt update
sudo apt install vim
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Mon, 22 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018-10-22/Vim-Cheatsheet/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-10-22/Vim-Cheatsheet/</guid>
      </item>
    
      <item>
        <title>Python cheat sheet</title>
        <description>&lt;h2 id=&quot;colorize-output&quot;&gt;Colorize output&lt;/h2&gt;

&lt;p&gt;Terminal에서 출력되는 nerdy하게 output에 색깔을 부여하기 &lt;a href=&quot;https://stackoverflow.com/a/287944&quot;&gt;출처&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;bcolors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;HEADER&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\033&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[95m'&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;OKBLUE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\033&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[94m'&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;OKGREEN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\033&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[92m'&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;WARNING&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\033&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[93m'&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;FAIL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\033&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[91m'&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ENDC&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\033&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[0m'&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;BOLD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\033&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[1m'&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;UNDERLINE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\033&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[4m'&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bcolors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OKBLUE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'okay this will be blue!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;subprocess-모듈&quot;&gt;Subprocess 모듈&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://soooprmx.com/archives/5932&quot;&gt;Subprocess 모듈은 파이썬 기본 모듈로, 파이썬 프로그램 내에서 새로운 프로세스를 만들고 여기에 대한 입출력을  관리할 수 있게 해준다.&lt;/a&gt; 쉽게 말해서 Python 내부에서 다른 프로그램 실행시키는 걸 가능하게 해주는 모듈이다. 예를 들어 파이토치를 이용해서 기계번역 모델을 만든다고 해보자. 열심히 코딩을 해서, 영어 문장을 입력하면 불어문장을 출력하는 거까지 했다고 치자. 이후,성능 측정을 위해서 BLEU score를 재야하는데, 이 코드까지 직접 짤 필요는 없어보인다. 남들도 다 쓰는 성능 지표니까 인터넷에 코드가 널려있으니까. &lt;a href=&quot;https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl&quot;&gt;보통 moses라는 옛날 기계번역 모델때 사용한 multi-bleu.perl 코드를 많이들 가져다 쓴다.&lt;/a&gt; 나는 python으로 코딩을 하고 있지만 subprocess 모듈을 사용하면 내부적으로 perl 코드를 실행시킬 수 있다!&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# perl multi-bleu.perl reference.txt &amp;lt; prediction.txt&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# shell에서 위 명령어를 실행하는 걸 파이썬 내부에서 하려면 아래와 같이 하면 된다&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;subprocess&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'prediction.txt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;subprocess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;call&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'perl'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'multi-bleu.perl'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'reference.txt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 입력 리다이렉션&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;subprocess.call()는 간단한 사용을 위한 함수이다. 보다 세세한 작업을 원하면 subprocess.Popen 클래스의 인스턴스를 만들고 이에 대한 메서드를 사용해야한다.&lt;/p&gt;

&lt;h2 id=&quot;multiprocessingpool&quot;&gt;multiprocessing.pool&lt;/h2&gt;

&lt;p&gt;많은 양의 데이터를 불러오기, 많은 양의 데이터를 처리해야할 때 용이! 큰 task를 여러 개의 작은 task로 쪼개서 각 process가 하나씩 담당하게 한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;multiprocessing&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mp&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;chunks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;chunk_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chunk_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chunk_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
  
  &lt;span class=&quot;c&quot;&gt;# Identical result to `result = function(dat_list)`&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;num_proc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;slices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chunks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_proc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;processes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_proc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pool&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;multiprocessingprocess&quot;&gt;multiprocessing.Process&lt;/h2&gt;

&lt;p&gt;새로운 프로세스를 생성할 때 stdin은 닫힌다. 새로운 프로세스에서 입력을 받으려면 &lt;a href=&quot;https://stackoverflow.com/a/30149635&quot;&gt;임의로 stdin을 열어줘야한다&lt;/a&gt;!&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Sat, 20 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018-10-20/Python-Cheatsheet/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-10-20/Python-Cheatsheet/</guid>
      </item>
    
      <item>
        <title>Python module 관련</title>
        <description>&lt;h2 id=&quot;pip&quot;&gt;pip&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;numpy &lt;span class=&quot;c&quot;&gt;# 설치&lt;/span&gt;
pip show numpy &lt;span class=&quot;c&quot;&gt;# 설치한 패키지에 대한 정보&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위와 같이 pip의 install 명령어를 통해 설치한 패키지는 pip show 명령어로 그 패키지에 대한 정보를 확인할 수 있다. &lt;code class=&quot;highlighter-rouge&quot;&gt;Location&lt;/code&gt;항목에 써져있는 게 소스코드가 저장된 위치다. 나는 pytorch를 Anaconda를 통한 가상환경 안에서 pip install을 는데, &lt;code class=&quot;highlighter-rouge&quot;&gt;~/anaconda3/hwijeen_3.6/lib/python3.6/site-package&lt;/code&gt;안에 저장되어있었다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TODO: site package?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;syspath&quot;&gt;sys.path&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'..'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;파이썬 스크립트에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;import numpy&lt;/code&gt;을 하면 &lt;code class=&quot;highlighter-rouge&quot;&gt;numpy&lt;/code&gt;라는 모듈을 불러온다(&lt;a href=&quot;https://leemendelowitz.github.io/blog/how-does-python-find-packages.html&quot;&gt;자세한 정보&lt;/a&gt;). 컴퓨터 어딘가에 &lt;code class=&quot;highlighter-rouge&quot;&gt;numpy&lt;/code&gt; 소스코드가 저장되어 있는데, &lt;code class=&quot;highlighter-rouge&quot;&gt;import&lt;/code&gt;는 &lt;code class=&quot;highlighter-rouge&quot;&gt;sys.path&lt;/code&gt;에 저장된 경로들에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;numpy&lt;/code&gt;를 찾아온다. 만약, &lt;code class=&quot;highlighter-rouge&quot;&gt;sys.path.append()&lt;/code&gt;를 통해 &lt;code class=&quot;highlighter-rouge&quot;&gt;sys.path&lt;/code&gt; 변수에 &lt;code class=&quot;highlighter-rouge&quot;&gt;'..'&lt;/code&gt;(바로 윗 디렉토리)를 추가해준다면, 바로 위 디렉토리에 있는 파일도 불러올 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://rampart81.github.io/post/python-ctags/&quot;&gt;sys.path를 활용해서 python에서 ctags 효과적으로 사용하는 방법&lt;/a&gt;이 있지만 사실 잘 안된다. Exuberant-ctag는 더 이상 유지 보수가 되지 않고, universal ctag가 최신(?)버전이라고 한다. 그런데 얘마저 pytorch module에 대해 태그를 잘 만들어주지 못한다ㅠㅠ. &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;&quot;&quot;파이썬 주석&quot;&quot;&quot;&lt;/code&gt;이 있는 파일은 제대로 태그 처리를 못하는 게 문제인듯하다ㅜㅜ.&lt;/p&gt;

</description>
        <pubDate>Tue, 09 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018-10-09/Python-%EB%AA%A8%EB%93%88-%EA%B4%80%EB%A0%A8/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-10-09/Python-%EB%AA%A8%EB%93%88-%EA%B4%80%EB%A0%A8/</guid>
      </item>
    
      <item>
        <title>Linux cheat sheet</title>
        <description>&lt;p&gt;From paste, &lt;a href=&quot;http://www.incodom.kr/Linux/기본명령어/awk&quot;&gt;awk&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;커널---쉘---터미널&quot;&gt;커널 - 쉘 - 터미널&lt;/h2&gt;

&lt;h2 id=&quot;standard-streams&quot;&gt;Standard streams&lt;/h2&gt;

&lt;p&gt;프로세스는 프로그램이 실행 중인 상태를 의미한다. &lt;a href=&quot;http://bowbowbow.tistory.com/16&quot;&gt;쉽게 말해 프로그램은 하드에 저장된 코드라면, 이를 실행(메모리에 적재)한 게 프로세스이다.&lt;/a&gt; 이러한 프로세스는 표준 입력 스트림(stdin), 표준 출력 스트림(stdout), 표준 에러 출력 스트림(stderr)을 가진다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# test.py&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;‘test.py’라는 프로그램이 실행되면 프로세스가 된다. Input()함수는 stdin에서 입력을 받는다. stdin은 &lt;strong&gt;키보드 입력&lt;/strong&gt;이다. print() 함수는 stdout로 결과값을 표시해주는 함수인데, stdout은 &lt;strong&gt;모니터 출력&lt;/strong&gt;이다. 만약 존재하지 않는 파일 &lt;code class=&quot;highlighter-rouge&quot;&gt;file.txt&lt;/code&gt;를 열기 시도하면 모니터에 &lt;code class=&quot;highlighter-rouge&quot;&gt;FileNotFoundError: [Errno 2] No such file or directory: 'file.txt'&lt;/code&gt;라며 오류 메시지가 뜨게 되는데, 이는 stderr을 통해 나온 오류 메시지이다. 오류메시지도 모니터에 출력되기는 하는데, 모니터까지 오는 경로가 stdout이랑은 다른 셈이다.&lt;/p&gt;

&lt;h2 id=&quot;redirection&quot;&gt;Redirection&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;(https://jdm.kr/blog/74)&quot;&gt;리다이렉션은 표준 스트림의 흐름을 바꿔주는 것이다.&lt;/a&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;test.py&lt;/code&gt;는 stdout(모니터 화면)에 파일 &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt;의 내용을 뿌리게끔 되어있는데, &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt;의 내용을 &lt;code class=&quot;highlighter-rouge&quot;&gt;output.txt&lt;/code&gt;라는 파일에 저장하고 싶다면 다음과 같이 하면 된다. 화면에 뭐가 너무 많이 나와서 보기 싫거나, 화면에 나온 결과를 저장하고 싶을 때 쓸 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python test.py &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; output.txt
&lt;span class=&quot;c&quot;&gt;# 물론, 실행 이후에 a에 들어갈 파일 이름은 키보드로 입력해주어야한다&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 명령어를 실행하면 모니터엔 아무런 내용이 뜨지 않고, 모니터에 떴어야할 내용이 output.txt에 쓰이게 된다. 즉, 프로세스의 출력 스트림을 파일로 리다이렉션 한 것이다.&lt;/p&gt;

&lt;p&gt;출력 스트림이 아니라 입력스트림을 리다이렉션할 수도 있다. &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt;라는 변수에 들어갈 파일 이름을 키보드로 직접 입력하지 않고, 다른 파일에서 읽어올 수 있다는 말이다. 아래의 명령어를 실행하면 &lt;code class=&quot;highlighter-rouge&quot;&gt;input_file.txt&lt;/code&gt;의 내용이 모니터에 출력되게 된다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python test.py &amp;lt; filename.txt
&lt;span class=&quot;c&quot;&gt;# filename.txt 내용&lt;/span&gt;
input_file.txt
&lt;span class=&quot;c&quot;&gt;# input_file.txt 내용&lt;/span&gt;
hi my name is hwijeen
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;입력과 출력에 대해 모두 리다이렉션을 할 수도 있고, 오류 스트림에 대한 리다이렉션을 할 수도 있다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 입출력 모두 리다이렉션&lt;/span&gt;
python test.py &amp;lt; filename.txt &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; output.txt
&lt;span class=&quot;c&quot;&gt;# 오류 스트림 리다이렉션. 오류 발생시 오류 메시지가 파일에 저장된다.&lt;/span&gt;
python test.py &amp;lt; filename.txt 2&amp;gt; output.txt
&lt;span class=&quot;c&quot;&gt;# 출력, 오류 스트림 모두 리다이렉션&lt;/span&gt;
python test.py &amp;amp;&amp;gt; output.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;environmental-variables&quot;&gt;Environmental variables&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python example.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;shell에서 위와 같이 입력하면 example.py가 실행되는데, 이 과정에서 사실 환경 변수가 사용된다. example.py라는 파일을 python으로 실행하라고 명령한 건데, 컴퓨터 내에 python이 어디에 위치했는지 알려주는 게 환경변수의 역할. (&lt;a href=&quot;정확한 개념은 여기서](http://blog.naver.com/PostView.nhn?blogId=koromoon&amp;amp;logNo=220793570727)&quot;&gt;개념 좀 더&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;여러 환경 변수 중에 ‘PATH’라는 환경변수는 실행파일을 찾는 경로를 알려준다. 이 변수에 python 위치, cudnn 위치 등이 잡혀있어야 된다. 예를 들어, Python위치가 제대로 잡혀있지 않으면 &lt;em&gt;python: command not found&lt;/em&gt; 오류가 난다.&lt;/p&gt;

&lt;p&gt;Unix 계열 운영체제에서 환경 변수는 ‘~/.bash_profile’, ‘~/.bashrc’에 저장해둔다. 터미널을 실행할 때마다 운영체제가 이 파일을 호출해서 환경 변수 값을 읽는 구조다. MacOS를 사용한다면 ‘~/.bash_profile’, Ubuntu라면 ‘~/.bashrc’에 환경 변수 등록을 추천한다(&lt;a href=&quot;http://uroa.tistory.com/114&quot;&gt;자세한 차이는 여기&lt;/a&gt;). 파일 하단에 다음과 같은 export 명령어를 추가해서 환경 변수 등록을 할 수 있다. &lt;strong&gt;주의할 점은 ‘=’ 좌우로 띄어쓰기를 하면 안 된다는 점이랑 ‘:’을 통해서 여러 경로를 등록한다는 거!&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# '/'는 최상위 directory를 의미&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/Users/hwii/anaconda/bin:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finder를 통해서 위 경로를 따라가 보면 ‘python3.6’과 같은 실행파일이 있는 걸 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;’~/.bash_profile’, ‘./bashrc’파일을 수정한 뒤엔 다음과 같은 확인 작업을 거친다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 변경사항 적용&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.bash_profile
&lt;span class=&quot;c&quot;&gt;# PATH 환경변수에 등록된 값 확인&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;참고로, 가상 환경 사용시 가상 환경 내부에 있는 python 경로를 직접 등록해줄 필요는 없다. ‘source activate 가상환경’을 할 경우 anaconda와 같은 가상환경 매니저가 알아서 가상환경 내부 python 경로를 path에 등록해준다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;activate hwijeen_2.7
&lt;span class=&quot;c&quot;&gt;# PATH 확인시 내가 직접 등록하지 않은 python_2.7도 잡혀있음을 볼 수 있다. anaconda가 대신 잡아준 거다.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 모든 환경변수 보기&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;env&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;tar&quot;&gt;Tar&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;tar&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-cvzf&lt;/span&gt; xxx.tar.gz &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 해당 경로의 모든 파일을 압축&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;tar&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-xvzf&lt;/span&gt; xxx.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;port-forwarding&quot;&gt;Port forwarding&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; 8888:localhost:9999 remote_user@remote_server
&lt;span class=&quot;c&quot;&gt;# -N no remote command&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -f put ssh in the background&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -L forward A:B&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;foreground-background&quot;&gt;Foreground, background&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python a.py &amp;amp; &lt;span class=&quot;c&quot;&gt;# background 프로세스로 실행&lt;/span&gt;
python a.py &amp;amp;&amp;gt; both.txt &amp;amp; &lt;span class=&quot;c&quot;&gt;# 출력, 오류 스트림 리다이렉션 + background 프로세스로 실행&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;jobs 
fg&lt;/span&gt; %1
&lt;span class=&quot;nb&quot;&gt;bg&lt;/span&gt; %1

ctrl + z &lt;span class=&quot;c&quot;&gt;# stops a process. List of stopped processes in 'jobs'.&lt;/span&gt;
ctrl + c to &lt;span class=&quot;c&quot;&gt;# interrupt(quit)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://kb.iu.edu/d/afnz&quot;&gt;Background 프로세스는 shell과 독립적으로 실행된다.&lt;/a&gt; 즉, 어떤 프로세스를 실행시키고 나서도 터미널 창을 이용할 수 있다! Background 프로세스가 내보내는 stderr, stdout은 방해가 될 때가 많으니까 파일에 적게끔 output redirection을 같이 해주면 편하다.&lt;/p&gt;

&lt;p&gt;이미 프로세스를 foreground에서 실행시켜버린 상태라면, ctrl+z로 잠시 stop한 뒤에, jobs에서 bg %1 식으로 background로 보낼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;package-management&quot;&gt;Package management&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get update &lt;span class=&quot;c&quot;&gt;# update package index&lt;/span&gt;
apt list &lt;span class=&quot;nt&quot;&gt;--installed&lt;/span&gt;	&lt;span class=&quot;c&quot;&gt;# show list of installed packages&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PACKAGE_NAME&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get remove &lt;span class=&quot;nv&quot;&gt;$PACKAGE_NAME&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-cache search &lt;span class=&quot;nv&quot;&gt;$KEYWORD&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;초기설정&quot;&gt;초기설정&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Add user&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;adduser &lt;span class=&quot;nv&quot;&gt;$USER_NAME&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;usermod &lt;span class=&quot;nt&quot;&gt;-aG&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$USER_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# add to sudo group&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Install anaconda&lt;/span&gt;
wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh

&lt;span class=&quot;c&quot;&gt;# Install zsh&lt;/span&gt;
zsh &lt;span class=&quot;nt&quot;&gt;--version&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# check if already installed&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;zsh
chsh &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; /usr/bin/zsh

&lt;span class=&quot;c&quot;&gt;# Install Oh My Zsh&lt;/span&gt;
curl &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh | sh 

&lt;span class=&quot;c&quot;&gt;# dotfiles&lt;/span&gt;
.zshrc
.vimrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;google-drive-link-wget으로-다운로드&quot;&gt;Google drive link wget으로 다운로드&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/@acpanjan/download-google-drive-files-using-wget-3c2c025a8b99&quot;&gt;출처&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#https://drive.google.com/file/d/1UibyVC_C2hoT_XEw15gPEwPW4yFyJFeOEA/view?usp=sharing&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# FIELDID=1UibyVC_C2hoT_XEw15gPEwPW4yFyJFeOEA&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# FILENAME=file.txt&lt;/span&gt;

wget &lt;span class=&quot;nt&quot;&gt;--load-cookies&lt;/span&gt; /tmp/cookies.txt &lt;span class=&quot;s2&quot;&gt;&quot;https://docs.google.com/uc?export=download&amp;amp;confirm=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;wget &lt;span class=&quot;nt&quot;&gt;--quiet&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--save-cookies&lt;/span&gt; /tmp/cookies.txt &lt;span class=&quot;nt&quot;&gt;--keep-session-cookies&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--no-check-certificate&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'https://docs.google.com/uc?export=download&amp;amp;id=FILEID'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-O-&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rn&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'s/.*confirm=([0-9A-Za-z_]+).*/\1\n/p'&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;amp;id=FILEID&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; FILENAME &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; /tmp/cookies.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Tue, 09 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018-10-09/Linux-Cheatsheet/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-10-09/Linux-Cheatsheet/</guid>
      </item>
    
      <item>
        <title>CUDA, Cudnn</title>
        <description>&lt;h2 id=&quot;cuda-cudnn&quot;&gt;CUDA, Cudnn&lt;/h2&gt;

&lt;p&gt;CUDA: GPU를 그래픽 계산 뿐만 아니라 사용자에 목적에 맞게끔 사용할 수 있도록 (GPGPU) Nvidia가 만들어 놓은 라이브러리다. 라이브러리는 추상적인 개념이지만 CUDA는 결국엔 컴퓨터에 .so 확장자를 갖는 파일로 존재한다. Pytorch 등 프레임워크는 내부적으로 CUDA를 사용하는데, CUDA에 접근하려면 바이너리 파일이 어디에 있는지 알 필요가 있다. 이 위치를 알려주는 게 LD_LIBRARY_PATH다. 이 변수에 CUDA위치가 잘 잡혀있다면 Pytorch가 오류 없이 돌아갈 것이다! &lt;strong&gt;Pytorch 혹은 Tensorflow 버전마다 지원하는 CUDA 버전이 다르므로, 지원 버전을 잘 확인하고 CUDA를 버전을 선택해서 다운 받아야한다.&lt;/strong&gt; &lt;a href=&quot;https://blog.kovalevskyi.com/multiple-version-of-cuda-libraries-on-the-same-machine-b9502d50ae77&quot;&gt;여러 버전의 CUDA를 깔아두고, 가상환경 별로 다른 CUDA를 잡아줄 수도 있다&lt;/a&gt;. Pytorch는 사용자가 직접 CUDA 경로를 건드리지 않아도 되게끔 자체적으로 CUDA를 관리한다(따로 지정된 곳에 설치).&lt;/p&gt;

&lt;p&gt;Cudnn:  &lt;a href=&quot;https://www.quora.com/What-is-CUDA-and-cuDNN&quot;&gt;딥러닝에 GPU를 활용할 수 있도록 해주는 라이브러리로, CUDA를 사용해 만들어져있다.&lt;/a&gt; CUDA위에 있는 라이브러리다. 그러니까, 파이토치는 Cudnn을 사용하고, Cudnn은 CUDA를 사용하는 구조다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CUDA 및 Cudnn는 설치 이후에 직접 환경 변수 설정까지 해줘야한다&lt;/strong&gt;. 설정 과정 설명은 구글링하면 친전한 글들이 많이 나온다.만약에 환경 변수 설정을 안 할 경우 혹은 잘못한 경우 다음과 같은 에러가 난다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pytorch가 CUDA 위치를 잡지 못해서 나는 에러다. CUDA 위치를 LD_LIBRARY_PATH 환경 변수에 등록해주면(위에서 설명한 방법대로) 문제 해결 가능하다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# '~/.bashrc'&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 기본적으로 아래와 같은 위치에 설치된다. 8.0 버전임에 유의!&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;LD_LIBRARY_PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/cuda-8.0/lib64:&lt;span class=&quot;nv&quot;&gt;$LD_LIBRARY_PATH&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 꼭 기존 거 앞에!&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이때, CUDA 파일이 다른 가상환경 안에 있을 수도, usr/local/에 있을 수도 있다! CUDA를 설치했는데 위치를 모르겠다면 터미널에서 다음 명령어로 파악이 가능하다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# CUDA 파일 중에 하나. 이 파일이 들어 있는 폴더가 CUDA가 설치된 경로다.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# MacOS에선 locate 명령어 안 됨&lt;/span&gt;
locate libcudart.so.8.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/17543&quot;&gt;NVIDIA RTX는 CUDA 9.0에서 오류가 난다.&lt;/a&gt; CUDA 10.0을 쓰려면 Pytorch 1.0이 필요하다.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html&quot;&gt;Cudnn 설치는  CUDA 폴더에 파일 몇 개 넣기&lt;/a&gt;. Cudnn버전만 바꿀 때 참고할 &lt;a href=&quot;https://stackoverflow.com/questions/38137828/how-do-i-update-cudnn-to-a-newer-version&quot;&gt;링크&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# returns version of the cuda found by pytorch&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;기타-지식&quot;&gt;기타 지식&lt;/h2&gt;

&lt;p&gt;.a파일은 정적 라이브러리고 .so파일은 동적 라이브러리다.&lt;/p&gt;

&lt;p&gt;ls -la를 통해 symbolic link 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;Anaconda는 가상환경마다 환경 변수를 따로 관리한다. $CONDA_PREFIX는 지금 activate된 가상환경 위치 알려주는 환경 변수.&lt;/p&gt;

&lt;p&gt;기본적으로 /usr/local/cuda-9.0에 쿠다가 깔려있다. 가상환경을 만들고 CUDA를 별도로 설치했을 경우, $CONDA_PREFIX/lib 안에도 CUDA 바이너리 파일이 존재한다.&lt;/p&gt;

</description>
        <pubDate>Mon, 17 Sep 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018-09-17/CUDA,-Cudnn/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-09-17/CUDA,-Cudnn/</guid>
      </item>
    
      <item>
        <title>Jupyter notebook cheat sheet</title>
        <description>&lt;h2 id=&quot;명령어&quot;&gt;명령어&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# shows list of jupyter notebook servers&lt;/span&gt;
jupyter notebook list

&lt;span class=&quot;c&quot;&gt;# stops localhost:8888&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# available on notebook version 5.1.0 or after&lt;/span&gt;
jupyter notebook stop 8888
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;jupyter notebook list 상에 많은 서버가 켜져있더라도, 컴퓨터 &lt;strong&gt;재부팅&lt;/strong&gt;이후엔 싹 다 꺼진다.&lt;/p&gt;

&lt;h2 id=&quot;port-forwarding-활용해-로컬에서-원격-주피터-노트북-접속&quot;&gt;Port forwarding 활용해 로컬에서 원격 주피터 노트북 접속&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://coderwall.com/p/ohk6cg/remote-access-to-ipython-notebooks-via-ssh&quot;&gt;출처&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# on remote host&lt;/span&gt;
jupyter notebook &lt;span class=&quot;nt&quot;&gt;--no-browser&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;8889
&lt;span class=&quot;c&quot;&gt;# on local machine&lt;/span&gt;
ssh &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; 8888:localhost:8888 &lt;span class=&quot;nv&quot;&gt;$REMOTE_USER&lt;/span&gt;@&lt;span class=&quot;nv&quot;&gt;$REMOTE_HOST&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -N for no remote command to be executed&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -f for ssh in background&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -L port forwarding&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;원격-주피터-노트북-서버에-접속하기&quot;&gt;원격 주피터 노트북 서버에 접속하기&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://light-tree.tistory.com/111&quot;&gt;링크 1&lt;/a&gt;를 따라하되 &lt;strong&gt;서버 비밀번호 생성&lt;/strong&gt;부분은 &lt;a href=&quot;https://stackoverflow.com/questions/42848130/why-i-cant-access-remote-jupyter-notebook-server&quot;&gt;링크 2&lt;/a&gt; step 4를 따라한다. 만약 안 된다면 유력한 이유는 port 접근 자체가 막힌 거..&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;ufw allow 8888
jupyter notebook &lt;span class=&quot;nt&quot;&gt;--generate-config&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter notebook password
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#	in ~/.jupyter/jupyter_notebook_cofing.py&lt;/span&gt;
c.NotebookApp.allow_origin &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'*'&lt;/span&gt;
c.NotebookApp.ip &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'$IP'&lt;/span&gt;
c.NotebookApp.notebook_dir &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'$DESIRED_PATH'&lt;/span&gt;
c.NotebookApp.password &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$COPIED_RESULT&lt;/span&gt;
c.NotebookApp.open_browser &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; False
c.NotebookApp.port &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'8888'&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 8888 is default&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# on local browser&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;:8888 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;misc&quot;&gt;Misc&lt;/h2&gt;

&lt;h3 id=&quot;cell-width&quot;&gt;Cell width&lt;/h3&gt;

&lt;p&gt;결과 창이 가로로 너무 작아서 잘릴 때 대처방법 &lt;a href=&quot;https://stackoverflow.com/a/34058270&quot;&gt;출처&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;IPython.core.display&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HTML&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HTML&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;style&amp;gt;.container { width:100&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;% &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;!important; }&amp;lt;/style&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter notebook &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; /dev/null 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;span class=&quot;c&quot;&gt;# redirection first, and then to background&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;jupyter-themes&quot;&gt;Jupyter themes&lt;/h3&gt;

&lt;p&gt;주피터 노트북 테마 사용법 &lt;a href=&quot;https://pinkwink.kr/1120&quot;&gt;출처&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;jupyterthemes
jt &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; grade3 &lt;span class=&quot;nt&quot;&gt;-fs&lt;/span&gt; 12 &lt;span class=&quot;nt&quot;&gt;-tfs&lt;/span&gt; 12 &lt;span class=&quot;nt&quot;&gt;-nfs&lt;/span&gt; 11 &lt;span class=&quot;nt&quot;&gt;-ofs&lt;/span&gt; 12 &lt;span class=&quot;nt&quot;&gt;-cellw&lt;/span&gt; 980 &lt;span class=&quot;nt&quot;&gt;-T&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; hack &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Sun, 01 Jul 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018-07-01/Jupyter-notebook-cheat-sheet/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-07-01/Jupyter-notebook-cheat-sheet/</guid>
      </item>
    
      <item>
        <title>Git cheat sheet</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://suwoni-codelab.com/git/2018/04/05/Git-reset/&quot;&gt;good blog&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;prep&quot;&gt;Prep&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git init

git config user.name &lt;span class=&quot;s2&quot;&gt;&quot;hwijeen&quot;&lt;/span&gt;
git config user.email &lt;span class=&quot;s2&quot;&gt;&quot;aurorall@naver.com&quot;&lt;/span&gt;
- &lt;span class=&quot;nt&quot;&gt;--global&lt;/span&gt; 옵션을 사용하지 않은 경우 해당 폴더 scope에서 git 사용자 등록 가능
 
&lt;span class=&quot;c&quot;&gt;# cf) git clone&lt;/span&gt;
git remote add origin &lt;span class=&quot;nv&quot;&gt;$address_copied_from_github_repo&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 주소에 나온 repository를 origin이라는 이름으로 remote에 등록한다&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;git의-4개의-영역&quot;&gt;Git의 4개의 영역&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/Users/hwijeen/Desktop/git areas.png&quot; alt=&quot;git areas&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;1-working-directory&quot;&gt;1. Working directory&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;코드가 수정되고 저장되는 실제 프로젝트 디렉토리. &lt;code class=&quot;highlighter-rouge&quot;&gt;.git&lt;/code&gt;을 제외한 모든 영역&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;2-index&quot;&gt;2. Index&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Working directory와 repository 사이에 있는 준비 영역.&lt;/li&gt;
  &lt;li&gt;Staging area라고도 함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3-repository&quot;&gt;3. Repository&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;스냅샷을 저장해두는 곳. &lt;code class=&quot;highlighter-rouge&quot;&gt;.git&lt;/code&gt; 내부에 존재.&lt;/li&gt;
  &lt;li&gt;Local repository / remote repository.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;4-stash&quot;&gt;4. Stash&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;임시로 작업사항을 저장해놓고 꺼내올 수 있는 별개의 영역.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;git add&lt;/code&gt;로 working directory에서 index로 정보가 저장되고, &lt;code class=&quot;highlighter-rouge&quot;&gt;git commit&lt;/code&gt;으로 index의 정보를 repository에 저장한다.&lt;/p&gt;

&lt;h2 id=&quot;git-file-lifecycle&quot;&gt;Git file lifecycle&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;http://git-scm.com/figures/18333fig0201-tn.png&quot; alt=&quot;git file lifecycle에 대한 이미지 검색결과&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;브랜치-관리-정책&quot;&gt;브랜치 관리 정책&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;master branch:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;완전한 코드&lt;/li&gt;
      &lt;li&gt;버전 단위로 관리한다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;dev branch:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;stable하지만 개발중인 브랜치&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;협업할 때 각자의 브랜치에서 작업한 내용 local의 dev로 merge한 뒤, 그 브랜치를 github의 dev로 push한다.&lt;/li&gt;
      &lt;li&gt;이 브랜치의 내용 자주 확인할 필요 있다. merge &amp;amp; pull을 통해&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;임의의 작업 branch&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;내가 지정한 단위의 작업을 하는 브랜치&lt;/li&gt;
      &lt;li&gt;여기서 commit은 내 맘대로&lt;/li&gt;
      &lt;li&gt;지정한 단위의 작업이 다 끝나고 test까지 완료되면 local의 dev branch로 merge&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;명령어&quot;&gt;명령어&lt;/h2&gt;

&lt;h4 id=&quot;기본-명령어&quot;&gt;기본 명령어&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git add &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--all&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# --all 옵션으로 '지운 파일'까지 track, stage&lt;/span&gt;

git commit &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;commit message&quot;&lt;/span&gt;
git commit &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# automatically stage tracked files&lt;/span&gt;
git commit &lt;span class=&quot;nt&quot;&gt;--ammend&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# commit 덮어쓰기&lt;/span&gt;

git &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--cached&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$DIRECTORY_NAME$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# remove a folder only from staging area.&lt;/span&gt;

git diff &lt;span class=&quot;c&quot;&gt;# difference between working directory - staging area&lt;/span&gt;
git diff &lt;span class=&quot;nt&quot;&gt;--staged&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# difference between index and repository&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# git diff는 &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;branch&quot;&gt;branch&lt;/h4&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git branch &lt;span class=&quot;c&quot;&gt;# branch list 보여주기&lt;/span&gt;
git branch &lt;span class=&quot;nv&quot;&gt;$BRANCH_NAME&lt;/span&gt;
git checkout &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$BRANCH_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 만들고 checkout까지&lt;/span&gt;
git branch &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$BRANCH_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 삭제&lt;/span&gt;
git checkout &lt;span class=&quot;c&quot;&gt;# 해당 branch로 워킹디렉토리를 이동하기&lt;/span&gt;
git branch &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$NEW_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# rename&lt;/span&gt;
git branch &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# show branches in remote &lt;/span&gt;

git checkout &lt;span class=&quot;nv&quot;&gt;$BRANCH_NAME&lt;/span&gt;
git pull origin &lt;span class=&quot;nv&quot;&gt;$BRANCH_NAME&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;병합하기&quot;&gt;병합하기&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git merge &lt;span class=&quot;nv&quot;&gt;$BRANCH_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 지금 branch로 &amp;lt;BRANCH_NAME&amp;gt;을 병합하기 &lt;/span&gt;
git rebase &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$BRANCH_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# conflict는 손으로 지우고 다시 add해야한다&lt;/span&gt;
git checkout &lt;span class=&quot;nv&quot;&gt;$BRANCH_NAME&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$FILE_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 주의!&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;원격-저장소-관련&quot;&gt;원격 저장소 관련&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git remote 
git remote update
git remote add &lt;span class=&quot;nv&quot;&gt;$URL&lt;/span&gt;

git push origin &lt;span class=&quot;nv&quot;&gt;$BRANCH_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# remote repository로 &amp;lt;BRANCH_NAME&amp;gt;을 push&lt;/span&gt;

git pull &lt;span class=&quot;c&quot;&gt;# 현재 branch pull(fetch &amp;amp; commit)&lt;/span&gt;
git pull &lt;span class=&quot;nv&quot;&gt;$REMOTE_NAME&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$BRANCH_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# remote의 &amp;lt;BRANCH_NAME&amp;gt;으로부터 pull&lt;/span&gt;

git branch &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$BRANCH_NAME&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$LOCAL_BRANCH_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# to get remote branch to current branch&lt;/span&gt;
git checkout &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$REMOTE_NAME&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;$REMOTE_BRANCH_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# local에 없는 branch 가져오기&lt;/span&gt;

git fetch &lt;span class=&quot;nv&quot;&gt;$REMOTE_NAME&lt;/span&gt;

git push origin &lt;span class=&quot;nt&quot;&gt;--delete&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$REMOTE_BRANCH_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# delete branch in remote&lt;/span&gt;
git push origin :&lt;span class=&quot;nv&quot;&gt;$REMOTE_BRANCH_NAME&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# same&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;취소하기&quot;&gt;취소하기&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git reset HEAD &lt;span class=&quot;nv&quot;&gt;$FILE&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# staging area에서 빼기(add 취소)&lt;/span&gt;
git &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--cached&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$FILE&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# stage area에서 삭제(파일의 삭제를 add)&lt;/span&gt;

git reset &lt;span class=&quot;nt&quot;&gt;--soft&lt;/span&gt; HEAD^ &lt;span class=&quot;c&quot;&gt;# commit 취소하고 working directory, staging area 모두 보존&lt;/span&gt;
git reset &lt;span class=&quot;nt&quot;&gt;--mixed&lt;/span&gt; HEAD^ &lt;span class=&quot;c&quot;&gt;# commit 취소하고 working directory 보존, stageing area는 예전 그때로&lt;/span&gt;
git reset &lt;span class=&quot;nt&quot;&gt;--hard&lt;/span&gt; HEAD^ &lt;span class=&quot;c&quot;&gt;# commit 취소하고 working directory, staging area 모두 그때로&lt;/span&gt;

git reset &lt;span class=&quot;nt&quot;&gt;--hard&lt;/span&gt; ORIG_HEAD &lt;span class=&quot;c&quot;&gt;# pull 취소!&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 이후에 git push -f origin master가 필요&lt;/span&gt;
git reset HEAD &lt;span class=&quot;nv&quot;&gt;$FILE&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 이 file unstage&lt;/span&gt;

git checkout &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$FILE&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 최근 commit 상태로 되돌리기, 위험!&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;기타&quot;&gt;기타&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git config &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://gmlwjd9405.github.io/2018/05/25/git-add-cancle.html&quot;&gt;참고 링크&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;gitignore&quot;&gt;.gitignore&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# comment&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.pyc
data/
__pycache__/
.ideas/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;참고&quot;&gt;참고&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://coderwall.com/p/fasnya/add-git-branch-name-to-bash-prompt&quot;&gt;bash prompt에 현재 들어와있는 branch이름 표시하기.&lt;/a&gt; PS1라는 환경변수가 bash prompt에 나오는 정보를 결정하는 모양인데, 거기에 git branch를 추가해주는 방식인 거 같다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mac에서 git이 한국어를 하는 경우 &lt;a href=&quot;https://stackoverflow.com/a/54574337&quot;&gt;영어로 바꾸기&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.lesstif.com/pages/viewpage.action?pageId=20776404&quot;&gt;OS간 개행문자 문제&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 29 Apr 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018-04-29/Git-Cheatsheet/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-04-29/Git-Cheatsheet/</guid>
      </item>
    
      <item>
        <title>Encoding</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/&quot;&gt;Introduction&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;unicode&quot;&gt;Unicode&lt;/h2&gt;

&lt;p&gt;유니코드는 &lt;strong&gt;문자 - 코드(숫자)의 매핑&lt;/strong&gt;일 뿐이다. 전 세계 모든 문자에 대한 숫자 매핑이 있는 게 특징. 그러니까 ASCII 코드표랑 같은 개념이라고 보면 된다. ASCII는 한글 등 다양한 문자를 표현하지 못하기에, 확장판인 유니코드가 나온 격이다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;char&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Unicode&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ASCII&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;U+0041&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;65&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;가&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;U+AC00&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;N/A&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;encoding&quot;&gt;Encoding&lt;/h2&gt;

&lt;p&gt;인코딩은 &lt;strong&gt;코드를 저장하는 방식&lt;/strong&gt;이다! 같은 유니코드라 하더라도, 컴퓨터에 저장할 땐 다양한 방식으로 저장할 수 있다. 어떤 인코딩 방식은 ‘ㄱ’를 e18480으로 저장하고, 어떤 인코딩은 feff11a8로 저장한다.  즉, 유니코드를 바이트로 변환하는 방식.&lt;/p&gt;

&lt;p&gt;여러 인코딩 방식이 있는 이유는, “내가 더 많은 양의 자료를 효과적으로 저장할 수 있어!”, “알파벳이 많은 자료라면 내가 더 효과적이야!”라는 식으로 각자의 장점이 있기 때문이다.&lt;/p&gt;

&lt;p&gt;텍스트 파일은 &lt;strong&gt;특정 인코딩 방식으로 저장되어 있다.&lt;/strong&gt; 따라서 그 파일을 읽으려면 그 방식대로 디코딩** 해줘야한다. 다른 방식으로 파일을 읽을 경우 파일이 깨진다.&lt;/p&gt;

&lt;h2 id=&quot;utf-8-utf-16-euc-kr-cp494&quot;&gt;UTF-8? UTF-16? EUC-KR? CP494?&lt;/h2&gt;

&lt;p&gt;이런 것들이 다 여러가지 인코딩 방식의  종류들이다. 각자의 특징이 있으나, UTF-8을 쓰면 큰 탈이 날 일은 없다. 호환성이 좋다는 장점이 있단다. 별일 없으면 UTF-8을 쓰자.&lt;/p&gt;

&lt;h2 id=&quot;인코딩디코딩-관련-오류&quot;&gt;인코딩/디코딩 관련 오류&lt;/h2&gt;

&lt;h3 id=&quot;vi로-txt파일을-열었는데-깨진다&quot;&gt;vi로 .txt파일을 열었는데 깨진다?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;linux에서 .txt파일을 열었는데 문자가(특히 한글) 다 깨진다&lt;/li&gt;
  &lt;li&gt;문자는 안 깨지는데 문장의 끝마다 ^M같이 보기 싫은 문자가 있다&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# reload a file using different encoding&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# utf-8자리에 이 파일의 인코딩을 써줘야한다&lt;/span&gt;
:e ++enc&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;utf-8

:set &lt;span class=&quot;nv&quot;&gt;fileencoding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;utf-8 &lt;span class=&quot;c&quot;&gt;# to save using utf-8&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# get rid of broken new line symbol&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# :$s/[Control]+v+m//g&lt;/span&gt;
:&lt;span class=&quot;nv&quot;&gt;$s&lt;/span&gt;/^M//g 

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;python-소스코드가-깨진다&quot;&gt;python 소스코드가 깨진다?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;소스코드에서 한글 주석 등이 다 깨진다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 소스 코드 맨 위에 주석 달아주기&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#-*- coding utf-8-*-&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 저것도 안 되면!&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;reload&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setdefaultencoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'utf-8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://libsora.so/posts/python-hangul/&quot;&gt;이유가 궁금하다면&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;파이썬-2&quot;&gt;파이썬 2&lt;/h2&gt;

&lt;p&gt;파일을 불러오는 즉시 Unicode로 바꿔주는 게 상책. 즉 decode(‘인코딩방식’)을 해준다. 이후 스크립트에서는 모든 걸 unicode로 처리한다. 마지막 단계에서 파일을 저장할 때, encode해서 그걸 저장한다!&lt;/p&gt;

&lt;h2 id=&quot;기타&quot;&gt;기타&lt;/h2&gt;

&lt;p&gt;json.dumps할 때 ensure_ascii=False해주는 것 잊지 말기&lt;/p&gt;

</description>
        <pubDate>Wed, 25 Apr 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018-04-25/Encoding/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-04-25/Encoding/</guid>
      </item>
    
      <item>
        <title>Anaconda cheat sheet</title>
        <description>&lt;h2 id=&quot;개념&quot;&gt;개념&lt;/h2&gt;

&lt;p&gt;Anaconda는  &lt;strong&gt;패키지 관리&lt;/strong&gt; &amp;amp; &lt;strong&gt;가상환경 관리&lt;/strong&gt;용 프로그램이다.&lt;/p&gt;

&lt;h2 id=&quot;패키지-관련-명령어&quot;&gt;패키지 관련 명령어&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PACKAGE_NAME&lt;/span&gt;
conda &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ENVIRONMENT_NAME&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PACKAGE_NAME&lt;/span&gt;
conda update &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ENVIRONMENT_NAME&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PACKAGE_NAME&lt;/span&gt;
conda update conda
conda remove &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ENVIRONMENT_NAME&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PACKAGE_NAME&lt;/span&gt;
conda list &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ENVIORNMENT_NAME&lt;/span&gt;

pip show &lt;span class=&quot;nv&quot;&gt;$PACKAGE_NAME&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;가상환경-관련-명령어&quot;&gt;가상환경 관련 명령어&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda create &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ENVIRONMENT_NAME&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;python&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3.6 &lt;span class=&quot;nv&quot;&gt;$PACKAGE_NAME&lt;/span&gt;
conda activate &lt;span class=&quot;nv&quot;&gt;$ENVIRONMENT_NAME&lt;/span&gt; 
&lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;deactivate
conda &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;remove &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ENVIRONMENT_NAME&lt;/span&gt;
conda info &lt;span class=&quot;nt&quot;&gt;--envs&lt;/span&gt;
conda &lt;span class=&quot;nb&quot;&gt;env export&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ENVIORMENT_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; environment.yml
conda &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; enviornmnet.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conda-install-vs-pip-install&quot;&gt;conda install vs. pip install&lt;/h2&gt;

&lt;p&gt;conda install은 conda repository, pip install은 PyPI에 있는 패키지 파일을 설치한다. 같은 내용의 패키지 일지라도 conda repository는 conda package형식으로 패키지를 가지고 있고, PyPI는 wheel 파일 혹은 source 파일 형태로 패키지를 저장하고 있다.&lt;/p&gt;

&lt;p&gt;conda를 이용해서 가상환경을 만들시 환경 내에 기본적으로 pip도 같이 설치된다. 간혹 conda install $PACKAGE_NAME을 했는데 패키지를 찾을 수 없다고 나오는 경우(conda repository에 없어서) pip install을 통해 패키지 설치를 하면 된다. 주의할 점은 &lt;strong&gt;가상환경 내의 pip&lt;/strong&gt;를 써야한다는 것. 커맨드 라인에 pip install을 치면 global version이 실행될 수도 있다. &lt;a href=&quot;https://www.puzzlr.org/install-packages-pip-conda-environment/&quot;&gt;가상환경 내의 pip 경로를 직접 잡아서 실행시켜줘야한다&lt;/a&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;which pip&lt;/code&gt;명령어를 통해 내가 무슨 pip를 사용하고 있는지 확인하자.&lt;/p&gt;

</description>
        <pubDate>Sat, 20 Jan 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2018-01-20/Anaconda-Cheatsheet/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-01-20/Anaconda-Cheatsheet/</guid>
      </item>
    
  </channel>
</rss>
