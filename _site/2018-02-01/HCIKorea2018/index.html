<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HCI Korea 2018 감상평</title>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css" type="text/css">
  <!-- Font -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Ubuntu+Mono" rel="stylesheet">
  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Hwijeen Ahn" href="/feed.xml" />
  <!-- Begin Jekyll SEO tag v2.4.0 -->
<title>HCI Korea 2018 감상평 | Hwijeen Ahn</title>
<meta name="generator" content="Jekyll v3.8.0" />
<meta property="og:title" content="HCI Korea 2018 감상평" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="학회 구성 Keynote speech Tutorial Workshop Panel Discussion Case Study Papers Doctoral Consortium Posters Creative Award (진한 표시는 내가 참석한 것들) 학회는 2박 3일간 빽빽하게 진행됐다. 여러 가지 프로그램이 준비되어 있기에 내 목적과 관심사에 맞게 프로그램을 선택하는 게 중요했다. 더군다나 HCI는 융합학문이기 때문에 다른 학회보다도 더 다양한 주제를 다루는 것 같았다. 일례로, paper 발표 세션은 디자인, 기술, 인문 세 분야로 분류되어 있었다. 내가 주로 관심을 가졌던 건 machine learning을 활용한 기술이다. 그 외에도 VR과 디자인 관련 프로그램도 참가했다. Keynote Speech Everyday Interaction with Machine Learning이라는 제목으로 James Forgarty라는 교수님이 한 시간 동안 발표를 했다. 지각한 탓에 앞 20분 정도는 놓쳤지만 그래도 나머지 40분 동안 HCI라는 학문이 어떤 문제를 염두에 두는지 큰 그림을 접할 수 있었다. 기술 연구의 방향성에 대해서도 다시 생각해볼 수 있는 시간이었다. 이걸 듣기 전까지 내가 참여했던 건 주로 paper 발표 세션이었다. 주로 석사생들이 자신이 연구한 내용에 대해 20분 남짓한 시간 동안 발표하는 시간이었다. Wordle을 이용한 워드 시각화, data noise 증폭해서 예술작품 만들기, 휘발성 사진에 대한 연구, 분산표상기법을 활용한 단어의 비표준 의미 탐지, 시각장애인들을 위한 TouchPhoto 기술 등등… 각각의 연구는 완성도가 높아 보였지만, 이게 왜 HCI지?라는 생각이 계속 맴돌았다. 특정 분야에 대한 기술 연구에서는 human이 어디에 들어가는지 찾기 어려웠고, 인문적 접근에선 computer가 어디에 있는지 모르겠다는 느낌이었다. Forgarty 교수도 석사생들과 마찬가지로 자신의 연구를 소개했는데, 다른 점 중 하나는 연구 배경에 대해서 많은 고민을 한 걸 보여줬다는 거다. 결국엔 어떤 기술을 구현해서 소프트웨어를 만든 격인데, 왜 그걸 만들었나에 대해서 풍부한 이유가 있었다. 이 기술과 관련된 얕은 서론이 아니라 공감할 수 있는 문제에 대한 해결책으로써 기술을 제시한다는 점이 인상적이었다. 예를 들어, CueFlik이라는 이미지 검색 도우미(?)를 소개하기에 앞서 이미지 검색의 어려움을 예시로 들었다. Category 구조에서 폴더를 선택해가면서 선택지를 좁혀가는 것과 방대한 자료에서 키워드 기반으로 원하는 걸 찾아내는 건 사고의 구조가 근본적으로 다른 문제라는 점부터 시작했다. 그러고는 깔끔한 스피커 이미지를 얻기 위해서 검색 엔진에 ‘stereo’라고 검색을 하면 지저분한 배경에 있는 스피커는 물론이고 stereo라는 이름을 가진 가수까지 검색된다는 문제점을 지적했다. 이미지의 태그, 이미지 주변 텍스트를 기반으로 검색하는 엔진 특성상 ‘stereo on white background’는 해결책이 되지 못한다. 이러한 문제 상황에서 CueFlik이 지원하는 기능은 몇천 장에 이르는 stereo 검색 결과에서 사용자가 needs에 맞는 사진 몇 장을 고르면, 나머지 사진에 대해 macihe learning기반 classification을 수행해준다는 것이다. 내가 고른 기준에 따라 실시간으로 이미지가 sorting되면 원하는 이미지 여러 장을 손쉽게 찾을 수 있다는 점을 강조했다. 인간이 겪는 문제를 해주는 컴퓨터가 해결해준다는 점에서 HCI 느낌이 났다. 두 번째는 HCI의 이론적인 내용에 대해서도 강조했다는 점이다. 나처럼 HCI를 잘 모르는 사람은 HCI가 human과 computer의 interaction을 연구하는 분야구나라는 막연한 느낌밖에 없을 거다. HCI의 이론적 토대와 HCI에서 고민하는 주제들을 보여주니 HCI라는 학문이 조금은 더 와닿았다. 기억에 남는 건 HCI 연구할 떄 염두에 두어야 할 주제 5가지 : Address / Attention / Action / Alignment / Accident. 쉽게 말하면 Attention은 컴퓨터가 나에게 집중하는 중인지 보이나? Action은 컴퓨터에게 어떤 방식으로 말하나? Alignment는 컴퓨터가 잘 하고 있는지 어떻게 아나?라고 했다. Address와 Accident에 대한 쉬운 설명은 놓쳤다ㅠ. Interaction이라는 모호한 개념을 다섯 가지 부분으로 세분화하니 좀 더 와닿았다. 학회에서 소개하는 여러 연구을 다섯 가지 분류법으로 나눠도 얼추 맞는 거 같다. 이런게 다 HCI의 출발점이 아닐까 세 번째는 본질적인 질문을 해야한다는 거. 이건 뻔한 말이지만 자주 까먹는 점이다. 게다가 뻔한 내용이라 아우라 있는 교수 정도가 말해야 들어오는 어려운 메시지다ㅋㅋ. 기술을 위한 기술을 경계해야 한다는 포인트가 있었다. Personal data, self-tracking, machine learning, visualization이라는 멋진 키워드로 무장한 기술도 사실 쓸모 없을 수도 있다는 얘기를 했다. 개인의 이동 기록, 운동 기록 등등을 종합해서 사용자가 학생인지, 회사원인지를 classify해주는 서비스가 과연 사용자의 needs를 고려한 것인지 의문을 제기했다. 항상 사람의 needs를 먼저 파악하는 게 먼저고, 그에 따른 기술 구현을 생각해야 한다고 했다. 물론, 어떤 회사 입장에선 사용자 데이터를 토대로 고객의 신상을 추론하는 게 중요할 수 있으니 위의 기술도 기업 needs엔 맞는 거라고 할 수 있겠다. 그리고 결론 부분에서 강조한 not data about us, but for us. 데이터가 넘치는 세상이라지만 사실 내 개인 데이터는 구글, 페이스북이 쥐고 있는 형국이다. Big data 그 자체에 관서만 이야기할 뿐만 아니라 그것의 사용처가 human을 위하는 방향일 필요가 있다는 점을 강조했다. 그리고 machine learning에 관련해서는 그것이 doing many things on behalf of us, without explicit ordering이 현재 상황이라고 했다. 사용자의 편의를 위해 자동화된 부분이 많다는 얘기로 이해했다. 이럴 때 일수록 그 기술을 제어할 방법에 대한 연구가 필요하고, 인간이 효과적으로 제어하기 위해선 language based approach가 필요하다고 했다(human language를 통한 기술 제어가 이 교수의 관심사인 듯?). HCI라는 학문이 태어난 게 이런 맥락에서 아닐까 싶었다. Doctoral Consortium Consortium이란 게 무슨 뜻인지도 모르고 doctoral이라는 단어에 끌려서 갔다. 박사들이니까 좀 더 내공 있는 모습을 볼 수 있겠지! 기대를 하며. 기대에 부응하는 시간이었다. 김주호 교수의 사회로 세션이 시작됐는데 해외 유수 학회의 경우 doctoral consortium이 그 꽃이라고 했다. 열띤 논쟁과 피드백이 오가는 자리이고, 큰 학자가 되기 위한 등용문이라고 했다. 소개대로 paper session보다 깊이 있는 발표가 이뤄졌다. 박사 5명이 각자 연구 주제에 대해서 15분가량 발표하고, 나머지 15분 동안은 심사 교수들 및 다른 박사들의 피드백을 받았다. 참가자가 많지 않지만 다들 진지한 게 이런 느낌(사진 무관) Dematerialized 된 현대 사회에서 material 측면을 고민하여 physical object를 만드는 것에 대한 연구가 인상적이었다. 제목이 너무 복잡해서 발표자가 설명하기 전까진 이게 뭐야.. 알 수 없는 공대의 세계인가 싶었다. 세련됐다고 할 순 없지만 열의와 진심이 느껴지는 발표자는 서론을 통해 연구 주제를 잘 설명해줬다. 현대 기술 사회에서 기술과의 상호작용이 대부분 screen-based인 걸 dematerialization의 예시로 들었다. 편의를 위해 다양한 task를 screen 기반 구현으로 통일시키다보니 도구 각각이 가지던 physical routine, physical richeness가 사라지는 것에 대한 문제의식을 가졌다. 그래서 연구자는 다양한 도구의 physical form을 되찾는 노력을 한다. 이 과정에서 material(재질)을 통해 디자인적 요소를 살려냈다. 전혀 생각해보지 못한 측면을 파고드는 연구 방향성이 매력적이었다. 새롭기만 한 것이 아니라, 기술을 통해 아날로그를 재현한다는 점에서 의미도 있어 보였다. 멋있는 서론에 묻히지 않을 만큼 예쁜 도구의 prototype(황동, 대리석, 콘크리트, 나무 등을 사용한 실시간 날씨 및 일정 알리미, 나무와 스크린을 접목한 스케줄 관리 및 교통 정보 제공 도구)도 사고 싶은 생각이 들 정도로 완성도가 있었다. 이에 대한 심사 교수님들의 평가도 인상적이었다. 본인은 디자인 문외한이라고 밝힌 김주호 교수는 전반적인 연구의 절차 및 방향성에 대해 조언을 해줬다. 전공 분야가 아닌데 조언할 부분을 찾아내는 것도 능력이다. 디자인 연구의 이론적 토대(conceptual framework)가 탄탄한 점을 칭찬했다. 그렇지만 연구의 의미에 대해서는 끝없이 고민할 필요가 있다고 했다. “너가 한 게 100배가 되면 그게 무슨 의미 일까? prototype이 100개가 생기면 그땐 어떤 의미가 생기는 건가?”라는 질문을 스스로에게 던져보라고 조언했다. 프라이탁 가방을 메고 지각을 해서 나에게 강렬한 인상을 남긴 이병주 교수도 훌륭한 피드백을 남겼다. 이 전 발표자에겐 “predictive brain”에 대해 알아보라고 간접적인 도움을 줬다. 수동적 인간을 가정하는 연구의 본질적인 한계에 대해서 고민해보라고 말하는데 그치지 않고 구체적인 리소스를 알려준다는 점에서 훌륭한 피드백이었다. 이 발표자에겐 “materiality”라는 게 재질에 한정되는 게 아니고 버튼을 누를 때의 강도 등 다양한 요소를 포함하는 개념이라고 전문적인 조언을 해줬다(전공이 기계공학이었다). 그러고는 기계공학이 아닌 디자인 전공 발표자가 materiality를 다루려면 “현상학적 건축”이라는 개념으로 접근하는 게 좋을 수도 있다는 전략적인 조언도 해줬다. 두 교수의 코멘트는 짧지만, 청중인 나에게도 기억이 남을 정도로 훌륭했다. Case Study Case Study 세션은 Paper 세션과 버금갈 만큼의 비중이다. 학계 뿐만 아니라 기업에서도 나와 UI/UX 적용 사례, 앱 개발 사례, 음성디자인 인터페이스 디자인 사례 등 다양한 주제를 다룬다. 나는 SK Telecom이 한 Tmap x NUGU 음성 인터페이스 디자인 사례 발표에 참석했다. 기업 발표는 처음 듣는 거였는데, 학생 발표와 다른 점이 보였다. 일단 발표자가 떠는 기색 없이 차분하게 발표를 잘 하는 데서 이런 자리에 익숙한 게 보였다. 듣는 사람들에게 잘 보여야 하는 을의 입장이라기보다 “우리 정보 공유해줄게” 포스를 풍기는 자신감 있는 태도였다. 실제로 발표 시간 내내 찰칵찰칵 소리가 들릴 정도로 사람들이 계속 사진을 찍었다. 대기업의 업무 절차, 고민, 아이디어 등등을 담은 피피티에서 배워갈 점이 많아서 그랬나 보다. 발표 중간중간에 짤막한 동영상을 넣는 것도 부가적이지만 재밌는 발표 전략으로 보였다. 터치 -&gt; 음성 인터페이스 전환에 따라 디자이너도 일이 많아졌다 대기업의 업무 절차는 매우 꼼꼼했다. 기획부터 구현, 테스트까지 일련의 절차에 대한 가이드라인이 뚜렷했다. 우선, 음성 인터페이스가 기존의 터치 인터페이스보다 작동 depth가 얕다는 걸 기회로 인식하고 기획을 시작했다. 길 안내를 위해 터치를 5번 정도 해야 하던 것을 음성 명령 하나로 대체할 수 있다는 것이다. 이후에 네비게이션 관련 소비자의 구체적 needs를 파악하고, 원칙을 세워 어떤 needs를 반영할지, 안 할지를 정한다. 소비자가 네비게이션에게 엔터테인먼트 기능을 바란다고 조사에서 드러났어도, 안전 내비 원칙 상 동영상 기능은 제공하지 않는다는 식이다. 또한, 환경에 따른 현실적 제약을 고려해 서비스 범위를 명확히 한다. 운전 중인 사용자의 인지 능력은 50퍼센트밖에 안 된다는 한계를 고려해서 대표적 기능만을 제공한다. 핵심 기능에 대한 구상이 어느 정도 진전되면 target 고객의 입장에서 다시 한번 생각한다. Tmap 주요 고객층은 30-50대 남성으로 매우 성격이 급하고 디지털 literacy가 낮은 게 특징이니 기존의 인터페이스를 최대한 살린다. 그런 다음에야 관련 분야 research를 진행한다. 음성 인터페이스 관련 교재를 정해 스터디를 하기도 하고, 타사 서비스에 대한 case study도 한단다. 이후에 본격적인 디자인이 이루어지고, 결과물이 서비스로 거듭나기 전에 꼼꼼한 test 과정도 빼먹지 않는다. 디자인 과정에서 여러 고민이 있었고, 그에 따라 아이디어를 발전시키고 의사결정을 했다. 버튼 위치, 화면 구성, 폰트, 검색 시 노출되는 리스트 개수, 내비 대사의 정보량 등등 모든 의사결정에는 그에 선행하는 고민과 토론 과정이 있었다는 데서 기업은 기업이구나 싶었다. 터치가 아닌 음성 인터페이스라는 특성상 고려할 것이 몇 가지가 있었는데, 두 가지가 기억난다. 첫째는 정보량에 대한 부분이다. 사용자에게 제공할 수 있는 정보를 많이 가지고 있지만, 사용자가 가만히 기다리며 귀로 정보를 접하는 상황이므로 정보 개수를 최대 3개로 제한했다고 한다. “주유소 알려줘”라는 요청에 대해 가는 길에 있는 주유소, 가는 길은 아니지만 거리가 가까운 주요소, 가격이 싼 주요소 등 다양한 정보가 있지만 적절한 것 몇 개만 사용자에게 제공하는 식이다. 둘째는 음성 인식의 한계를 고려해서 유도성 대사(?)를 만든다는 부분이다. 같은 기능이더라도 사람마다 말하는 방식이 많은데, 그 모든 것을 이해하도록 음성인식 기술을 구현하는 건 쉽지 않다. “몇 번째” 장소로 갈까요? 이렇게 하려면 “안내 시작”이라고 말하세요, 같이 사용자의 대답을 특정 방향으로 유도하는 꾀를 썼다. 이게 다 고민의 결과물 대기업의 힘은 사후 관리에서도 드러났다. 내가 주변에서 흔히 볼 수 있는 프로젝트는(주로 대학생) 데드라인이 가까워질수록 노력이 많이 투입되고, 제출과 동시에 프로젝트가 끝이 나는 순서였다. 기업은 서비스 출시 이후에도 유지 보수에 많은 자원을 투자한다. 사용자 로그 데이터를 통해 어떤 명령어가 많이 유입되는지 분석하고, Tmap 이용률이 타사 음성인식 서비스에 비해 높은 이유는 차 안이라는 개인적 공간에서 사용하기 때문이라는 분석도 내놓고, 생각보다 많은 사람이 AI와 잡담하기를 바란다는 점을 파악한다. 이 모든 데이터는 추후 업데이트에 반영된다고 한다. 기업의 규모 있는 프로젝트를 엿볼 수 있는 좋은 시간이었다." />
<meta property="og:description" content="학회 구성 Keynote speech Tutorial Workshop Panel Discussion Case Study Papers Doctoral Consortium Posters Creative Award (진한 표시는 내가 참석한 것들) 학회는 2박 3일간 빽빽하게 진행됐다. 여러 가지 프로그램이 준비되어 있기에 내 목적과 관심사에 맞게 프로그램을 선택하는 게 중요했다. 더군다나 HCI는 융합학문이기 때문에 다른 학회보다도 더 다양한 주제를 다루는 것 같았다. 일례로, paper 발표 세션은 디자인, 기술, 인문 세 분야로 분류되어 있었다. 내가 주로 관심을 가졌던 건 machine learning을 활용한 기술이다. 그 외에도 VR과 디자인 관련 프로그램도 참가했다. Keynote Speech Everyday Interaction with Machine Learning이라는 제목으로 James Forgarty라는 교수님이 한 시간 동안 발표를 했다. 지각한 탓에 앞 20분 정도는 놓쳤지만 그래도 나머지 40분 동안 HCI라는 학문이 어떤 문제를 염두에 두는지 큰 그림을 접할 수 있었다. 기술 연구의 방향성에 대해서도 다시 생각해볼 수 있는 시간이었다. 이걸 듣기 전까지 내가 참여했던 건 주로 paper 발표 세션이었다. 주로 석사생들이 자신이 연구한 내용에 대해 20분 남짓한 시간 동안 발표하는 시간이었다. Wordle을 이용한 워드 시각화, data noise 증폭해서 예술작품 만들기, 휘발성 사진에 대한 연구, 분산표상기법을 활용한 단어의 비표준 의미 탐지, 시각장애인들을 위한 TouchPhoto 기술 등등… 각각의 연구는 완성도가 높아 보였지만, 이게 왜 HCI지?라는 생각이 계속 맴돌았다. 특정 분야에 대한 기술 연구에서는 human이 어디에 들어가는지 찾기 어려웠고, 인문적 접근에선 computer가 어디에 있는지 모르겠다는 느낌이었다. Forgarty 교수도 석사생들과 마찬가지로 자신의 연구를 소개했는데, 다른 점 중 하나는 연구 배경에 대해서 많은 고민을 한 걸 보여줬다는 거다. 결국엔 어떤 기술을 구현해서 소프트웨어를 만든 격인데, 왜 그걸 만들었나에 대해서 풍부한 이유가 있었다. 이 기술과 관련된 얕은 서론이 아니라 공감할 수 있는 문제에 대한 해결책으로써 기술을 제시한다는 점이 인상적이었다. 예를 들어, CueFlik이라는 이미지 검색 도우미(?)를 소개하기에 앞서 이미지 검색의 어려움을 예시로 들었다. Category 구조에서 폴더를 선택해가면서 선택지를 좁혀가는 것과 방대한 자료에서 키워드 기반으로 원하는 걸 찾아내는 건 사고의 구조가 근본적으로 다른 문제라는 점부터 시작했다. 그러고는 깔끔한 스피커 이미지를 얻기 위해서 검색 엔진에 ‘stereo’라고 검색을 하면 지저분한 배경에 있는 스피커는 물론이고 stereo라는 이름을 가진 가수까지 검색된다는 문제점을 지적했다. 이미지의 태그, 이미지 주변 텍스트를 기반으로 검색하는 엔진 특성상 ‘stereo on white background’는 해결책이 되지 못한다. 이러한 문제 상황에서 CueFlik이 지원하는 기능은 몇천 장에 이르는 stereo 검색 결과에서 사용자가 needs에 맞는 사진 몇 장을 고르면, 나머지 사진에 대해 macihe learning기반 classification을 수행해준다는 것이다. 내가 고른 기준에 따라 실시간으로 이미지가 sorting되면 원하는 이미지 여러 장을 손쉽게 찾을 수 있다는 점을 강조했다. 인간이 겪는 문제를 해주는 컴퓨터가 해결해준다는 점에서 HCI 느낌이 났다. 두 번째는 HCI의 이론적인 내용에 대해서도 강조했다는 점이다. 나처럼 HCI를 잘 모르는 사람은 HCI가 human과 computer의 interaction을 연구하는 분야구나라는 막연한 느낌밖에 없을 거다. HCI의 이론적 토대와 HCI에서 고민하는 주제들을 보여주니 HCI라는 학문이 조금은 더 와닿았다. 기억에 남는 건 HCI 연구할 떄 염두에 두어야 할 주제 5가지 : Address / Attention / Action / Alignment / Accident. 쉽게 말하면 Attention은 컴퓨터가 나에게 집중하는 중인지 보이나? Action은 컴퓨터에게 어떤 방식으로 말하나? Alignment는 컴퓨터가 잘 하고 있는지 어떻게 아나?라고 했다. Address와 Accident에 대한 쉬운 설명은 놓쳤다ㅠ. Interaction이라는 모호한 개념을 다섯 가지 부분으로 세분화하니 좀 더 와닿았다. 학회에서 소개하는 여러 연구을 다섯 가지 분류법으로 나눠도 얼추 맞는 거 같다. 이런게 다 HCI의 출발점이 아닐까 세 번째는 본질적인 질문을 해야한다는 거. 이건 뻔한 말이지만 자주 까먹는 점이다. 게다가 뻔한 내용이라 아우라 있는 교수 정도가 말해야 들어오는 어려운 메시지다ㅋㅋ. 기술을 위한 기술을 경계해야 한다는 포인트가 있었다. Personal data, self-tracking, machine learning, visualization이라는 멋진 키워드로 무장한 기술도 사실 쓸모 없을 수도 있다는 얘기를 했다. 개인의 이동 기록, 운동 기록 등등을 종합해서 사용자가 학생인지, 회사원인지를 classify해주는 서비스가 과연 사용자의 needs를 고려한 것인지 의문을 제기했다. 항상 사람의 needs를 먼저 파악하는 게 먼저고, 그에 따른 기술 구현을 생각해야 한다고 했다. 물론, 어떤 회사 입장에선 사용자 데이터를 토대로 고객의 신상을 추론하는 게 중요할 수 있으니 위의 기술도 기업 needs엔 맞는 거라고 할 수 있겠다. 그리고 결론 부분에서 강조한 not data about us, but for us. 데이터가 넘치는 세상이라지만 사실 내 개인 데이터는 구글, 페이스북이 쥐고 있는 형국이다. Big data 그 자체에 관서만 이야기할 뿐만 아니라 그것의 사용처가 human을 위하는 방향일 필요가 있다는 점을 강조했다. 그리고 machine learning에 관련해서는 그것이 doing many things on behalf of us, without explicit ordering이 현재 상황이라고 했다. 사용자의 편의를 위해 자동화된 부분이 많다는 얘기로 이해했다. 이럴 때 일수록 그 기술을 제어할 방법에 대한 연구가 필요하고, 인간이 효과적으로 제어하기 위해선 language based approach가 필요하다고 했다(human language를 통한 기술 제어가 이 교수의 관심사인 듯?). HCI라는 학문이 태어난 게 이런 맥락에서 아닐까 싶었다. Doctoral Consortium Consortium이란 게 무슨 뜻인지도 모르고 doctoral이라는 단어에 끌려서 갔다. 박사들이니까 좀 더 내공 있는 모습을 볼 수 있겠지! 기대를 하며. 기대에 부응하는 시간이었다. 김주호 교수의 사회로 세션이 시작됐는데 해외 유수 학회의 경우 doctoral consortium이 그 꽃이라고 했다. 열띤 논쟁과 피드백이 오가는 자리이고, 큰 학자가 되기 위한 등용문이라고 했다. 소개대로 paper session보다 깊이 있는 발표가 이뤄졌다. 박사 5명이 각자 연구 주제에 대해서 15분가량 발표하고, 나머지 15분 동안은 심사 교수들 및 다른 박사들의 피드백을 받았다. 참가자가 많지 않지만 다들 진지한 게 이런 느낌(사진 무관) Dematerialized 된 현대 사회에서 material 측면을 고민하여 physical object를 만드는 것에 대한 연구가 인상적이었다. 제목이 너무 복잡해서 발표자가 설명하기 전까진 이게 뭐야.. 알 수 없는 공대의 세계인가 싶었다. 세련됐다고 할 순 없지만 열의와 진심이 느껴지는 발표자는 서론을 통해 연구 주제를 잘 설명해줬다. 현대 기술 사회에서 기술과의 상호작용이 대부분 screen-based인 걸 dematerialization의 예시로 들었다. 편의를 위해 다양한 task를 screen 기반 구현으로 통일시키다보니 도구 각각이 가지던 physical routine, physical richeness가 사라지는 것에 대한 문제의식을 가졌다. 그래서 연구자는 다양한 도구의 physical form을 되찾는 노력을 한다. 이 과정에서 material(재질)을 통해 디자인적 요소를 살려냈다. 전혀 생각해보지 못한 측면을 파고드는 연구 방향성이 매력적이었다. 새롭기만 한 것이 아니라, 기술을 통해 아날로그를 재현한다는 점에서 의미도 있어 보였다. 멋있는 서론에 묻히지 않을 만큼 예쁜 도구의 prototype(황동, 대리석, 콘크리트, 나무 등을 사용한 실시간 날씨 및 일정 알리미, 나무와 스크린을 접목한 스케줄 관리 및 교통 정보 제공 도구)도 사고 싶은 생각이 들 정도로 완성도가 있었다. 이에 대한 심사 교수님들의 평가도 인상적이었다. 본인은 디자인 문외한이라고 밝힌 김주호 교수는 전반적인 연구의 절차 및 방향성에 대해 조언을 해줬다. 전공 분야가 아닌데 조언할 부분을 찾아내는 것도 능력이다. 디자인 연구의 이론적 토대(conceptual framework)가 탄탄한 점을 칭찬했다. 그렇지만 연구의 의미에 대해서는 끝없이 고민할 필요가 있다고 했다. “너가 한 게 100배가 되면 그게 무슨 의미 일까? prototype이 100개가 생기면 그땐 어떤 의미가 생기는 건가?”라는 질문을 스스로에게 던져보라고 조언했다. 프라이탁 가방을 메고 지각을 해서 나에게 강렬한 인상을 남긴 이병주 교수도 훌륭한 피드백을 남겼다. 이 전 발표자에겐 “predictive brain”에 대해 알아보라고 간접적인 도움을 줬다. 수동적 인간을 가정하는 연구의 본질적인 한계에 대해서 고민해보라고 말하는데 그치지 않고 구체적인 리소스를 알려준다는 점에서 훌륭한 피드백이었다. 이 발표자에겐 “materiality”라는 게 재질에 한정되는 게 아니고 버튼을 누를 때의 강도 등 다양한 요소를 포함하는 개념이라고 전문적인 조언을 해줬다(전공이 기계공학이었다). 그러고는 기계공학이 아닌 디자인 전공 발표자가 materiality를 다루려면 “현상학적 건축”이라는 개념으로 접근하는 게 좋을 수도 있다는 전략적인 조언도 해줬다. 두 교수의 코멘트는 짧지만, 청중인 나에게도 기억이 남을 정도로 훌륭했다. Case Study Case Study 세션은 Paper 세션과 버금갈 만큼의 비중이다. 학계 뿐만 아니라 기업에서도 나와 UI/UX 적용 사례, 앱 개발 사례, 음성디자인 인터페이스 디자인 사례 등 다양한 주제를 다룬다. 나는 SK Telecom이 한 Tmap x NUGU 음성 인터페이스 디자인 사례 발표에 참석했다. 기업 발표는 처음 듣는 거였는데, 학생 발표와 다른 점이 보였다. 일단 발표자가 떠는 기색 없이 차분하게 발표를 잘 하는 데서 이런 자리에 익숙한 게 보였다. 듣는 사람들에게 잘 보여야 하는 을의 입장이라기보다 “우리 정보 공유해줄게” 포스를 풍기는 자신감 있는 태도였다. 실제로 발표 시간 내내 찰칵찰칵 소리가 들릴 정도로 사람들이 계속 사진을 찍었다. 대기업의 업무 절차, 고민, 아이디어 등등을 담은 피피티에서 배워갈 점이 많아서 그랬나 보다. 발표 중간중간에 짤막한 동영상을 넣는 것도 부가적이지만 재밌는 발표 전략으로 보였다. 터치 -&gt; 음성 인터페이스 전환에 따라 디자이너도 일이 많아졌다 대기업의 업무 절차는 매우 꼼꼼했다. 기획부터 구현, 테스트까지 일련의 절차에 대한 가이드라인이 뚜렷했다. 우선, 음성 인터페이스가 기존의 터치 인터페이스보다 작동 depth가 얕다는 걸 기회로 인식하고 기획을 시작했다. 길 안내를 위해 터치를 5번 정도 해야 하던 것을 음성 명령 하나로 대체할 수 있다는 것이다. 이후에 네비게이션 관련 소비자의 구체적 needs를 파악하고, 원칙을 세워 어떤 needs를 반영할지, 안 할지를 정한다. 소비자가 네비게이션에게 엔터테인먼트 기능을 바란다고 조사에서 드러났어도, 안전 내비 원칙 상 동영상 기능은 제공하지 않는다는 식이다. 또한, 환경에 따른 현실적 제약을 고려해 서비스 범위를 명확히 한다. 운전 중인 사용자의 인지 능력은 50퍼센트밖에 안 된다는 한계를 고려해서 대표적 기능만을 제공한다. 핵심 기능에 대한 구상이 어느 정도 진전되면 target 고객의 입장에서 다시 한번 생각한다. Tmap 주요 고객층은 30-50대 남성으로 매우 성격이 급하고 디지털 literacy가 낮은 게 특징이니 기존의 인터페이스를 최대한 살린다. 그런 다음에야 관련 분야 research를 진행한다. 음성 인터페이스 관련 교재를 정해 스터디를 하기도 하고, 타사 서비스에 대한 case study도 한단다. 이후에 본격적인 디자인이 이루어지고, 결과물이 서비스로 거듭나기 전에 꼼꼼한 test 과정도 빼먹지 않는다. 디자인 과정에서 여러 고민이 있었고, 그에 따라 아이디어를 발전시키고 의사결정을 했다. 버튼 위치, 화면 구성, 폰트, 검색 시 노출되는 리스트 개수, 내비 대사의 정보량 등등 모든 의사결정에는 그에 선행하는 고민과 토론 과정이 있었다는 데서 기업은 기업이구나 싶었다. 터치가 아닌 음성 인터페이스라는 특성상 고려할 것이 몇 가지가 있었는데, 두 가지가 기억난다. 첫째는 정보량에 대한 부분이다. 사용자에게 제공할 수 있는 정보를 많이 가지고 있지만, 사용자가 가만히 기다리며 귀로 정보를 접하는 상황이므로 정보 개수를 최대 3개로 제한했다고 한다. “주유소 알려줘”라는 요청에 대해 가는 길에 있는 주유소, 가는 길은 아니지만 거리가 가까운 주요소, 가격이 싼 주요소 등 다양한 정보가 있지만 적절한 것 몇 개만 사용자에게 제공하는 식이다. 둘째는 음성 인식의 한계를 고려해서 유도성 대사(?)를 만든다는 부분이다. 같은 기능이더라도 사람마다 말하는 방식이 많은데, 그 모든 것을 이해하도록 음성인식 기술을 구현하는 건 쉽지 않다. “몇 번째” 장소로 갈까요? 이렇게 하려면 “안내 시작”이라고 말하세요, 같이 사용자의 대답을 특정 방향으로 유도하는 꾀를 썼다. 이게 다 고민의 결과물 대기업의 힘은 사후 관리에서도 드러났다. 내가 주변에서 흔히 볼 수 있는 프로젝트는(주로 대학생) 데드라인이 가까워질수록 노력이 많이 투입되고, 제출과 동시에 프로젝트가 끝이 나는 순서였다. 기업은 서비스 출시 이후에도 유지 보수에 많은 자원을 투자한다. 사용자 로그 데이터를 통해 어떤 명령어가 많이 유입되는지 분석하고, Tmap 이용률이 타사 음성인식 서비스에 비해 높은 이유는 차 안이라는 개인적 공간에서 사용하기 때문이라는 분석도 내놓고, 생각보다 많은 사람이 AI와 잡담하기를 바란다는 점을 파악한다. 이 모든 데이터는 추후 업데이트에 반영된다고 한다. 기업의 규모 있는 프로젝트를 엿볼 수 있는 좋은 시간이었다." />
<link rel="canonical" href="http://localhost:4000/2018-02-01/HCIKorea2018/" />
<meta property="og:url" content="http://localhost:4000/2018-02-01/HCIKorea2018/" />
<meta property="og:site_name" content="Hwijeen Ahn" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-02-01T00:00:00+09:00" />
<script type="application/ld+json">
{"headline":"HCI Korea 2018 감상평","dateModified":"2018-02-01T00:00:00+09:00","datePublished":"2018-02-01T00:00:00+09:00","description":"학회 구성 Keynote speech Tutorial Workshop Panel Discussion Case Study Papers Doctoral Consortium Posters Creative Award (진한 표시는 내가 참석한 것들) 학회는 2박 3일간 빽빽하게 진행됐다. 여러 가지 프로그램이 준비되어 있기에 내 목적과 관심사에 맞게 프로그램을 선택하는 게 중요했다. 더군다나 HCI는 융합학문이기 때문에 다른 학회보다도 더 다양한 주제를 다루는 것 같았다. 일례로, paper 발표 세션은 디자인, 기술, 인문 세 분야로 분류되어 있었다. 내가 주로 관심을 가졌던 건 machine learning을 활용한 기술이다. 그 외에도 VR과 디자인 관련 프로그램도 참가했다. Keynote Speech Everyday Interaction with Machine Learning이라는 제목으로 James Forgarty라는 교수님이 한 시간 동안 발표를 했다. 지각한 탓에 앞 20분 정도는 놓쳤지만 그래도 나머지 40분 동안 HCI라는 학문이 어떤 문제를 염두에 두는지 큰 그림을 접할 수 있었다. 기술 연구의 방향성에 대해서도 다시 생각해볼 수 있는 시간이었다. 이걸 듣기 전까지 내가 참여했던 건 주로 paper 발표 세션이었다. 주로 석사생들이 자신이 연구한 내용에 대해 20분 남짓한 시간 동안 발표하는 시간이었다. Wordle을 이용한 워드 시각화, data noise 증폭해서 예술작품 만들기, 휘발성 사진에 대한 연구, 분산표상기법을 활용한 단어의 비표준 의미 탐지, 시각장애인들을 위한 TouchPhoto 기술 등등… 각각의 연구는 완성도가 높아 보였지만, 이게 왜 HCI지?라는 생각이 계속 맴돌았다. 특정 분야에 대한 기술 연구에서는 human이 어디에 들어가는지 찾기 어려웠고, 인문적 접근에선 computer가 어디에 있는지 모르겠다는 느낌이었다. Forgarty 교수도 석사생들과 마찬가지로 자신의 연구를 소개했는데, 다른 점 중 하나는 연구 배경에 대해서 많은 고민을 한 걸 보여줬다는 거다. 결국엔 어떤 기술을 구현해서 소프트웨어를 만든 격인데, 왜 그걸 만들었나에 대해서 풍부한 이유가 있었다. 이 기술과 관련된 얕은 서론이 아니라 공감할 수 있는 문제에 대한 해결책으로써 기술을 제시한다는 점이 인상적이었다. 예를 들어, CueFlik이라는 이미지 검색 도우미(?)를 소개하기에 앞서 이미지 검색의 어려움을 예시로 들었다. Category 구조에서 폴더를 선택해가면서 선택지를 좁혀가는 것과 방대한 자료에서 키워드 기반으로 원하는 걸 찾아내는 건 사고의 구조가 근본적으로 다른 문제라는 점부터 시작했다. 그러고는 깔끔한 스피커 이미지를 얻기 위해서 검색 엔진에 ‘stereo’라고 검색을 하면 지저분한 배경에 있는 스피커는 물론이고 stereo라는 이름을 가진 가수까지 검색된다는 문제점을 지적했다. 이미지의 태그, 이미지 주변 텍스트를 기반으로 검색하는 엔진 특성상 ‘stereo on white background’는 해결책이 되지 못한다. 이러한 문제 상황에서 CueFlik이 지원하는 기능은 몇천 장에 이르는 stereo 검색 결과에서 사용자가 needs에 맞는 사진 몇 장을 고르면, 나머지 사진에 대해 macihe learning기반 classification을 수행해준다는 것이다. 내가 고른 기준에 따라 실시간으로 이미지가 sorting되면 원하는 이미지 여러 장을 손쉽게 찾을 수 있다는 점을 강조했다. 인간이 겪는 문제를 해주는 컴퓨터가 해결해준다는 점에서 HCI 느낌이 났다. 두 번째는 HCI의 이론적인 내용에 대해서도 강조했다는 점이다. 나처럼 HCI를 잘 모르는 사람은 HCI가 human과 computer의 interaction을 연구하는 분야구나라는 막연한 느낌밖에 없을 거다. HCI의 이론적 토대와 HCI에서 고민하는 주제들을 보여주니 HCI라는 학문이 조금은 더 와닿았다. 기억에 남는 건 HCI 연구할 떄 염두에 두어야 할 주제 5가지 : Address / Attention / Action / Alignment / Accident. 쉽게 말하면 Attention은 컴퓨터가 나에게 집중하는 중인지 보이나? Action은 컴퓨터에게 어떤 방식으로 말하나? Alignment는 컴퓨터가 잘 하고 있는지 어떻게 아나?라고 했다. Address와 Accident에 대한 쉬운 설명은 놓쳤다ㅠ. Interaction이라는 모호한 개념을 다섯 가지 부분으로 세분화하니 좀 더 와닿았다. 학회에서 소개하는 여러 연구을 다섯 가지 분류법으로 나눠도 얼추 맞는 거 같다. 이런게 다 HCI의 출발점이 아닐까 세 번째는 본질적인 질문을 해야한다는 거. 이건 뻔한 말이지만 자주 까먹는 점이다. 게다가 뻔한 내용이라 아우라 있는 교수 정도가 말해야 들어오는 어려운 메시지다ㅋㅋ. 기술을 위한 기술을 경계해야 한다는 포인트가 있었다. Personal data, self-tracking, machine learning, visualization이라는 멋진 키워드로 무장한 기술도 사실 쓸모 없을 수도 있다는 얘기를 했다. 개인의 이동 기록, 운동 기록 등등을 종합해서 사용자가 학생인지, 회사원인지를 classify해주는 서비스가 과연 사용자의 needs를 고려한 것인지 의문을 제기했다. 항상 사람의 needs를 먼저 파악하는 게 먼저고, 그에 따른 기술 구현을 생각해야 한다고 했다. 물론, 어떤 회사 입장에선 사용자 데이터를 토대로 고객의 신상을 추론하는 게 중요할 수 있으니 위의 기술도 기업 needs엔 맞는 거라고 할 수 있겠다. 그리고 결론 부분에서 강조한 not data about us, but for us. 데이터가 넘치는 세상이라지만 사실 내 개인 데이터는 구글, 페이스북이 쥐고 있는 형국이다. Big data 그 자체에 관서만 이야기할 뿐만 아니라 그것의 사용처가 human을 위하는 방향일 필요가 있다는 점을 강조했다. 그리고 machine learning에 관련해서는 그것이 doing many things on behalf of us, without explicit ordering이 현재 상황이라고 했다. 사용자의 편의를 위해 자동화된 부분이 많다는 얘기로 이해했다. 이럴 때 일수록 그 기술을 제어할 방법에 대한 연구가 필요하고, 인간이 효과적으로 제어하기 위해선 language based approach가 필요하다고 했다(human language를 통한 기술 제어가 이 교수의 관심사인 듯?). HCI라는 학문이 태어난 게 이런 맥락에서 아닐까 싶었다. Doctoral Consortium Consortium이란 게 무슨 뜻인지도 모르고 doctoral이라는 단어에 끌려서 갔다. 박사들이니까 좀 더 내공 있는 모습을 볼 수 있겠지! 기대를 하며. 기대에 부응하는 시간이었다. 김주호 교수의 사회로 세션이 시작됐는데 해외 유수 학회의 경우 doctoral consortium이 그 꽃이라고 했다. 열띤 논쟁과 피드백이 오가는 자리이고, 큰 학자가 되기 위한 등용문이라고 했다. 소개대로 paper session보다 깊이 있는 발표가 이뤄졌다. 박사 5명이 각자 연구 주제에 대해서 15분가량 발표하고, 나머지 15분 동안은 심사 교수들 및 다른 박사들의 피드백을 받았다. 참가자가 많지 않지만 다들 진지한 게 이런 느낌(사진 무관) Dematerialized 된 현대 사회에서 material 측면을 고민하여 physical object를 만드는 것에 대한 연구가 인상적이었다. 제목이 너무 복잡해서 발표자가 설명하기 전까진 이게 뭐야.. 알 수 없는 공대의 세계인가 싶었다. 세련됐다고 할 순 없지만 열의와 진심이 느껴지는 발표자는 서론을 통해 연구 주제를 잘 설명해줬다. 현대 기술 사회에서 기술과의 상호작용이 대부분 screen-based인 걸 dematerialization의 예시로 들었다. 편의를 위해 다양한 task를 screen 기반 구현으로 통일시키다보니 도구 각각이 가지던 physical routine, physical richeness가 사라지는 것에 대한 문제의식을 가졌다. 그래서 연구자는 다양한 도구의 physical form을 되찾는 노력을 한다. 이 과정에서 material(재질)을 통해 디자인적 요소를 살려냈다. 전혀 생각해보지 못한 측면을 파고드는 연구 방향성이 매력적이었다. 새롭기만 한 것이 아니라, 기술을 통해 아날로그를 재현한다는 점에서 의미도 있어 보였다. 멋있는 서론에 묻히지 않을 만큼 예쁜 도구의 prototype(황동, 대리석, 콘크리트, 나무 등을 사용한 실시간 날씨 및 일정 알리미, 나무와 스크린을 접목한 스케줄 관리 및 교통 정보 제공 도구)도 사고 싶은 생각이 들 정도로 완성도가 있었다. 이에 대한 심사 교수님들의 평가도 인상적이었다. 본인은 디자인 문외한이라고 밝힌 김주호 교수는 전반적인 연구의 절차 및 방향성에 대해 조언을 해줬다. 전공 분야가 아닌데 조언할 부분을 찾아내는 것도 능력이다. 디자인 연구의 이론적 토대(conceptual framework)가 탄탄한 점을 칭찬했다. 그렇지만 연구의 의미에 대해서는 끝없이 고민할 필요가 있다고 했다. “너가 한 게 100배가 되면 그게 무슨 의미 일까? prototype이 100개가 생기면 그땐 어떤 의미가 생기는 건가?”라는 질문을 스스로에게 던져보라고 조언했다. 프라이탁 가방을 메고 지각을 해서 나에게 강렬한 인상을 남긴 이병주 교수도 훌륭한 피드백을 남겼다. 이 전 발표자에겐 “predictive brain”에 대해 알아보라고 간접적인 도움을 줬다. 수동적 인간을 가정하는 연구의 본질적인 한계에 대해서 고민해보라고 말하는데 그치지 않고 구체적인 리소스를 알려준다는 점에서 훌륭한 피드백이었다. 이 발표자에겐 “materiality”라는 게 재질에 한정되는 게 아니고 버튼을 누를 때의 강도 등 다양한 요소를 포함하는 개념이라고 전문적인 조언을 해줬다(전공이 기계공학이었다). 그러고는 기계공학이 아닌 디자인 전공 발표자가 materiality를 다루려면 “현상학적 건축”이라는 개념으로 접근하는 게 좋을 수도 있다는 전략적인 조언도 해줬다. 두 교수의 코멘트는 짧지만, 청중인 나에게도 기억이 남을 정도로 훌륭했다. Case Study Case Study 세션은 Paper 세션과 버금갈 만큼의 비중이다. 학계 뿐만 아니라 기업에서도 나와 UI/UX 적용 사례, 앱 개발 사례, 음성디자인 인터페이스 디자인 사례 등 다양한 주제를 다룬다. 나는 SK Telecom이 한 Tmap x NUGU 음성 인터페이스 디자인 사례 발표에 참석했다. 기업 발표는 처음 듣는 거였는데, 학생 발표와 다른 점이 보였다. 일단 발표자가 떠는 기색 없이 차분하게 발표를 잘 하는 데서 이런 자리에 익숙한 게 보였다. 듣는 사람들에게 잘 보여야 하는 을의 입장이라기보다 “우리 정보 공유해줄게” 포스를 풍기는 자신감 있는 태도였다. 실제로 발표 시간 내내 찰칵찰칵 소리가 들릴 정도로 사람들이 계속 사진을 찍었다. 대기업의 업무 절차, 고민, 아이디어 등등을 담은 피피티에서 배워갈 점이 많아서 그랬나 보다. 발표 중간중간에 짤막한 동영상을 넣는 것도 부가적이지만 재밌는 발표 전략으로 보였다. 터치 -&gt; 음성 인터페이스 전환에 따라 디자이너도 일이 많아졌다 대기업의 업무 절차는 매우 꼼꼼했다. 기획부터 구현, 테스트까지 일련의 절차에 대한 가이드라인이 뚜렷했다. 우선, 음성 인터페이스가 기존의 터치 인터페이스보다 작동 depth가 얕다는 걸 기회로 인식하고 기획을 시작했다. 길 안내를 위해 터치를 5번 정도 해야 하던 것을 음성 명령 하나로 대체할 수 있다는 것이다. 이후에 네비게이션 관련 소비자의 구체적 needs를 파악하고, 원칙을 세워 어떤 needs를 반영할지, 안 할지를 정한다. 소비자가 네비게이션에게 엔터테인먼트 기능을 바란다고 조사에서 드러났어도, 안전 내비 원칙 상 동영상 기능은 제공하지 않는다는 식이다. 또한, 환경에 따른 현실적 제약을 고려해 서비스 범위를 명확히 한다. 운전 중인 사용자의 인지 능력은 50퍼센트밖에 안 된다는 한계를 고려해서 대표적 기능만을 제공한다. 핵심 기능에 대한 구상이 어느 정도 진전되면 target 고객의 입장에서 다시 한번 생각한다. Tmap 주요 고객층은 30-50대 남성으로 매우 성격이 급하고 디지털 literacy가 낮은 게 특징이니 기존의 인터페이스를 최대한 살린다. 그런 다음에야 관련 분야 research를 진행한다. 음성 인터페이스 관련 교재를 정해 스터디를 하기도 하고, 타사 서비스에 대한 case study도 한단다. 이후에 본격적인 디자인이 이루어지고, 결과물이 서비스로 거듭나기 전에 꼼꼼한 test 과정도 빼먹지 않는다. 디자인 과정에서 여러 고민이 있었고, 그에 따라 아이디어를 발전시키고 의사결정을 했다. 버튼 위치, 화면 구성, 폰트, 검색 시 노출되는 리스트 개수, 내비 대사의 정보량 등등 모든 의사결정에는 그에 선행하는 고민과 토론 과정이 있었다는 데서 기업은 기업이구나 싶었다. 터치가 아닌 음성 인터페이스라는 특성상 고려할 것이 몇 가지가 있었는데, 두 가지가 기억난다. 첫째는 정보량에 대한 부분이다. 사용자에게 제공할 수 있는 정보를 많이 가지고 있지만, 사용자가 가만히 기다리며 귀로 정보를 접하는 상황이므로 정보 개수를 최대 3개로 제한했다고 한다. “주유소 알려줘”라는 요청에 대해 가는 길에 있는 주유소, 가는 길은 아니지만 거리가 가까운 주요소, 가격이 싼 주요소 등 다양한 정보가 있지만 적절한 것 몇 개만 사용자에게 제공하는 식이다. 둘째는 음성 인식의 한계를 고려해서 유도성 대사(?)를 만든다는 부분이다. 같은 기능이더라도 사람마다 말하는 방식이 많은데, 그 모든 것을 이해하도록 음성인식 기술을 구현하는 건 쉽지 않다. “몇 번째” 장소로 갈까요? 이렇게 하려면 “안내 시작”이라고 말하세요, 같이 사용자의 대답을 특정 방향으로 유도하는 꾀를 썼다. 이게 다 고민의 결과물 대기업의 힘은 사후 관리에서도 드러났다. 내가 주변에서 흔히 볼 수 있는 프로젝트는(주로 대학생) 데드라인이 가까워질수록 노력이 많이 투입되고, 제출과 동시에 프로젝트가 끝이 나는 순서였다. 기업은 서비스 출시 이후에도 유지 보수에 많은 자원을 투자한다. 사용자 로그 데이터를 통해 어떤 명령어가 많이 유입되는지 분석하고, Tmap 이용률이 타사 음성인식 서비스에 비해 높은 이유는 차 안이라는 개인적 공간에서 사용하기 때문이라는 분석도 내놓고, 생각보다 많은 사람이 AI와 잡담하기를 바란다는 점을 파악한다. 이 모든 데이터는 추후 업데이트에 반영된다고 한다. 기업의 규모 있는 프로젝트를 엿볼 수 있는 좋은 시간이었다.","url":"http://localhost:4000/2018-02-01/HCIKorea2018/","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018-02-01/HCIKorea2018/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="http://localhost:4000/logo.png">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="http://localhost:4000/logo.png">

  <!-- Google Analytics -->

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-112068488-2', 'auto');
ga('send', 'pageview');

</script>



</head>

<body>
  <div class="content-container">
    


<header>
  <div class="header-small">
    <a href="http://localhost:4000">Hwijeen Ahn</a>
  </div>
</header>
<div class="post">
  <div class="post-title">HCI Korea 2018 감상평</div>
  <span class="post-date">
    <time>01 Feb 2018</time>
  </span>

  
    <span class="post-minute"> 
     8 minute read
    </span>
  


  <div class="post-tag">
    <ul>
      
      <li>
        <a href="http://localhost:4000/tags#HCI">
          <span>HCI</span>
        </a>
      </li>
      
      
      <li>
        <a href="http://localhost:4000/tags#학회">
          <span>학회</span>
        </a>
      </li>
      
      
    </ul>
  </div>

  <h1 id="학회-구성">학회 구성</h1>
<ul>
  <li><strong>Keynote speech</strong></li>
  <li>Tutorial</li>
  <li>Workshop</li>
  <li>Panel Discussion</li>
  <li><strong>Case Study</strong></li>
  <li><strong>Papers</strong></li>
  <li><strong>Doctoral Consortium</strong></li>
  <li>Posters</li>
  <li><strong>Creative Award</strong></li>
</ul>
<font size="3" color="grey"><i>(진한 표시는 내가 참석한 것들)</i></font>

<p>학회는 2박 3일간 빽빽하게 진행됐다. 여러 가지 프로그램이 준비되어 있기에 내 목적과 관심사에 맞게 프로그램을 선택하는 게 중요했다. 더군다나 HCI는 융합학문이기 때문에 다른 학회보다도 더 다양한 주제를 다루는 것 같았다. 일례로, paper 발표 세션은 디자인, 기술, 인문 세 분야로 분류되어 있었다. 내가 주로 관심을 가졌던 건 machine learning을 활용한 기술이다. 그 외에도 VR과 디자인 관련 프로그램도 참가했다.</p>

<h2 id="keynote-speech">Keynote Speech</h2>
<p>Everyday Interaction with Machine Learning이라는 제목으로 James Forgarty라는 교수님이 한 시간 동안 발표를 했다. 지각한 탓에 앞 20분 정도는 놓쳤지만 그래도 나머지 40분 동안 HCI라는 학문이 어떤 문제를 염두에 두는지 큰 그림을 접할 수 있었다. 기술 연구의 방향성에 대해서도 다시 생각해볼 수 있는 시간이었다. 이걸 듣기 전까지 내가 참여했던 건 주로 paper 발표 세션이었다. 주로 석사생들이 자신이 연구한 내용에 대해 20분 남짓한 시간 동안 발표하는 시간이었다. Wordle을 이용한 워드 시각화, data noise 증폭해서 예술작품 만들기, 휘발성 사진에 대한 연구, 분산표상기법을 활용한 단어의 비표준 의미 탐지, 시각장애인들을 위한 TouchPhoto 기술 등등… 각각의 연구는 완성도가 높아 보였지만, 이게 왜 HCI지?라는 생각이 계속 맴돌았다. <strong>특정 분야에 대한 기술 연구에서는 human이 어디에 들어가는지 찾기 어려웠고, 인문적 접근에선 computer가 어디에 있는지 모르겠다는 느낌</strong>이었다.</p>

<p><img src="/assets/img/forgarty.jpg" alt="" /></p>

<p>Forgarty 교수도 석사생들과 마찬가지로 자신의 연구를 소개했는데, 다른 점 중 하나는 연구 배경에 대해서 많은 고민을 한 걸 보여줬다는 거다. 결국엔 어떤 기술을 구현해서 소프트웨어를 만든 격인데, 왜 그걸 만들었나에 대해서 풍부한 이유가 있었다. 이 기술과 관련된 얕은 서론이 아니라 <strong>공감할 수 있는 문제에 대한 해결책으로써 기술</strong>을 제시한다는 점이 인상적이었다. 예를 들어, CueFlik이라는 이미지 검색 도우미(?)를 소개하기에 앞서 이미지 검색의 어려움을 예시로 들었다. Category 구조에서 폴더를 선택해가면서 선택지를 좁혀가는 것과 방대한 자료에서 키워드 기반으로 원하는 걸 찾아내는 건 사고의 구조가 근본적으로 다른 문제라는 점부터 시작했다. 그러고는 깔끔한 스피커 이미지를 얻기 위해서 검색 엔진에 ‘stereo’라고 검색을 하면 지저분한 배경에 있는 스피커는 물론이고 stereo라는 이름을 가진 가수까지 검색된다는 문제점을 지적했다. 이미지의 태그, 이미지 주변 텍스트를 기반으로 검색하는 엔진 특성상 ‘stereo on white background’는 해결책이 되지 못한다. 이러한 문제 상황에서 CueFlik이 지원하는 기능은 몇천 장에 이르는 stereo 검색 결과에서 사용자가 needs에 맞는 사진 몇 장을 고르면, 나머지 사진에 대해 macihe learning기반 classification을 수행해준다는 것이다. 내가 고른 기준에 따라 실시간으로 이미지가 sorting되면 원하는 이미지 여러 장을 손쉽게 찾을 수 있다는 점을 강조했다. 인간이 겪는 문제를 해주는 컴퓨터가 해결해준다는 점에서 HCI 느낌이 났다.</p>

<p>두 번째는 HCI의 이론적인 내용에 대해서도 강조했다는 점이다. 나처럼 HCI를 잘 모르는 사람은 HCI가 human과 computer의 interaction을 연구하는 분야구나라는 막연한 느낌밖에 없을 거다. HCI의 이론적 토대와 HCI에서 고민하는 주제들을 보여주니 HCI라는 학문이 조금은 더 와닿았다. 기억에 남는 건 HCI 연구할 떄 염두에 두어야 할 주제 5가지 : Address / Attention / Action / Alignment / Accident. 쉽게 말하면 Attention은 컴퓨터가 나에게 집중하는 중인지 보이나? Action은 컴퓨터에게 어떤 방식으로 말하나? Alignment는 컴퓨터가 잘 하고 있는지 어떻게 아나?라고 했다. Address와 Accident에 대한 쉬운 설명은 놓쳤다ㅠ. <strong>Interaction이라는 모호한 개념을 다섯 가지 부분으로 세분화</strong>하니 좀 더 와닿았다. 학회에서 소개하는 여러 연구을 다섯 가지 분류법으로 나눠도 얼추 맞는 거 같다.</p>
<p style="text-align:center;"><img src="/assets/img/HCI.jpg" alt="HCI" width="450" align="middle" /><br /><font size="3" color="grey">이런게 다 HCI의 출발점이 아닐까</font></p>

<p>세 번째는 본질적인 질문을 해야한다는 거. 이건 뻔한 말이지만 자주 까먹는 점이다. 게다가 뻔한 내용이라 아우라 있는 교수 정도가 말해야 들어오는 어려운 메시지다ㅋㅋ. 기술을 위한 기술을 경계해야 한다는 포인트가 있었다. Personal data, self-tracking, machine learning, visualization이라는 멋진 키워드로 무장한 기술도 사실 쓸모 없을 수도 있다는 얘기를 했다. 개인의 이동 기록, 운동 기록 등등을 종합해서 사용자가 학생인지, 회사원인지를 classify해주는 서비스가 과연 <strong>사용자의 needs를 고려</strong>한 것인지 의문을 제기했다. 항상 사람의 needs를 먼저 파악하는 게 먼저고, 그에 따른 기술 구현을 생각해야 한다고 했다. 물론, 어떤 회사 입장에선 사용자 데이터를 토대로 고객의 신상을 추론하는 게 중요할 수 있으니 위의 기술도 기업 needs엔 맞는 거라고 할 수 있겠다.</p>

<p>그리고 결론 부분에서 강조한 <strong>not data about us, but for us.</strong> 데이터가 넘치는 세상이라지만 사실 내 개인 데이터는 구글, 페이스북이 쥐고 있는 형국이다. Big data 그 자체에 관서만 이야기할 뿐만 아니라 그것의 사용처가 human을 위하는 방향일 필요가 있다는 점을 강조했다. 그리고 machine learning에 관련해서는 그것이 doing many things on behalf of us, without explicit ordering이 현재 상황이라고 했다. 사용자의 편의를 위해 자동화된 부분이 많다는 얘기로 이해했다. 이럴 때 일수록 그 기술을 제어할 방법에 대한 연구가 필요하고, 인간이 효과적으로 제어하기 위해선 language based approach가 필요하다고 했다(human language를 통한 기술 제어가 이 교수의 관심사인 듯?). HCI라는 학문이 태어난 게 이런 맥락에서 아닐까 싶었다.</p>

<h2 id="doctoral-consortium">Doctoral Consortium</h2>
<p>Consortium이란 게 무슨 뜻인지도 모르고 doctoral이라는 단어에 끌려서 갔다. 박사들이니까 좀 더 내공 있는 모습을 볼 수 있겠지! 기대를 하며. 기대에 부응하는 시간이었다. 김주호 교수의 사회로 세션이 시작됐는데 해외 유수 학회의 경우 doctoral consortium이 그 꽃이라고 했다. 열띤 논쟁과 피드백이 오가는 자리이고, 큰 학자가 되기 위한 등용문이라고 했다. 소개대로 paper session보다 깊이 있는 발표가 이뤄졌다. 박사 5명이 각자 연구 주제에 대해서 15분가량 발표하고, 나머지 15분 동안은 심사 교수들 및 다른 박사들의 피드백을 받았다.</p>
<p style="text-align:center;"><img src="/assets/img/consortium.jpg" alt="HCI" width="550" align="middle" /><br /><font size="3" color="grey">참가자가 많지 않지만 다들 진지한 게 이런 느낌(사진 무관)</font></p>

<p>Dematerialized 된 현대 사회에서 material 측면을 고민하여 physical object를 만드는 것에 대한 연구가 인상적이었다. 제목이 너무 복잡해서 발표자가 설명하기 전까진 이게 뭐야.. 알 수 없는 공대의 세계인가 싶었다. 세련됐다고 할 순 없지만 열의와 진심이 느껴지는 발표자는 서론을 통해 연구 주제를 잘 설명해줬다. 현대 기술 사회에서 기술과의 상호작용이 대부분 screen-based인 걸 dematerialization의 예시로 들었다. 편의를 위해 다양한 task를 screen 기반 구현으로 통일시키다보니 도구 각각이 가지던 physical routine, physical richeness가 사라지는 것에 대한 문제의식을 가졌다. 그래서 연구자는 다양한 도구의 physical form을 되찾는 노력을 한다. 이 과정에서 material(재질)을 통해 디자인적 요소를 살려냈다. 전혀 생각해보지 못한 측면을 파고드는 연구 방향성이 매력적이었다. 새롭기만 한 것이 아니라, <strong>기술을 통해 아날로그를 재현</strong>한다는 점에서 의미도 있어 보였다. 멋있는 서론에 묻히지 않을 만큼 예쁜 도구의 prototype(황동, 대리석, 콘크리트, 나무 등을 사용한 실시간 날씨 및 일정 알리미, 나무와 스크린을 접목한 스케줄 관리 및 교통 정보 제공 도구)도 사고 싶은 생각이 들 정도로 완성도가 있었다.</p>

<p>이에 대한 심사 교수님들의 평가도 인상적이었다. 본인은 디자인 문외한이라고 밝힌 김주호 교수는 전반적인 연구의 절차 및 방향성에 대해 조언을 해줬다. 전공 분야가 아닌데 조언할 부분을 찾아내는 것도 능력이다. 디자인 연구의 이론적 토대(conceptual framework)가 탄탄한 점을 칭찬했다. 그렇지만 연구의 의미에 대해서는 끝없이 고민할 필요가 있다고 했다. <strong>“너가 한 게 100배가 되면 그게 무슨 의미 일까? prototype이 100개가 생기면 그땐 어떤 의미가 생기는 건가?”</strong>라는 질문을 스스로에게 던져보라고 조언했다. 프라이탁 가방을 메고 지각을 해서 나에게 강렬한 인상을 남긴 이병주 교수도 훌륭한 피드백을 남겼다. 이 전 발표자에겐 “predictive brain”에 대해 알아보라고 간접적인 도움을 줬다. 수동적 인간을 가정하는 연구의 본질적인 한계에 대해서 고민해보라고 말하는데 그치지 않고 구체적인 리소스를 알려준다는 점에서 훌륭한 피드백이었다. 이 발표자에겐 “materiality”라는 게 재질에 한정되는 게 아니고 버튼을 누를 때의 강도 등 다양한 요소를 포함하는 개념이라고 전문적인 조언을 해줬다(전공이 기계공학이었다). 그러고는 기계공학이 아닌 디자인 전공 발표자가 materiality를 다루려면 “현상학적 건축”이라는 개념으로 접근하는 게 좋을 수도 있다는 전략적인 조언도 해줬다. 두 교수의 코멘트는 짧지만, 청중인 나에게도 기억이 남을 정도로 훌륭했다.</p>

<h2 id="case-study">Case Study</h2>
<p>Case Study 세션은 Paper 세션과 버금갈 만큼의 비중이다. 학계 뿐만 아니라 기업에서도 나와 UI/UX 적용 사례, 앱 개발 사례, 음성디자인 인터페이스 디자인 사례 등 다양한 주제를 다룬다. 나는 SK Telecom이 한 Tmap x NUGU 음성 인터페이스 디자인 사례 발표에 참석했다. 기업 발표는 처음 듣는 거였는데, 학생 발표와 다른 점이 보였다. 일단 발표자가 떠는 기색 없이 차분하게 발표를 잘 하는 데서 이런 자리에 익숙한 게 보였다. 듣는 사람들에게 잘 보여야 하는 을의 입장이라기보다 “우리 정보 공유해줄게” 포스를 풍기는 자신감 있는 태도였다. 실제로 발표 시간 내내 찰칵찰칵 소리가 들릴 정도로 사람들이 계속 사진을 찍었다. 대기업의 업무 절차, 고민, 아이디어 등등을 담은 피피티에서 배워갈 점이 많아서 그랬나 보다. 발표 중간중간에 짤막한 동영상을 넣는 것도 부가적이지만 재밌는 발표 전략으로 보였다.</p>
<p style="text-align:center;"><img src="/assets/img/voice.jpg" alt="HCI" width="550" align="middle" /><br /><font size="3" color="grey">터치 -&gt; 음성 인터페이스 전환에 따라 디자이너도 일이 많아졌다</font></p>

<p>대기업의 업무 절차는 매우 꼼꼼했다. 기획부터 구현, 테스트까지 일련의 절차에 대한 가이드라인이 뚜렷했다. 우선, 음성 인터페이스가 기존의 터치 인터페이스보다 작동 depth가 얕다는 걸 기회로 인식하고 기획을 시작했다. 길 안내를 위해 터치를 5번 정도 해야 하던 것을 음성 명령 하나로 대체할 수 있다는 것이다. 이후에 네비게이션 관련 소비자의 구체적 needs를 파악하고, 원칙을 세워 어떤 needs를 반영할지, 안 할지를 정한다. 소비자가 네비게이션에게 엔터테인먼트 기능을 바란다고 조사에서 드러났어도, 안전 내비 원칙 상 동영상 기능은 제공하지 않는다는 식이다. 또한, 환경에 따른 현실적 제약을 고려해 서비스 범위를 명확히 한다. 운전 중인 사용자의 인지 능력은 50퍼센트밖에 안 된다는 한계를 고려해서 대표적 기능만을 제공한다. 핵심 기능에 대한 구상이 어느 정도 진전되면 target 고객의 입장에서 다시 한번 생각한다. Tmap 주요 고객층은 30-50대 남성으로 매우 성격이 급하고 디지털 literacy가 낮은 게 특징이니 기존의 인터페이스를 최대한 살린다. 그런 다음에야 관련 분야 research를 진행한다. 음성 인터페이스 관련 교재를 정해 스터디를 하기도 하고, 타사 서비스에 대한 case study도 한단다. 이후에 본격적인 디자인이 이루어지고, 결과물이 서비스로 거듭나기 전에 꼼꼼한 test 과정도 빼먹지 않는다.</p>

<p>디자인 과정에서 여러 고민이 있었고, 그에 따라 아이디어를 발전시키고 의사결정을 했다. 버튼 위치, 화면 구성, 폰트, 검색 시 노출되는 리스트 개수, 내비 대사의 정보량 등등 모든 의사결정에는 그에 선행하는 고민과 토론 과정이 있었다는 데서 기업은 기업이구나 싶었다. 터치가 아닌 음성 인터페이스라는 특성상 고려할 것이 몇 가지가 있었는데, 두 가지가 기억난다. 첫째는 정보량에 대한 부분이다. 사용자에게 제공할 수 있는 정보를 많이 가지고 있지만, 사용자가 가만히 기다리며 귀로 정보를 접하는 상황이므로 정보 개수를 최대 3개로 제한했다고 한다. “주유소 알려줘”라는 요청에 대해 가는 길에 있는 주유소, 가는 길은 아니지만 거리가 가까운 주요소, 가격이 싼 주요소 등 다양한 정보가 있지만 적절한 것 몇 개만 사용자에게 제공하는 식이다. 둘째는 음성 인식의 한계를 고려해서 유도성 대사(?)를 만든다는 부분이다. 같은 기능이더라도 사람마다 말하는 방식이 많은데, 그 모든 것을 이해하도록 음성인식 기술을 구현하는 건 쉽지 않다. “몇 번째” 장소로 갈까요? 이렇게 하려면 “안내 시작”이라고 말하세요, 같이 사용자의 대답을 특정 방향으로 유도하는 꾀를 썼다.</p>
<p style="text-align:center;"><img src="/assets/img/tmap.png" alt="Tmap x NUGU" width="250" align="middle" /><br /><font size="3" color="grey">이게 다 고민의 결과물</font></p>

<p>대기업의 힘은 사후 관리에서도 드러났다. 내가 주변에서 흔히 볼 수 있는 프로젝트는(주로 대학생) 데드라인이 가까워질수록 노력이 많이 투입되고, 제출과 동시에 프로젝트가 끝이 나는 순서였다. 기업은 서비스 출시 이후에도 유지 보수에 많은 자원을 투자한다. 사용자 로그 데이터를 통해 어떤 명령어가 많이 유입되는지 분석하고, Tmap 이용률이 타사 음성인식 서비스에 비해 높은 이유는 차 안이라는 개인적 공간에서 사용하기 때문이라는 분석도 내놓고, 생각보다 많은 사람이 AI와 잡담하기를 바란다는 점을 파악한다. 이 모든 데이터는 추후 업데이트에 반영된다고 한다. 기업의 규모 있는 프로젝트를 엿볼 수 있는 좋은 시간이었다.</p>


  
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  </script>
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</div>

  <!-- Disqus -->

<div id="disqus_thread"></div>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */

var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://hwijeen-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  <!--<div class="post-disqus">
      <section id="disqus_thread"></section>
      <div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */

var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://hwijeen-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  </div>-->


<div class="relatedPosts">

  <h3>Related Posts</h3>

  
  

  
  

  

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

</div>

<div class="recentPosts">

  <h3>Recent Posts</h3>

  
  

    
      
      <div class="post ml2">
        <a href="/2018-10-27/torchtext/" class="post-link">
          torchtext 사용법
          <small class="archive-list-post-date">27 Oct 2018</small>
        </a>
      </div>
      
    
      
      <div class="post ml2">
        <a href="/2018-10-22/vim-%E1%84%8A%E1%85%B3%E1%84%80%E1%85%B5/" class="post-link">
          vim 쓰기
          <small class="archive-list-post-date">22 Oct 2018</small>
        </a>
      </div>
      
    
      
      <div class="post ml2">
        <a href="/2018-10-20/Python-Subprocess-%E1%84%86%E1%85%A9%E1%84%83%E1%85%B2%E1%86%AF/" class="post-link">
          Python subprocess 모듈
          <small class="archive-list-post-date">20 Oct 2018</small>
        </a>
      </div>
      
    
      
      <div class="post ml2">
        <a href="/2018-10-09/Ubuntu-&-Bash/" class="post-link">
          Bash shell
          <small class="archive-list-post-date">09 Oct 2018</small>
        </a>
      </div>
      
    
      
      <div class="post ml2">
        <a href="/2018-10-09/Python-%E1%84%86%E1%85%A9%E1%84%83%E1%85%B2%E1%86%AF-%E1%84%80%E1%85%AA%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB/" class="post-link">
          Python module 관련
          <small class="archive-list-post-date">09 Oct 2018</small>
        </a>
      </div>
      
    
      
        

</div>


    <!-- Documents about icons are here: http://fontawesome.io/icons/ -->
<div class="footer">
  <hr />
  <div class="footer-link">
    
	
	
	
	

    

    
    <a href="https://github.com/hwijeen"><i class="fa fa-github" aria-hidden="true"></i></a>
    
	
	
	
	

    
	
	
	
	
	
	
	
	

    

    

    
    <a href="mailto:aurorall@naver.com"><i class="fa fa-envelope" aria-hidden="true"></i></a>
    

  </div>
  © 2018 Hwijeen Ahn. All rights reserved.
</div>

  </div>
</body>
</html>
