---
layout: post
title: "Bias, ethics, faireness in AI"
description: ""
comments: true
categories: []
tags:
- 
typora-root-url: ../../hwijeen.github.io
---

## Introduction

There has been a surge of attention given to the word bias in the AI community. It seems true that the question of bias and ethics are somewhat reserved to philosophers in the past. However, when it come to AI, not only philosophers but also AI researchers and all the other parties involved should all collaborate in the discussion. There has been lots of initiatives including ethics class in computer science departments ([UW](http://faculty.washington.edu/ebender/2019_575/), [CMU](http://demo.clab.cs.cmu.edu/ethical_nlp/)), tutorials in conference ([Tutorial: Bias and Fairness in Natural Language Processing](http://web.cs.ucla.edu/~kwchang/talks/emnlp19-fairnlp/)), public talks ([Fei-Fei Li & Yuval Noah Harari in Conversation](https://www.youtube.com/watch?v=d4rBh6DBHyw)), and blog posts ([Kyunghyun Cho's blog post](https://kyunghyuncho.me/social-impacts-bias-of-ai/)) to raise attention to the issue.

Arguably, the current AI research put too much emphasis on accuracy of the model, leaving fairness relatively unaddressed. The advocates of AI ethics wave point out that "they (models) run the risk of exploiting and reinforcing (or even amplify) the societal biases (e.g. gender bias) that are present in the underlying data."  The impact of this could be especially serious nowadays and in the future since AI technology is becoming prevalent in everyday life more than ever. As Fei-Fei Li mentions in the public talk, it is about the time that we start fostering consensus on wearing "AI seatbelt"; even if we sacrifice some efficacy, we should agree to take actions in order to prevent the critical loss.

There is a wave of research on bias during the past few years. They demonstrate that some NLP systems exhibit "bias" and try to measure and mitigate them. However, it seems like the term "bias" is inconsistent in concept among them. As a field, we must pay attention when we mean "bias." What is "bias" in machine learning? And why is it so essential to get rid of them?

Bias itself is not always bad. They can be good, bad, or neutral. According to Margaret Mitchell, from her talk in [CS224n](https://www.youtube.com/watch?v=XR8YSRcuVLE&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index), bias people talk about in machine learning can be categorized into three types. First, it is the "statistical bias", often written as b in y=ax+b. They can be referred to as good bias since they help us make a better prediction with the model. The second bias is "cognitive bias." This is something that happens when we look at the world in our own way. It can be confirmation bias, optimism bias, etc. And unless it hurts other people, it doesn't necessarily be bad. The third bias is "algorithmic bias." This is what we were talking about and what we try to erase out from our system. Algorithmic bias is the unjust, unfair, or prejudicial treatment of people related to race, income, sexual orientation, religion, gender, and any other characteristics when they manifest in algorithmic systems. The typical example can be a criminality prediction system that has an algorithmic bias towards black people by overgeneralization. 

![image-20201120182330437](/assets/img/image-20201120182330437.png)

As the picture showed above, bias can happen everywhere during the pipelined steps of machine learning. What's more important is that it amplifies during iteration because of the feedback loop. There are various approaches to solve this problem, and each of them focuses on different steps of the pipeline.

Multilinguality adds another layer to the complexity of the bias problem. There is a surge of research in multilingual NLP since it boosts performance for low-resources languages in many task. One of the popular ways to benefit from high resource languages is to leverage transfer learning. In transfer learning, knowledge learned from one language is used to make a prediction in another language.   It is reported that that there are various factors that influence the effectiveness of transfer, and some factors like typological dissimilarity turned out to be detrimental. The question for us is, does the transfer learning incur the transfer of bias? Is the "bad" bias picked up from one languages transferred to another language? We have directed our focus on bias in multilingual settings. In later posts, we will cover how we make our question more concrete and do relevant literature review.





----

## Interesting problem

How to intentionally bias model? Given the sufficient amount of partially and potentially biased data, is there any way we could directly influence the model to be biased toward certain aspect?

We will need to specify what kind of bias we would like to address. Need to be clear about how it affects people, including the mechanism by which the bias is imposed / experienced and the seriousness of the impact. 

The motivation behind is 1) to show how difficult it is to pinpoint and inject specific bias 2) learned model could be a source of adversarial signal.

-----

In multilingual model, is the level / aspect of bias different across languages? What could be the source of this difference?

In the most simplistic task, generated a sentence from template (The Woman Worked as a Babysitter: On Biases in Language Generation), to see how the model behaves. We will need to come up with a way to quantify this.

If we can show that the multilingual model behaves differently across language, we can make an interesting case that its representation is not language agnostic.

Related: *Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer*

----

Knowledge distillation to get rid of unintended bias.

## Knowing the difference

* Necessary to fully understand / differentiate stereotyping, implicit bias, etc.
* Language (Technology) is Power: A Critical Survey of "Bias" in NLP.
  * What kind of system behaviors are harmful, in what ways, to whom, and why.

## Interesting technical approaches

* Men Also Like Shopping: Reducing Gender Bias Amplification Using Corpus Level Constraints

## References

[Tutorial: Bias and Fairness in Natural Language Processing](http://web.cs.ucla.edu/~kwchang/talks/emnlp19-fairnlp/)

[Fei-Fei Li & Yuval Noah Harari in Conversation](https://www.youtube.com/watch?v=d4rBh6DBHyw)

[Kyunghyun Cho's blog post](https://kyunghyuncho.me/social-impacts-bias-of-ai/)





