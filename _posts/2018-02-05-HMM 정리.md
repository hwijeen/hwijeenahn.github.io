---
layout: post
title: "HMM 정리"
description: "문일철 교수님 HMM 강의 정리"
comments: true
categories: [NLP]
tags:
- HMM
- Machine Learning
---

문일철 HMM 강의 Objectives:

1. Learn hidden Markov model  
	1-1. Transition from the static clustering to the dynamic clustering  
	1-2. Understand the difference of the graphical model  

2. Know and albe to answer the three major questions of HMM  
	2-1. Know how to solve the evaluation question  
	2-2. Know how to solve the decoding question  
	2-3. Know how to solve the learning question  

3. Link to the previous lectures  
	3-1. Link the forward-backward algorithm to the message passing  
	3-2. Link the baum-welch algorithm to the EM algorithm  

<br>
<br>
1-1. Dynamic clustering 
<p style="text-align:center;"><img src="../assets/img/latent variable.png" width="550" alt="latentvariable" align="middle"><font size ="3" color="grey"><br>HMM in graphical notation</font></p>

- '시간'이라는 요소를 넣어서 모델링한다는 점에서 'dynamic'이라는 용어가 붙었다.
- '시간' 요소가 더해짐에 따라, data points는 더이상 서로 독립이 아니고 temporal relation을 갖는다.
- 즉, t+1 시점의 data는 t시점의 영향을 받는다.
- 이때, 다음 시점에 영향을 주는 건 개별 데이터가 아닌 latent factor다.
- 각 시점마다 잠재적인 동력(latent variable)이 있다. 각 시점에서 관측된 data는 그 잠재 동력의 발현일 뿐이다. HMM은 이런 생각을 담은 모델이다.
- 대표적인 예시는 HMM을 활용한 POS tagging. $$x_k$$를 단어, $$z_k$$를 품사로 생각한다. 

<br>
<br>
1-2. Properties of HMM
- $$x_k$$, $$z_k$$는 확률 변수. 어떤 분포를 가정하냐에 따라 continuous할 수도, discrete할 수도 있다.
- 여기서 $$x_k$$, $$z_k$$는 multinomial distribution을 따른다고 가정한다. 즉, discrete 변수. 
- Initial state probability는 첫 시점의 latent variable z값을 설정해주는 확률. 첫 시점은 '이전'이 없기에 따로 설정해주어야한다.  

$$P(z_1) \text{~} Mult(\pi_1, \dots, \pi_k)$$  

- Transition probability는 다음 시점으로 '넘어갈 때'의 확률. 지금 latent variable이 $$i$$로 주어질 때 다음 latent variable이 j일 확률.

$$P(z_t^j =1 \mid z_{t-1}^i = 1) = a_{i,j}$$

- Emission Probability는 현 시점 latent varialbe이 그 data를 '뱉어낼' 확률. 지금 이 시점엔 명사가 올텐데, 사과라는 단어일 확률은?

$$P(x_t^j = 1 \mid z_t^i = 1) = b_{i,j}$$

- $$x$$가 $$z$$를 '따르는' 걸 정확하게 뭐라하지?  

<br>
<br>
2-1. Evaluation question
- $$P(X \mid M, \pi, a, b)$$를 찾는 문제. trained model에서 주어진 데이터가 관측될 확률은?
- learning question을 통해 $$\pi, a, b$$를 다 추정하고 나서야 이 문제들을 풀 수 있다. 
- X & Z라는 labeled training data를 가지고 있다면 쉽게 구할 수 있다.  

$$
\begin{align}
P(X,Z) &= P(x_1, \dots, x_t, z_1, \dots, z_t) \\  
		&= P(z_1)P(x_1 \mid z_1)P(z2 \mid z1) \dots P(z_t \mid z_{t-1})P(x_t \mid z_t) \\
		&= \pi_{z1} b_{z1,x1} a_{z1,z2} \dots a_{z_{t-1},z_t} b_{z_t, x_t}

\end{align}
$$

- 이 식은 $$Z$$값을 다 알고 있을 때나 써먹을 수 있다. 게다가 $$Z$$값의 조합에 따라 $$P(X,Z)$$값을 각각 계산해줘야한다는 단점이 있다. 
- 그래서 대안으로 나온 게 forward probability. 
- forward probability를 사용하면 $$Z$$없이도 evaluation question 답을 찾을 수 있다.  
- 방법은 $$P(x_1, \dots, x_t, z_t^k=1)$$이라는 식을 derive하고, 이후에 $$z_t^k$$를 marginalize out하자는 것.

$$
\begin{align}
&P(x_1, \dots, x_t, z_t^k=1) = \alpha_t^k \\
&\alpha_1^k = b_{k,x1} \pi_k \\
&\alpha_t^k =b_{k,x_t} \sum_{i}\alpha_{t-1}^i a_{i,k} \\
&RETURN \sum_{i}\alpha_t^i \\
&\because \sum_{i}\alpha_t^i = \sum_{i}P(x_1, \dots, x_t, z_t^i) = P(x_1, \dots, x_t)
\end{align}
$$

