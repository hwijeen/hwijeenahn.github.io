---
layout: post
title: "Noise Contrastive Estimation / Negative Sampling / Energy-Based Model"
description: ""
comments: true
categories: []
tags:
- NCE
- Negative Sampling
- Energy-Based Model
typora-root-url: ../../hwijeen.github.io
---

## Motivation

The problem with skipgram and CBOW was that their objective funtion includes softmax normalization, which is computationally expensive. The remedy was to modify objective function with **negative sampling**. A closely related idea is **noise contrastive estimation (NCE)**. These work by transforming the computationally expensive learning problem into a binary classification proxy problem that uses the same parameters but requires statistics that are easier to compute. Chris Dyer's note illustrates how they relate to each other. In sum, *NCE is a general parameter estimation technique that is asymptotically unbiased, while negative sampling is best understood as a variation of NCE useful for learning word representations but not a general-purpose estimator.*

NCE also appears frequently in self-supervised learning literature. In fact, Yann LeCun's talk illustrated where NCE stands in the discussion of **Energy-Based Models**, which he believes is an effective direction toward self-supervised learning.

## NCE

1. Transform the objective by adding some noise. NCE reduces the language model estimation problem to the problem of estimating the parameters of a probabilistic binary classifier that uses the same parameters to distinguish samples from the empirical distribution from samples generated by the noise distribution. 

2. Replace empirical distribution with the model distribution (neural network). 
3. NCE proposes that the expensive partition function value can be estimated as some parameter, and it turns out that setting that parameter as 1 is effective. This means that we are replacing the expensive summation with value of just 1. 
4. Binary classification problem can be trained to maximied conditional log likelihood.
5. In addition, expectation over negative samples can be replaced with Monte Carlo approximation.

## Why NCE works?

![Screen Shot 2020-11-03 at 15.01.20](/assets/img/Screen Shot 2020-11-03 at 15.01.20.png)

## Negative sampling

Negative sampling models probability with *sigmoid function*. The asymptotic analysis does not hold here, thus it cannot be used as general parameter estimation technique. It is only effective (surprisingly?) for learning word vectors.

## Energy-Based Models

Energy based models, which includes contrastive methods, are an important topic in self-supervised learning. It can be understood as pusing the energy up for negative samples and down for real sample. One of the core problems is how to select negative samples, in such a high dimensional space.

![Screen Shot 2020-11-03 at 15.22.37](/assets/img/Screen Shot 2020-11-03 at 15.22.37.png)

![Screen Shot 2020-11-03 at 15.24.10](/assets/img/Screen Shot 2020-11-03 at 15.24.10.png)

![Screen Shot 2020-11-03 at 15.30.56](/assets/img/Screen Shot 2020-11-03 at 15.30.56.png)

Contrastive method includes includes MLE, metric learning siamese network, GAN, masked auto encoder and so more! 

VAE is an example of regularized method.

![Screen Shot 2020-11-03 at 15.37.22](/assets/img/Screen Shot 2020-11-03 at 15.37.22.png)

These are some example papers on contrastive embedding. 

![image-20201103154053780](/assets/img/image-20201103154053780.png)

Here are three exmple ways to push negatvie samples up and postive samples down.![image-20201103154210471](/assets/img/image-20201103154210471.png)

MLE boils down to contrastive methods?

![image-20201103154421825](/assets/img/image-20201103154421825.png)

Some methods don't require contrastive samples?!



## References

[Chris Dyer's Note](http://demo.clab.cs.cmu.edu/cdyer/nce_notes.pdf)

CS224n classnote1

[Yann Lecun's talk on SAIF 2020](https://www.youtube.com/watch?v=xT8wMnPLnP0)



