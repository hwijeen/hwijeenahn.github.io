---
layout: post
title: "GRU & LSTM 질문 정리"
comments: true
description: "cs224n 듣고 질문 정리"
categories: [NLP]
tags: 
- NLP
- Deep Learning
---

*** ReLU가 왜 non-linear일까?**  
<p style="text-align:center;"><img src="/assets/img/relu.png" width="450" alt="ReLU"></p>

ReLU를 여러 개 더하면 non-linear decision boundary를 근사할 수 있다.    
[출처](https://medium.com/@malay.haldar/how-do-neural-networks-work-57d1ab5337ce)

*** bias의 의미?**  

<p style="text-align:center;"><img src="/assets/img/sigmoid.png" width="450" alt="sigmoid"></p>

sigmoid의 인자 $$wx$$에서 $$w$$값을 조정하면 "기울기"를 조절할 수 있다.

<p style="text-align:center;"><img src="/assets/img/bias.png" width="450" alt="bias"></p>

sigmoid의 인자 $$wx+b$$에서 $$b$$값을 조정하면 "x축에 대해 평행이동"하는 효과가 있다.  
[출처](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks)

*** RNN with variable length input?**  

*** $$(1-z_t)$$과 $$z_t$$의 위치**  
$$
\\
Hidden\:state\:of\:GRU \\
h_t = (1-z_t) \odot \tilde{h_t} + z_t \odot h_{t-1}
$$

여기서 $$(1-z_t)$$과 $$z_t$$의 위치를 바꿔도 알아서 learning 해준다.

*** GRU와 LSTM이 vanishing gradient problem 어느 정도 해결해주는 이유?**   

$$h_t$$와 $$h_{t-1}$$이 $$z_t$$(0~1 사이)를 곱한 관계니까 vanishing gradient problem 여전한 거 아닌가?
실제론 z값이 1에 근접한가 항상..?
"linear carousel"
더 조사해봐야겠다

*** 왜 $$\tilde{h_t}$$엔 $$tanh$$를 적용?**  

<p style="text-align:center;"><img src="/assets/img/activations.png" width="500" alt="activations"></p>
sigmoid 보다 tanh가 수렴 속도가 빠르다. 
[출처](https://ratsgo.github.io/deep%20learning/2017/04/22/NNtricks/)
게다가 sigmoid의 경우 미분계수가 최대 0.25니까 vanishing 현상이 더욱 잘 발생한다. tanh의 경우 최대 미분계수는 1.
그럼에도 불구하고 GRU reset gate($$z_t$$) & reset gate($$r_T$$)에서 sigmoid를 쓰는 이유는 0~1사이의 "비율"값을 뽑아내기 위해.


*** GRU과 LSTM엔 왜 bias가 없지?**  
있음ㅋ activation function의 평행이동을 가능하게 해주는 기능이니까, 어느 모델이나 없으면 절대 안 될 듯!


*** GRU와 LSTM의 $$\hat{y}$$?**  


*** LSTM은 GRU보다 parameter가 많으니 data가 더 많이 필요한가?**  
일단 LSTM 및 GRU는 data가 기본적을 많아야하고, 많을 수록 성능이 좋은 건 당연한 사실


*** encoder, decoder 적용한 GRU, LSTM 모델은?**  




더 알아볼 것:  
soft sampling? hard sampling? attention이 alignment를 어느 정도 잡는다?