---
layout: post
title: "Self-training"
description: ""
comments: true
categories: []
tags:
- 
typora-root-url: ../../hwijeen.github.io
---

## Related words

Both **pre-training** and **self-training** leverage unlabeled data. The difference is that pre-training is done in an unsupervised fashion where self-training uses unlabeled data through semi-supervised learning. However, it seems that some people also consider pre-training as a form of semi-supervised learning.

> Deep Learning relies heavily on unsupervised or sem-supervised learning, and assumes that representations of X that are useful to capture P(X) are also in part useful to capture P(Y|X). [Bengio, 12]

> Self-supervised pretraining can learn useful representations by training with *pretext task*, such as cloze and masked language modeling, denoising autoencoder, etc. [CRISS papper]

Another related term is **self-supervised learning**. See the below for why we should use 'self-supervised' instead of unsupervised.

![Screen Shot 2020-11-09 at 10.19.39](/assets/img/Screen Shot 2020-11-09 at 10.19.39.png)



One of the major challenge is to acquire in-domain data.

데이터 효율성을 올리기 위한 관점



## Some works

**Self-training Improves Pre-training for Natural Language Understanding** The authors argues that the two methods are complementary. They provide *SentAugment*, a semantic sentence encoder, to retrieve task/domain-specific unlabeled data. The  retrieved are then used for self-training.

**SSMBA: Self-supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness** In this work, pretrained language models are used to generate synthetic data. Synthetic examples are generated by using a pair of corruption and reconstruction. The effect is that the model trained with the augmented data is more robust and generalizable.

**Cross-lingual Retrieval for Iterative Self-Supervised Training** The authors first show that the encoder outputs of mBART is language agnostic representation. Mining parallel sentence pair using the representation in an iterative manner, CRISS achieves the state of the art performance in unsupervised neural machine translation.

---

1 & 3: Mining unlabeled data from a big corpus. 



## ETC

Contrastive learning, ranking loss, 좀 더 어려운 pretext task

Bootstrap, distant supervision

Details (continuous/discrete label, joint/separate,  iterative?) 



## References

[Yann Lecun's talk on SAIF 2020](https://www.youtube.com/watch?v=xT8wMnPLnP0)

[Self-supervisded learning, 돌아보고 살펴보고 내다보기](https://aifrenz.github.io/present_file/Self-supervised%20learning,%20돌아보고%20살펴보고%20내다보기.pdf)

